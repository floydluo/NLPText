{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical NER Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_r/46yh1st54y35_jbqhdz82lrh0000gp/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/clinical_ner_sample/MEntity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.664 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 659\n",
      "Total Num of Unique Tokens 245\n",
      "CORPUS\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 3\n",
      "SENT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 21\n",
      "TOKEN\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 659\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 245\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.Entity',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 1, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedPOS Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medical_pos_sample/batch1\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "corpus/medical_pos_sample/batch2\n",
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "Total Num of All    Tokens 6460\n",
      "Total Num of Unique Tokens 713\n",
      "CORPUS\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 179\n",
      "TOKEN\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6460\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 153\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 713\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/medical_pos_sample/' # TODO\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = 'annofile4sent'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.UMLSTag',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n",
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3865\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 3865\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_r/46yh1st54y35_jbqhdz82lrh0000gp/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/ResumeCN/test.char.bmes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/ResumeCN/train.char.bmes\n",
      "corpus/ResumeCN/dev.char.bmes\n",
      "Total Num of All    Tokens 149123\n",
      "Total Num of Unique Tokens 1865\n",
      "CORPUS\tit is Dumped into file: data/ResumeCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/ResumeCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/ResumeCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4476\n",
      "SENT\tit is Dumped into file: data/ResumeCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4476\n",
      "TOKEN\tit is Dumped into file: data/ResumeCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 149123\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 33\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 1865\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/ResumeCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.bmes'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewsCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floydluo/Environments/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_r/46yh1st54y35_jbqhdz82lrh0000gp/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.693 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/NewsCN/demo.train.char\n",
      "corpus/NewsCN/demo.test.char\n",
      "Total Num of All    Tokens 54635\n",
      "Total Num of Unique Tokens 2329\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1129\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1129\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 54635\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 2329\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CoNLL-2003/eng.train.openNLP\n",
      "corpus/CoNLL-2003/eng.testa.openNLP\n",
      "corpus/CoNLL-2003/eng.testb.openNLP\n",
      "Total Num of All    Tokens 254708\n",
      "Total Num of Unique Tokens 27270\n",
      "CORPUS\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16477\n",
      "SENT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16477\n",
      "TOKEN\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254708\n",
      "**************************************** \n",
      "\n",
      "annoE-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 27270\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CoNLL-2003/'\n",
    "\n",
    "Corpus2GroupMethod = '.openNLP'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = False\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    'change_tags': True, \n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPBA2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NLPBA2004/sampletest2.iob2\n",
      "corpus/NLPBA2004/Genia4ERtask2.iob2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floydluo/Environments/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/NLPBA2004/Genia4ERtask1.iob2\n",
      "corpus/NLPBA2004/sampletest1.iob2\n",
      "Total Num of All    Tokens 913580\n",
      "Total Num of Unique Tokens 20663\n",
      "CORPUS\tit is Dumped into file: data/NLPBA2004/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NLPBA2004/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 4\n",
      "TEXT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 33492\n",
      "SENT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 33492\n",
      "TOKEN\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 913580\n",
      "**************************************** \n",
      "\n",
      "annoE-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/annoE-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/token.voc\n",
      "annoE-bioes\tthe length of it is   : 20663\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NLPBA2004/'\n",
    "\n",
    "Corpus2GroupMethod = '.iob2'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = False\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': '\\t',\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Handle Labels\n",
    "\n",
    "## strText and SSET\n",
    "\n",
    "For each text, we must get a strText and SSET, which meet the following restriction.\n",
    "\n",
    "Besides, we also need a strSents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结肠多发息肉。\n",
      "患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('多发息肉', 2, 6, '疾病'),\n",
       " ('慢性', 15, 17, '修饰'),\n",
       " ('多发息肉', 29, 33, '疾病'),\n",
       " ('3月余', 33, 36, '修饰'),\n",
       " ('无阳性体征', 43, 48, '不确定')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "\n",
    "notZeroIndex = 1 \n",
    "for idx, sset in enumerate(SSETText):\n",
    "    data = (sset[0], int(sset[1]) - notZeroIndex, int(sset[2]), sset[-1])\n",
    "    SSETText[idx] = data\n",
    "\n",
    "\n",
    "# check\n",
    "for sset in SSETText:\n",
    "    try:\n",
    "        assert strText[sset[1]: sset[2]] == sset[0]\n",
    "    except:\n",
    "        print('strText:', strText[sset[1] : sset[2]])\n",
    "        print('SSETText:', sset[0])\n",
    "        \n",
    "SSETText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['结', 0, 'O'],\n",
       " ['肠', 1, 'O'],\n",
       " ['多', 2, '疾病-B'],\n",
       " ['发', 3, '疾病-I'],\n",
       " ['息', 4, '疾病-I'],\n",
       " ['肉', 5, '疾病-E'],\n",
       " ['。', 6, 'O'],\n",
       " ['\\n', 7, 'O'],\n",
       " ['患', 8, 'O'],\n",
       " ['中', 9, 'O'],\n",
       " ['老', 10, 'O'],\n",
       " ['年', 11, 'O'],\n",
       " ['男', 12, 'O'],\n",
       " ['性', 13, 'O'],\n",
       " [',', 14, 'O'],\n",
       " ['慢', 15, '修饰-B'],\n",
       " ['性', 16, '修饰-E'],\n",
       " ['病', 17, 'O'],\n",
       " ['程', 18, 'O'],\n",
       " ['。', 19, 'O'],\n",
       " [' ', 20, 'O'],\n",
       " ['因', 21, 'O'],\n",
       " ['“', 22, 'O'],\n",
       " ['体', 23, 'O'],\n",
       " ['检', 24, 'O'],\n",
       " ['发', 25, 'O'],\n",
       " ['现', 26, 'O'],\n",
       " ['大', 27, 'O'],\n",
       " ['肠', 28, 'O'],\n",
       " ['多', 29, '疾病-B'],\n",
       " ['发', 30, '疾病-I'],\n",
       " ['息', 31, '疾病-I'],\n",
       " ['肉', 32, '疾病-E'],\n",
       " ['3', 33, '修饰-B'],\n",
       " ['月', 34, '修饰-I'],\n",
       " ['余', 35, '修饰-E'],\n",
       " ['”', 36, 'O'],\n",
       " ['入', 37, 'O'],\n",
       " ['院', 38, 'O'],\n",
       " ['。', 39, 'O'],\n",
       " ['查', 40, 'O'],\n",
       " ['体', 41, 'O'],\n",
       " [':', 42, 'O'],\n",
       " ['无', 43, '不确定-B'],\n",
       " ['阳', 44, '不确定-I'],\n",
       " ['性', 45, '不确定-I'],\n",
       " ['体', 46, '不确定-I'],\n",
       " ['征', 47, '不确定-E'],\n",
       " ['。', 48, 'O']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "\n",
    "SSETText = [ ('多发息肉', 2, 6, '疾病'),\n",
    "             ('慢性', 15, 17, '修饰'),\n",
    "             ('多发息肉', 29, 33, '疾病'),\n",
    "             ('3月余', 33, 36, '修饰'),\n",
    "             ('无阳性体征', 43, 48, '不确定')]\n",
    "\n",
    "def getCITText(strText, SSETText, TOKENLevel='char'):\n",
    "    len(SSETText) > 0 \n",
    "    if TOKENLevel == 'char':\n",
    "        for sset in SSETText:\n",
    "            try:\n",
    "                assert strText[sset[1]: sset[2]] == sset[0]\n",
    "            except:\n",
    "                print('strText:', strText[sset[1] : sset[2]])\n",
    "                print('SSETText:', sset[0])\n",
    "        CITAnnoText = []\n",
    "        for sset in SSETText:\n",
    "            # BIOES\n",
    "            strAnno, s, e, tag = sset\n",
    "            CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "            CIT[-1][2] = tag + '-E'\n",
    "            CIT[ 0][2] = tag + '-B'\n",
    "            if len(CIT) == 1:\n",
    "                CIT[0][2] = tag + '-S' \n",
    "            CITAnnoText.extend(CIT)\n",
    "\n",
    "        # print(strAnnoText)\n",
    "        CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "        for citAnno in CITAnnoText:\n",
    "            c, idx, t = citAnno\n",
    "            assert CITText[idx][0] == c\n",
    "            CITText[idx] = citAnno\n",
    "\n",
    "    elif TOKENLevel == 'word':\n",
    "        CITText = []\n",
    "        for idx, sset in enumerate(SSETText):\n",
    "            try:\n",
    "                assert sset[0] == strText[idx]\n",
    "            except:\n",
    "                print(strText)[idx]\n",
    "                print(sset[0])\n",
    "\n",
    "            CITText.append(sset)\n",
    "    return CITText\n",
    "\n",
    "\n",
    "CITText = getCITText(strText, SSETText, TOKENLevel='char')\n",
    "CITText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['结', 0, 'O'],\n",
       "  ['肠', 1, 'O'],\n",
       "  ['多', 2, '疾病-B'],\n",
       "  ['发', 3, '疾病-I'],\n",
       "  ['息', 4, '疾病-I'],\n",
       "  ['肉', 5, '疾病-E'],\n",
       "  ['。', 6, 'O']],\n",
       " [['患', 0, 'O'],\n",
       "  ['中', 1, 'O'],\n",
       "  ['老', 2, 'O'],\n",
       "  ['年', 3, 'O'],\n",
       "  ['男', 4, 'O'],\n",
       "  ['性', 5, 'O'],\n",
       "  [',', 6, 'O'],\n",
       "  ['慢', 7, '修饰-B'],\n",
       "  ['性', 8, '修饰-E'],\n",
       "  ['病', 9, 'O'],\n",
       "  ['程', 10, 'O'],\n",
       "  ['。', 11, 'O']],\n",
       " [['因', 0, 'O'],\n",
       "  ['“', 1, 'O'],\n",
       "  ['体', 2, 'O'],\n",
       "  ['检', 3, 'O'],\n",
       "  ['发', 4, 'O'],\n",
       "  ['现', 5, 'O'],\n",
       "  ['大', 6, 'O'],\n",
       "  ['肠', 7, 'O'],\n",
       "  ['多', 8, '疾病-B'],\n",
       "  ['发', 9, '疾病-I'],\n",
       "  ['息', 10, '疾病-I'],\n",
       "  ['肉', 11, '疾病-E'],\n",
       "  ['3', 12, '修饰-B'],\n",
       "  ['月', 13, '修饰-I'],\n",
       "  ['余', 14, '修饰-E'],\n",
       "  ['”', 15, 'O'],\n",
       "  ['入', 16, 'O'],\n",
       "  ['院', 17, 'O'],\n",
       "  ['。', 18, 'O']],\n",
       " [['查', 0, 'O'],\n",
       "  ['体', 1, 'O'],\n",
       "  [':', 2, 'O'],\n",
       "  ['无', 3, '不确定-B'],\n",
       "  ['阳', 4, '不确定-I'],\n",
       "  ['性', 5, '不确定-I'],\n",
       "  ['体', 6, '不确定-I'],\n",
       "  ['征', 7, '不确定-E'],\n",
       "  ['。', 8, 'O']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。', '因“体检发现大肠多发息肉3月余”入院。', '查体:无阳性体征。']\n",
    "\n",
    "CITText  =  [['结', 0, 'O'],\n",
    "             ['肠', 1, 'O'],\n",
    "             ['多', 2, '疾病-B'],\n",
    "             ['发', 3, '疾病-I'],\n",
    "             ['息', 4, '疾病-I'],\n",
    "             ['肉', 5, '疾病-E'],\n",
    "             ['。', 6, 'O'],\n",
    "             ['\\n', 7, 'O'],\n",
    "             ['患', 8, 'O'],\n",
    "             ['中', 9, 'O'],\n",
    "             ['老', 10, 'O'],\n",
    "             ['年', 11, 'O'],\n",
    "             ['男', 12, 'O'],\n",
    "             ['性', 13, 'O'],\n",
    "             [',', 14, 'O'],\n",
    "             ['慢', 15, '修饰-B'],\n",
    "             ['性', 16, '修饰-E'],\n",
    "             ['病', 17, 'O'],\n",
    "             ['程', 18, 'O'],\n",
    "             ['。', 19, 'O'],\n",
    "             [' ', 20, 'O'],\n",
    "             ['因', 21, 'O'],\n",
    "             ['“', 22, 'O'],\n",
    "             ['体', 23, 'O'],\n",
    "             ['检', 24, 'O'],\n",
    "             ['发', 25, 'O'],\n",
    "             ['现', 26, 'O'],\n",
    "             ['大', 27, 'O'],\n",
    "             ['肠', 28, 'O'],\n",
    "             ['多', 29, '疾病-B'],\n",
    "             ['发', 30, '疾病-I'],\n",
    "             ['息', 31, '疾病-I'],\n",
    "             ['肉', 32, '疾病-E'],\n",
    "             ['3', 33, '修饰-B'],\n",
    "             ['月', 34, '修饰-I'],\n",
    "             ['余', 35, '修饰-E'],\n",
    "             ['”', 36, 'O'],\n",
    "             ['入', 37, 'O'],\n",
    "             ['院', 38, 'O'],\n",
    "             ['。', 39, 'O'],\n",
    "             ['查', 40, 'O'],\n",
    "             ['体', 41, 'O'],\n",
    "             [':', 42, 'O'],\n",
    "             ['无', 43, '不确定-B'],\n",
    "             ['阳', 44, '不确定-I'],\n",
    "             ['性', 45, '不确定-I'],\n",
    "             ['体', 46, '不确定-I'],\n",
    "             ['征', 47, '不确定-E'],\n",
    "             ['。', 48, 'O']]\n",
    "\n",
    "def getCITSents(strSents, CITText):\n",
    "    lenLastSent = 0\n",
    "    collapse    = 0 # don't need to move \n",
    "    CITSents = []\n",
    "    for strSent in strSents:\n",
    "        CITSent = []\n",
    "        for sentTokenIdx, c in enumerate(strSent):\n",
    "            # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            while c != cT and c != ' ':\n",
    "                collapse = collapse + 1\n",
    "                txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                cT, _, tT = CITText[txtTokenIdx]\n",
    "            CITSent.append([c,sentTokenIdx, tT])\n",
    "        lenLastSent = lenLastSent + len(strSent)\n",
    "        CITSents.append(CITSent)\n",
    "    # CITSents\n",
    "    # Here we get CITSents  \n",
    "    return CITSents\n",
    "       \n",
    "    \n",
    "CITSents = getCITSents(strSents, CITText)\n",
    "CITSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSSET_from_CIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['结', 0, 'O'], ['肠', 1, 'O'], ['多', 2, '疾病-B'], ['发', 3, '疾病-I'], ['息', 4, '疾病-I'], ['肉', 5, '疾病-E'], ['。', 6, 'O']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'O', '疾病-B', '疾病-I', '疾病-I', '疾病-E', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CITSent = CITSents[0]\n",
    "print(CITSent)\n",
    "tag_seq = [i[-1] for i in CITSent]\n",
    "tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSET_from_CIT(orig_seq = None, tag_seq = None, CIT = None, tag_seq_tagScheme = 'BIO', join_char = ''):\n",
    "    # orig_seq is sentence without start or end\n",
    "    # tag_seq may have start or end\n",
    "        \n",
    "    tagScheme = tag_seq_tagScheme\n",
    "    if tagScheme == 'BIOES':\n",
    "        tag_seq = [i.replace('-S', '-B').replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOE':\n",
    "        tag_seq = [i.replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOS':\n",
    "        tag_seq = [i.replace('-S', '-B') for i in tag_seq]\n",
    "    elif tagScheme == 'BIO':\n",
    "        pass\n",
    "    else:\n",
    "        print('The tagScheme', tagScheme, 'is not supported yet...')\n",
    "    \n",
    "    if not CIT:\n",
    "        # use BIO tagScheme\n",
    "        CIT = list(zip(orig_seq, range(len(orig_seq)), tag_seq))\n",
    "    taggedCIT = [cit for cit in CIT if cit[2]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedCIT)) if taggedCIT[idx][2][-2:] == '-B']\n",
    "    startIdx.append(len(taggedCIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedCIT[startIdx[i]: startIdx[i+1]]\n",
    "        string = join_char.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][1], entityAtom[-1][1] + 1\n",
    "        tag = entityAtom[0][2].split('-')[0]\n",
    "        entitiesList.append((string, start, end, tag))\n",
    "    return entitiesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get strText, strSents, and SSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
