{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical NER Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/clinical_ner_sample/MEntity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.610 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 659\n",
      "Total Num of Unique Tokens 245\n",
      "CORPUS\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 3\n",
      "SENT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 21\n",
      "TOKEN\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 659\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/clinical_ner_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 245\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.Entity',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 1, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedPOS Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medical_pos_sample/batch2\n",
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "corpus/medical_pos_sample/batch1\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "Total Num of All    Tokens 6460\n",
      "Total Num of Unique Tokens 713\n",
      "CORPUS\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 179\n",
      "TOKEN\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6460\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 153\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/medical_pos_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 713\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/medical_pos_sample/' # TODO\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'annofile4sent'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.UMLSTag',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.589 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3825\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3825\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è®° è ä» æ­ å· æ± å¹² åº å¬ å® å å± äº è§£ å° , ç» è¿ ä¸ ä¸ª å¤ æ ç ä¾¦ æ¥ å·¥ ä½ , æ± å¹² åº ç¦ æ¯ ä¸ æ¡ ç» æ è· å¸ è´© æ¯ äºº å 5 å , ç¼´ è· â å° æ¯ â 4 0 0 ä½ å , æ¯ èµ 3 0 0 0 0 ä½ å , æ£ æ¼ æ±½ è½¦ ä¸ è¾ ã'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 2\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_name': 4109,\n",
       " 'time': 4243,\n",
       " 'person_name': 5123,\n",
       " 'org_name': 2683,\n",
       " 'location': 4593,\n",
       " 'company_name': 2368}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/ResumeCN/test.char.bmes\n",
      "corpus/ResumeCN/train.char.bmes\n",
      "corpus/ResumeCN/dev.char.bmes\n",
      "Total Num of All    Tokens 149123\n",
      "Total Num of Unique Tokens 1865\n",
      "CORPUS\tit is Dumped into file: data/ResumeCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/ResumeCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/ResumeCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4476\n",
      "SENT\tit is Dumped into file: data/ResumeCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4476\n",
      "TOKEN\tit is Dumped into file: data/ResumeCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 149123\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 33\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/ResumeCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1865\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/ResumeCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.bmes'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NAME': 1048,\n",
       " 'PRO': 337,\n",
       " 'EDU': 1071,\n",
       " 'TITLE': 7762,\n",
       " 'ORG': 5684,\n",
       " 'CONT': 320,\n",
       " 'RACE': 143,\n",
       " 'LOC': 55}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewsCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T08:21:49.402497Z",
     "start_time": "2019-08-18T08:21:45.917868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.619 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/NewsCN/demo.test.char\n",
      "corpus/NewsCN/demo.train.char\n",
      "Total Num of All    Tokens 69463\n",
      "Total Num of Unique Tokens 2570\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1536\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1536\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 69463\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2570\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n",
      "corpus/NewsCN/demo.test.char\n",
      "corpus/NewsCN/demo.train.char\n",
      "Total Num of All    Tokens 54635\n",
      "Total Num of Unique Tokens 2329\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1129\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1129\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 54635\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2329\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GPE': 1488, 'ORG': 626, 'PER': 698, 'LOC': 210}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSRA-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:31:47.183093Z",
     "start_time": "2019-08-18T13:31:36.396767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/MSRA/testright1.txt\n",
      "corpus/MSRA/train1.txt\n",
      "Total Num of All    Tokens 2339922\n",
      "Total Num of Unique Tokens 4831\n",
      "CORPUS\tit is Dumped into file: data/MSRA/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/MSRA/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/MSRA/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 50725\n",
      "SENT\tit is Dumped into file: data/MSRA/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 50725\n",
      "TOKEN\tit is Dumped into file: data/MSRA/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2339922\n",
      "**************************************** \n",
      "\n",
      "annoE-bioes\tis Dumped into file: data/MSRA/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 13\n",
      "\t\tWrite to: data/MSRA/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/MSRA/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 4831\n",
      "\t\tWrite to: data/MSRA/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/MSRA/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = []\n",
    "\n",
    "# å¶å®/o é/o æ±/o é/o å/o ï¼/o åæ¯/o ä»ä¹/o ä¸/o ä»ä¹/o å¢/o ï¼/o \n",
    "anno = 'anno_embed_along_token' \n",
    "anno_keywords = {\n",
    "    'sep_between_tokens': ' ',\n",
    "    'sep_between_token_label': '/', \n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:31:57.821940Z",
     "start_time": "2019-08-18T13:31:54.520594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nt': 21900, 'nr': 19588, 'ns': 39394}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    if len(SSETs) == 0:\n",
    "        zlocidx = locidx\n",
    "    \n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:32:02.749608Z",
     "start_time": "2019-08-18T13:32:02.735130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train valid test from corpus itself\n",
      "The num of train sentences: 46364\n",
      "The num of test  sentences: 4361\n",
      "The num of valid sentences: 0\n"
     ]
    }
   ],
   "source": [
    "from nlptext.folder import Folder\n",
    "data = []\n",
    "for data_type in ['train', 'test', 'dev']:\n",
    "    t = []\n",
    "    for i in range(BasicObject.GROUP['length']):\n",
    "        f = Folder(i)\n",
    "        if data_type in f.name:\n",
    "            t = list(range(*f.IdxSentStartEnd))\n",
    "    data.append(t)\n",
    "    \n",
    "    \n",
    "train_sent_idx, test_sent_idx, valid_sent_idx = data\n",
    "print('load train valid test from corpus itself')\n",
    "print('The num of train sentences:', len(train_sent_idx))\n",
    "print('The num of test  sentences:', len(test_sent_idx))\n",
    "print('The num of valid sentences:', len(valid_sent_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CoNLL-2003/eng.train.openNLP\n",
      "corpus/CoNLL-2003/eng.testb.openNLP\n",
      "corpus/CoNLL-2003/eng.testa.openNLP\n",
      "Total Num of All    Tokens 254708\n",
      "Total Num of Unique Tokens 27270\n",
      "CORPUS\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16477\n",
      "SENT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16477\n",
      "TOKEN\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254708\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/CoNLL-2003/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 27270\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CoNLL-2003/'\n",
    "\n",
    "Corpus2GroupMethod = '.openNLP'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' ' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ', # the seperation\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    'change_tags': True, # change I-B tags\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares on the Dhaka Stock Exchange ( DSE ) may remain steady as small investors are expected to target mainly blue chips while overseas investors will prefer to keep to the sidelines when the market reopens after Moslem Friday weekend , brokers said .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shares', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Dhaka', 'ORG-B'),\n",
       " ('Stock', 'ORG-I'),\n",
       " ('Exchange', 'ORG-I'),\n",
       " ('(', 'O'),\n",
       " ('DSE', 'ORG-B'),\n",
       " (')', 'O'),\n",
       " ('may', 'O'),\n",
       " ('remain', 'O'),\n",
       " ('steady', 'O'),\n",
       " ('as', 'O'),\n",
       " ('small', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('are', 'O'),\n",
       " ('expected', 'O'),\n",
       " ('to', 'O'),\n",
       " ('target', 'O'),\n",
       " ('mainly', 'O'),\n",
       " ('blue', 'O'),\n",
       " ('chips', 'O'),\n",
       " ('while', 'O'),\n",
       " ('overseas', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('will', 'O'),\n",
       " ('prefer', 'O'),\n",
       " ('to', 'O'),\n",
       " ('keep', 'O'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('sidelines', 'O'),\n",
       " ('when', 'O'),\n",
       " ('the', 'O'),\n",
       " ('market', 'O'),\n",
       " ('reopens', 'O'),\n",
       " ('after', 'O'),\n",
       " ('Moslem', 'MISC-B'),\n",
       " ('Friday', 'O'),\n",
       " ('weekend', 'O'),\n",
       " (',', 'O'),\n",
       " ('brokers', 'O'),\n",
       " ('said', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16471)\n",
    "print(st.sentence)\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ORG': 10496, 'MISC': 5248, 'PER': 10990, 'LOC': 10944}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# from nlptext.base import BasicObject\n",
    "\n",
    "# CORPUSPath = 'corpus/ResumeEN/'\n",
    "\n",
    "# Corpus2GroupMethod = '.json'\n",
    "\n",
    "# Group2TextMethod   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 'whole'\n",
    "\n",
    "# Sent2TokenMethod = 'pos_en' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# min_token_freq = 1\n",
    "\n",
    "# use_hyper = ['pos_en']\n",
    "\n",
    "# anno = 'json_annotation'\n",
    "# anno_keywords = {\n",
    "#     'strText': 'content',\n",
    "#     'labels': 'annotation',\n",
    "# }\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, \n",
    "#                  Corpus2GroupMethod, \n",
    "#                  Group2TextMethod, \n",
    "#                  Text2SentMethod, \n",
    "#                  Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "#                  use_hyper = use_hyper, \n",
    "#                  anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPBA2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NLPBA2004/sampletest1.iob2\n",
      "corpus/NLPBA2004/Genia4ERtask1.iob2\n",
      "Total Num of All    Tokens 456790\n",
      "Total Num of Unique Tokens 20663\n",
      "CORPUS\tit is Dumped into file: data/NLPBA2004/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NLPBA2004/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16746\n",
      "SENT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16746\n",
      "TOKEN\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 456790\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NLPBA2004/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 20663\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NLPBA2004/'\n",
    "\n",
    "Corpus2GroupMethod = '.iob2'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': '\\t',\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    \"change_tags\": False, \n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aberrant activation of AP-1 by gp160 in CD4 positive T cells could result in up-regulation of cytokines containing AP-1 sites , e.g . interleukin-3 and granulocyte macrophage colony-stimulating factor , and concurrently lead to T cell unresponsiveness by inhibiting interleukin-2 secretion .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('aberrant', 'O'),\n",
       " ('activation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('AP-1', 'protein-B'),\n",
       " ('by', 'O'),\n",
       " ('gp160', 'protein-B'),\n",
       " ('in', 'O'),\n",
       " ('CD4', 'cell_type-B'),\n",
       " ('positive', 'cell_type-I'),\n",
       " ('T', 'cell_type-I'),\n",
       " ('cells', 'cell_type-I'),\n",
       " ('could', 'O'),\n",
       " ('result', 'O'),\n",
       " ('in', 'O'),\n",
       " ('up-regulation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('cytokines', 'protein-B'),\n",
       " ('containing', 'O'),\n",
       " ('AP-1', 'DNA-B'),\n",
       " ('sites', 'DNA-I'),\n",
       " (',', 'O'),\n",
       " ('e.g', 'O'),\n",
       " ('.', 'O'),\n",
       " ('interleukin-3', 'protein-B'),\n",
       " ('and', 'O'),\n",
       " ('granulocyte', 'protein-B'),\n",
       " ('macrophage', 'protein-I'),\n",
       " ('colony-stimulating', 'protein-I'),\n",
       " ('factor', 'protein-I'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('concurrently', 'O'),\n",
       " ('lead', 'O'),\n",
       " ('to', 'O'),\n",
       " ('T', 'cell_type-B'),\n",
       " ('cell', 'cell_type-I'),\n",
       " ('unresponsiveness', 'O'),\n",
       " ('by', 'O'),\n",
       " ('inhibiting', 'O'),\n",
       " ('interleukin-2', 'protein-B'),\n",
       " ('secretion', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16745)\n",
    "print(st.sentence)\n",
    "\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'protein': 30393,\n",
       " 'cell_type': 6774,\n",
       " 'DNA': 9539,\n",
       " 'cell_line': 3832,\n",
       " 'RNA': 966}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T13:16:16.948442Z",
     "start_time": "2019-09-23T13:16:16.287972Z"
    }
   },
   "outputs": [],
   "source": [
    "# from smart_open import smart_open\n",
    "\n",
    "\n",
    "# folderPath = 'corpus/MedPos/SegDataSheet.DB'\n",
    "\n",
    "# new_path = 'corpus/MedPos/New.txt'\n",
    "# with open(new_path, 'w', encoding='utf-8') as f1:\n",
    "#     with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "#         lastlineidx = 0\n",
    "#         lasttextidx = 0\n",
    "#         for line in f:\n",
    "#             # print(line)\n",
    "#             group_text_lineidx, text, label, start, end = line.replace('\\n', '').split('\\t')\n",
    "\n",
    "#             lineidx = int(group_text_lineidx.split('_')[-1])\n",
    "#             textidx = int(group_text_lineidx.split('_')[-2])\n",
    "#             # print(group_text_lineidx)\n",
    "#             # print(lineidx)\n",
    "#             # print(line, text, label, start, end)\n",
    "#             label = label.split('-')[0]\n",
    "#             for idx, char in enumerate(text):\n",
    "#                 suffix = '-B' if idx == 0 else '-I'\n",
    "#                 if lineidx != lastlineidx or textidx != lasttextidx:\n",
    "#                     lastlineidx = lineidx\n",
    "#                     lasttextidx = textidx\n",
    "#                     # print('\\n')\n",
    "#                     f1.write('\\n')\n",
    "#                 # print(char, label + suffix)\n",
    "#                 f1.write(char + ' ' + label + suffix + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.084885Z",
     "start_time": "2019-10-22T11:00:15.084526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/MedPos/New.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 326091\n",
      "Total Num of Unique Tokens 1597\n",
      "CORPUS\tit is Dumped into file: data/MedPos/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/MedPos/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/MedPos/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 7369\n",
      "SENT\tit is Dumped into file: data/MedPos/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 7369\n",
      "TOKEN\tit is Dumped into file: data/MedPos/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 326091\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/MedPos/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/MedPos/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/MedPos/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 129\n",
      "\t\tWrite to: data/MedPos/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/MedPos/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1597\n",
      "\t\tWrite to: data/MedPos/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/MedPos/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': True,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.097029Z",
     "start_time": "2019-10-22T11:00:33.087824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¹ é¨ å¹³ å¦ , æ  è¹ å£ é è æ² å¼  , æ  å ç ã å è·³ ç , æª è§¦ å å å ã'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 1804\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.102182Z",
     "start_time": "2019-10-22T11:00:33.098459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['f', 'Ã¹'],\n",
       " ['b', 'Ã¹'],\n",
       " ['p', 'Ã­ng'],\n",
       " ['t', 'Çn'],\n",
       " [','],\n",
       " ['w', 'Ãº'],\n",
       " ['f', 'Ã¹'],\n",
       " ['b', 'Ã¬'],\n",
       " ['j', 'Ã¬ng'],\n",
       " ['m', 'Ã ', 'i'],\n",
       " ['q', 'Å«'],\n",
       " ['zh', 'Äng'],\n",
       " [','],\n",
       " ['w', 'Ãº'],\n",
       " ['y', 'Ä'],\n",
       " ['t', 'Ã²ng'],\n",
       " ['ã'],\n",
       " ['f', 'Çn'],\n",
       " ['t', 'i', 'Ã ', 'o'],\n",
       " ['t', 'Ã²ng'],\n",
       " [','],\n",
       " ['w', 'Ã¨', 'i'],\n",
       " ['ch', 'Ã¹'],\n",
       " ['j', 'Ã­'],\n",
       " ['b', 'Ä', 'o'],\n",
       " ['k', 'u', 'Ã ', 'i'],\n",
       " ['ã']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_grain_str('pinyin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.107079Z",
     "start_time": "2019-10-22T11:00:33.103516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['èº«ä½é¨ä½-B'],\n",
       " ['ç©ºé´æ¦å¿µ-B'],\n",
       " ['å®æ§-B'],\n",
       " ['å®æ§-I'],\n",
       " ['æ ç¹-B'],\n",
       " ['æ -B'],\n",
       " ['èº«ä½é¨ä½-B'],\n",
       " ['èº«ä½é¨ä½-I'],\n",
       " ['èº«ä½é¨ä½-B'],\n",
       " ['èº«ä½é¨ä½-I'],\n",
       " ['ä½å¾ä¸çç¶-B'],\n",
       " ['ä½å¾ä¸çç¶-I'],\n",
       " ['æ ç¹-B'],\n",
       " ['æ -B'],\n",
       " ['ä½å¾ä¸çç¶-B'],\n",
       " ['ä½å¾ä¸çç¶-I'],\n",
       " ['è¿è¯-B'],\n",
       " ['ä½å¾ä¸çç¶-B'],\n",
       " ['ä½å¾ä¸çç¶-I'],\n",
       " ['ä½å¾ä¸çç¶-I'],\n",
       " ['æ ç¹-B'],\n",
       " ['æ -B'],\n",
       " ['æ-B'],\n",
       " ['æ-I'],\n",
       " ['ä½å¾ä¸çç¶-B'],\n",
       " ['ä½å¾ä¸çç¶-I'],\n",
       " ['æ ç¹-B']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_grain_str('annoE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.836720Z",
     "start_time": "2019-10-22T11:00:33.108355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'æ°å­': 14014,\n",
       " 'æ ç¹': 42448,\n",
       " 'äººç¾¤': 3059,\n",
       " 'å®æ§': 14508,\n",
       " 'ç¾ç': 3542,\n",
       " 'ä½å¾ä¸çç¶': 21261,\n",
       " 'å æ': 1359,\n",
       " 'è¿è¯': 8700,\n",
       " 'æ¶é´åä½': 3928,\n",
       " 'äºä»¶': 4442,\n",
       " 'èè¯': 3753,\n",
       " 'æ¶é´æ¦å¿µ': 4514,\n",
       " 'æ ': 13642,\n",
       " 'æ': 8745,\n",
       " 'èº«ä½ç©è´¨': 1103,\n",
       " 'èº«ä½é¨ä½': 19001,\n",
       " 'å°åæºæå': 2662,\n",
       " 'å»çè¡ä¸º': 3093,\n",
       " 'ç©ºé´æ¦å¿µ': 7626,\n",
       " 'æ£æ¥é¡¹ç®': 3200,\n",
       " 'æè®¸': 388,\n",
       " 'æ²»çé¡¹ç®': 1579,\n",
       " 'èº«ä½åè½': 5179,\n",
       " 'ä¸´åºå±æ§': 6003,\n",
       " 'æ°å­¦ç¬¦å·': 1098,\n",
       " 'åä½': 3763,\n",
       " 'è¯ç©': 728,\n",
       " 'ä»£è¯': 477,\n",
       " 'å»çä»ªå¨': 432,\n",
       " 'ç©ä½': 883,\n",
       " 'çç©': 150,\n",
       " 'notsure': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.840467Z",
     "start_time": "2019-10-22T11:00:33.837881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:51:29.486288Z",
     "start_time": "2019-10-24T02:51:29.480865Z"
    }
   },
   "outputs": [],
   "source": [
    "# [i for i in d]\n",
    "\n",
    "\n",
    "# a = [\n",
    "#     'çç©',\n",
    "#     'ç©ä½',\n",
    "#     'å°åæºæå',\n",
    "#     'äººç¾¤',\n",
    "#     'äºä»¶',\n",
    "    \n",
    "#     'èº«ä½é¨ä½',\n",
    "#     'èº«ä½ç©è´¨',\n",
    "#     'ä¸´åºå±æ§',\n",
    "#     'èº«ä½åè½',\n",
    "    \n",
    "    \n",
    "#     'å»çè¡ä¸º',\n",
    "#     'å»çä»ªå¨',\n",
    "#     'æ²»çé¡¹ç®',\n",
    "#     'æ£æ¥é¡¹ç®',\n",
    "#     'è¯ç©',\n",
    "    \n",
    "    \n",
    "    \n",
    "#     'æ¶é´æ¦å¿µ',\n",
    "#     'æ¶é´åä½',\n",
    "     \n",
    "#     'ç©ºé´æ¦å¿µ',\n",
    "#     'æ°å­¦ç¬¦å·',\n",
    "#     'æ°å­',\n",
    "#     'åä½',\n",
    " \n",
    "#     'ä½å¾ä¸çç¶',\n",
    "#     'ç¾ç',\n",
    "    \n",
    "    \n",
    "#     'æ ',\n",
    "#     'æ',\n",
    "#     'æè®¸',\n",
    "#     'å®æ§',\n",
    "    \n",
    "#     'æ ç¹',\n",
    "#     'ä»£è¯',\n",
    "#      'è¿è¯',\n",
    "#      'èè¯',\n",
    "#     'å æ',\n",
    "\n",
    "\n",
    " \n",
    "#  'notsure']\n",
    "\n",
    "# a\n",
    "\n",
    "# new_dict = [{'Tag': i, 'Num': d[i]} for i in a]\n",
    "# new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHIP2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T03:02:21.454746Z",
     "start_time": "2019-10-24T03:02:21.304176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n"
     ]
    }
   ],
   "source": [
    "from smart_open import smart_open\n",
    "\n",
    "\n",
    "folderPath = 'corpus/CHIP2019/train/train_data.txt'\n",
    "\n",
    "# new_path = 'corpus/MedPos/New.txt'\n",
    "All_Data = {}\n",
    "with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        try:\n",
    "            idx, category, sentence = line.replace('\\n', '').split('\\t')\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "        # print(category)\n",
    "        # print(sentence)\n",
    "        for bad_token in ['&gt; ', '&lt; ', '&gt;', '&lt;']:\n",
    "            if bad_token in sentence:\n",
    "                sentence = sentence.replace(bad_token, '')\n",
    "                print(bad_token)\n",
    "                \n",
    "        if category in All_Data:\n",
    "            All_Data[category].append(sentence)\n",
    "        else:\n",
    "            All_Data[category] = [sentence]\n",
    "                \n",
    "        \n",
    "import pickle\n",
    "\n",
    "# for category, data in All_Data.items():\n",
    "#     with open('clsdata/validation/' + category + '.p', 'wb') as handle:\n",
    "#         pickle.dump(data, handle)\n",
    "        \n",
    "\n",
    "for category, data in All_Data.items():\n",
    "    with open('clsdata/train/' + category + '.txt', 'w') as f:\n",
    "        f.write('\\n'.join(data))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T07:41:26.274526Z",
     "start_time": "2019-10-24T07:41:26.269378Z"
    }
   },
   "outputs": [],
   "source": [
    "# from smart_open import smart_open\n",
    "\n",
    "\n",
    "# folderPath = 'corpus/CHIP2019/validation/validation_data.txt'\n",
    "\n",
    "# # new_path = 'corpus/MedPos/New.txt'\n",
    "# All_Data = {}\n",
    "# with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "\n",
    "#     for line in f:\n",
    "#         # print(line)\n",
    "#         try:\n",
    "#             idx, category, sentence = line.replace('\\n', '').split('\\t')\n",
    "#         except:\n",
    "#             print(line)\n",
    "#             continue\n",
    "#         # print(category)\n",
    "#         # print(sentence)\n",
    "#         for bad_token in ['&gt; ', '&lt; ', '&gt;', '&lt;']:\n",
    "#             if bad_token in sentence:\n",
    "#                 sentence = sentence.replace(bad_token, '')\n",
    "#                 print(bad_token)\n",
    "                \n",
    "#         if category in All_Data:\n",
    "#             All_Data[category].append(sentence)\n",
    "#         else:\n",
    "#             All_Data[category] = [sentence]\n",
    "                \n",
    "        \n",
    "# import pickle\n",
    "\n",
    "# # for category, data in All_Data.items():\n",
    "# #     with open('clsdata/validation/' + category + '.p', 'wb') as handle:\n",
    "# #         pickle.dump(data, handle)\n",
    "        \n",
    "\n",
    "# for category, data in All_Data.items():\n",
    "#     with open('clsdata/validation/' + category + '.txt', 'w') as f:\n",
    "#         f.write('\\n'.join(data))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T07:42:06.642830Z",
     "start_time": "2019-10-24T07:41:43.209553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CHIP2019_new/train/Address.txt\n",
      "corpus/CHIP2019_new/train/Addictive Behavior.txt\n",
      "corpus/CHIP2019_new/train/Compliance with Protocol.txt\n",
      "corpus/CHIP2019_new/train/Symptom.txt\n",
      "corpus/CHIP2019_new/train/Blood Donation.txt\n",
      "corpus/CHIP2019_new/train/Non-Neoplasm Disease Stage.txt\n",
      "corpus/CHIP2019_new/train/Gender.txt\n",
      "corpus/CHIP2019_new/train/Sign.txt\n",
      "corpus/CHIP2019_new/train/Oral related.txt\n",
      "corpus/CHIP2019_new/train/Disease.txt\n",
      "corpus/CHIP2019_new/train/Smoking Status.txt\n",
      "corpus/CHIP2019_new/train/Researcher Decision.txt\n",
      "corpus/CHIP2019_new/train/Education.txt\n",
      "corpus/CHIP2019_new/train/Neoplasm Status.txt\n",
      "corpus/CHIP2019_new/train/Device.txt\n",
      "corpus/CHIP2019_new/train/Multiple.txt\n",
      "corpus/CHIP2019_new/train/Enrollment in other studies.txt\n",
      "corpus/CHIP2019_new/train/Therapy or Surgery.txt\n",
      "corpus/CHIP2019_new/train/Disabilities.txt\n",
      "corpus/CHIP2019_new/train/Life Expectancy.txt\n",
      "corpus/CHIP2019_new/train/Diagnostic.txt\n",
      "corpus/CHIP2019_new/train/Sexual related.txt\n",
      "corpus/CHIP2019_new/train/Special Patient Characteristic.txt\n",
      "corpus/CHIP2019_new/train/Encounter.txt\n",
      "corpus/CHIP2019_new/train/Bedtime.txt\n",
      "corpus/CHIP2019_new/train/Consent.txt\n",
      "corpus/CHIP2019_new/train/Alcohol Consumer.txt\n",
      "corpus/CHIP2019_new/train/Healthy.txt\n",
      "corpus/CHIP2019_new/train/Organ or Tissue Status.txt\n",
      "corpus/CHIP2019_new/train/Pharmaceutical Substance or Drug.txt\n",
      "corpus/CHIP2019_new/train/Age.txt\n",
      "corpus/CHIP2019_new/train/Laboratory Examinations.txt\n",
      "corpus/CHIP2019_new/train/Allergy Intolerance.txt\n",
      "corpus/CHIP2019_new/train/Ethical Audit.txt\n",
      "corpus/CHIP2019_new/train/Literacy.txt\n",
      "corpus/CHIP2019_new/train/Receptor Status.txt\n",
      "corpus/CHIP2019_new/train/Risk Assessment.txt\n",
      "corpus/CHIP2019_new/train/Capacity.txt\n",
      "corpus/CHIP2019_new/train/Exercise.txt\n",
      "corpus/CHIP2019_new/train/Diet.txt\n",
      "corpus/CHIP2019_new/train/Ethnicity.txt\n",
      "corpus/CHIP2019_new/train/Pregnancy-related Activity.txt\n",
      "corpus/CHIP2019_new/train/Data Accessible.txt\n",
      "corpus/CHIP2019_new/train/Nursing.txt\n",
      "corpus/CHIP2019_new/validation/Address.txt\n",
      "corpus/CHIP2019_new/validation/Addictive Behavior.txt\n",
      "corpus/CHIP2019_new/validation/Compliance with Protocol.txt\n",
      "corpus/CHIP2019_new/validation/Symptom.txt\n",
      "corpus/CHIP2019_new/validation/Blood Donation.txt\n",
      "corpus/CHIP2019_new/validation/Non-Neoplasm Disease Stage.txt\n",
      "corpus/CHIP2019_new/validation/Gender.txt\n",
      "corpus/CHIP2019_new/validation/Sign.txt\n",
      "corpus/CHIP2019_new/validation/Oral related.txt\n",
      "corpus/CHIP2019_new/validation/Disease.txt\n",
      "corpus/CHIP2019_new/validation/Smoking Status.txt\n",
      "corpus/CHIP2019_new/validation/Researcher Decision.txt\n",
      "corpus/CHIP2019_new/validation/Education.txt\n",
      "corpus/CHIP2019_new/validation/Neoplasm Status.txt\n",
      "corpus/CHIP2019_new/validation/Device.txt\n",
      "corpus/CHIP2019_new/validation/Multiple.txt\n",
      "corpus/CHIP2019_new/validation/Enrollment in other studies.txt\n",
      "corpus/CHIP2019_new/validation/Therapy or Surgery.txt\n",
      "corpus/CHIP2019_new/validation/Disabilities.txt\n",
      "corpus/CHIP2019_new/validation/Life Expectancy.txt\n",
      "corpus/CHIP2019_new/validation/Diagnostic.txt\n",
      "corpus/CHIP2019_new/validation/Sexual related.txt\n",
      "corpus/CHIP2019_new/validation/Special Patient Characteristic.txt\n",
      "corpus/CHIP2019_new/validation/Encounter.txt\n",
      "corpus/CHIP2019_new/validation/Bedtime.txt\n",
      "corpus/CHIP2019_new/validation/Consent.txt\n",
      "corpus/CHIP2019_new/validation/Alcohol Consumer.txt\n",
      "corpus/CHIP2019_new/validation/Healthy.txt\n",
      "corpus/CHIP2019_new/validation/Organ or Tissue Status.txt\n",
      "corpus/CHIP2019_new/validation/Pharmaceutical Substance or Drug.txt\n",
      "corpus/CHIP2019_new/validation/Age.txt\n",
      "corpus/CHIP2019_new/validation/Laboratory Examinations.txt\n",
      "corpus/CHIP2019_new/validation/Allergy Intolerance.txt\n",
      "corpus/CHIP2019_new/validation/Ethical Audit.txt\n",
      "corpus/CHIP2019_new/validation/Literacy.txt\n",
      "corpus/CHIP2019_new/validation/Receptor Status.txt\n",
      "corpus/CHIP2019_new/validation/Risk Assessment.txt\n",
      "corpus/CHIP2019_new/validation/Capacity.txt\n",
      "corpus/CHIP2019_new/validation/Exercise.txt\n",
      "corpus/CHIP2019_new/validation/Diet.txt\n",
      "corpus/CHIP2019_new/validation/Ethnicity.txt\n",
      "corpus/CHIP2019_new/validation/Pregnancy-related Activity.txt\n",
      "corpus/CHIP2019_new/validation/Data Accessible.txt\n",
      "corpus/CHIP2019_new/validation/Nursing.txt\n",
      "Total Num of All    Tokens 786482\n",
      "Total Num of Unique Tokens 2524\n",
      "CORPUS\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 88\n",
      "TEXT\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 30584\n",
      "SENT\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 30584\n",
      "TOKEN\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 786482\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/CHIP2019_new/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/CHIP2019_new/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/CHIP2019_new/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2524\n",
      "\t\tWrite to: data/CHIP2019_new/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CHIP2019_new/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCKS2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:46:03.419303Z",
     "start_time": "2019-10-16T06:45:50.264010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/CCKS2017/çå²ç¹ç¹\n",
      "corpus/CCKS2017/åºé¢æåµ\n",
      "corpus/CCKS2017/ä¸è¬é¡¹ç®\n",
      "corpus/CCKS2017/è¯çç»è¿\n",
      "Total Num of All    Tokens 260781\n",
      "Total Num of Unique Tokens 1731\n",
      "CORPUS\tit is Dumped into file: data/CCKS2017/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CCKS2017/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 4\n",
      "TEXT\tit is Dumped into file: data/CCKS2017/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1198\n",
      "SENT\tit is Dumped into file: data/CCKS2017/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 8644\n",
      "TOKEN\tit is Dumped into file: data/CCKS2017/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 260781\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/CCKS2017/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CCKS2017/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/CCKS2017/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1731\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CCKS2017/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 1, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:12.363856Z",
     "start_time": "2019-10-16T06:47:12.358482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ° ç®¡ å± ä¸­ , ç² ç¶ èº æ  è¿ å¤§ ã'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 100\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:26.230559Z",
     "start_time": "2019-10-16T06:47:26.221984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['èº«ä½é¨ä½-B',\n",
       " 'èº«ä½é¨ä½-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'èº«ä½é¨ä½-B',\n",
       " 'èº«ä½é¨ä½-I',\n",
       " 'èº«ä½é¨ä½-E',\n",
       " 'O',\n",
       " 'çç¶åä½å¾-B',\n",
       " 'çç¶åä½å¾-E',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:35.744475Z",
     "start_time": "2019-10-16T06:47:35.145054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'æ²»ç': 1048, 'èº«ä½é¨ä½': 10719, 'çç¶åä½å¾': 7830, 'ç¾çåè¯æ­': 722, 'æ£æ¥åæ£éª': 9546}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:52:12.993792Z",
     "start_time": "2019-10-16T06:51:57.523457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/LuohuNER/Digestion\n",
      "corpus/LuohuNER/Cardiovascular\n",
      "corpus/LuohuNER/Urinary\n",
      "corpus/LuohuNER/Respiratory\n",
      "corpus/LuohuNER/Gynecology\n",
      "Total Num of All    Tokens 324362\n",
      "Total Num of Unique Tokens 1596\n",
      "CORPUS\tit is Dumped into file: data/LuohuNER/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuNER/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/LuohuNER/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/LuohuNER/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 8663\n",
      "TOKEN\tit is Dumped into file: data/LuohuNER/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 324362\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuNER/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/LuohuNER/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuNER/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1596\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuNER/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 0, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu NER 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T12:42:36.999801Z",
     "start_time": "2019-10-22T12:42:11.936559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/LuohuNER750Neat/å¼å¸åç§\n",
      "corpus/LuohuNER750Neat/æ¶ååç§\n",
      "corpus/LuohuNER750Neat/å¦ä¸ç§\n",
      "corpus/LuohuNER750Neat/æ³å°¿å¤ç§\n",
      "corpus/LuohuNER750Neat/å¿è¡ç®¡åç§\n",
      "Total Num of All    Tokens 598039\n",
      "Total Num of Unique Tokens 1966\n",
      "CORPUS\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 749\n",
      "SENT\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16479\n",
      "TOKEN\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 598039\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuNER750Neat/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/LuohuNER750Neat/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuNER750Neat/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1966\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuNER750Neat/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 0, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T12:42:38.177913Z",
     "start_time": "2019-10-22T12:42:37.002244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'å¼å¸¸': 39465, 'å¼å¸¸åç±»': 7863, 'ä¸»ä½': 28079, 'æ£æ¥': 25404, 'æ²»ç': 7726}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Handle Labels\n",
    "\n",
    "## strText and SSET\n",
    "\n",
    "For each text, we must get a strText and SSET, which meet the following restriction.\n",
    "\n",
    "Besides, we also need a strSents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»è å¤åæ¯èã\n",
      "æ£ä¸­èå¹´ç·æ§,æ¢æ§çç¨ã å âä½æ£åç°å¤§è å¤åæ¯è3æä½âå¥é¢ãæ¥ä½:æ é³æ§ä½å¾ã\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('å¤åæ¯è', 2, 6, 'ç¾ç'),\n",
       " ('æ¢æ§', 15, 17, 'ä¿®é¥°'),\n",
       " ('å¤åæ¯è', 29, 33, 'ç¾ç'),\n",
       " ('3æä½', 33, 36, 'ä¿®é¥°'),\n",
       " ('æ é³æ§ä½å¾', 43, 48, 'ä¸ç¡®å®')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''ç»è å¤åæ¯èã\\næ£ä¸­èå¹´ç·æ§,æ¢æ§çç¨ã å âä½æ£åç°å¤§è å¤åæ¯è3æä½âå¥é¢ãæ¥ä½:æ é³æ§ä½å¾ã'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''æ æ³¨ææ¬åç§°:/Users/zhangling/Documents/æ°æ çæ°æ®530/529æé-å·²æ£æ¥/Entity/patient4378.txt\\næ æ³¨ææ¬å­æ°ç»è®¡:87\\nå¤åæ¯è\\t3\\t6\\tç¾ç\\næ¢æ§\\t16\\t17\\tä¿®é¥°\\nå¤åæ¯è\\t30\\t33\\tç¾ç\\n3æä½\\t34\\t36\\tä¿®é¥°\\næ é³æ§ä½å¾\\t44\\t48\\tä¸ç¡®å®\\n'''\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "\n",
    "notZeroIndex = 1 \n",
    "for idx, sset in enumerate(SSETText):\n",
    "    data = (sset[0], int(sset[1]) - notZeroIndex, int(sset[2]), sset[-1])\n",
    "    SSETText[idx] = data\n",
    "\n",
    "\n",
    "# check\n",
    "for sset in SSETText:\n",
    "    try:\n",
    "        assert strText[sset[1]: sset[2]] == sset[0]\n",
    "    except:\n",
    "        print('strText:', strText[sset[1] : sset[2]])\n",
    "        print('SSETText:', sset[0])\n",
    "        \n",
    "SSETText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ç»', 0, 'O'],\n",
       " ['è ', 1, 'O'],\n",
       " ['å¤', 2, 'ç¾ç-B'],\n",
       " ['å', 3, 'ç¾ç-I'],\n",
       " ['æ¯', 4, 'ç¾ç-I'],\n",
       " ['è', 5, 'ç¾ç-E'],\n",
       " ['ã', 6, 'O'],\n",
       " ['\\n', 7, 'O'],\n",
       " ['æ£', 8, 'O'],\n",
       " ['ä¸­', 9, 'O'],\n",
       " ['è', 10, 'O'],\n",
       " ['å¹´', 11, 'O'],\n",
       " ['ç·', 12, 'O'],\n",
       " ['æ§', 13, 'O'],\n",
       " [',', 14, 'O'],\n",
       " ['æ¢', 15, 'ä¿®é¥°-B'],\n",
       " ['æ§', 16, 'ä¿®é¥°-E'],\n",
       " ['ç', 17, 'O'],\n",
       " ['ç¨', 18, 'O'],\n",
       " ['ã', 19, 'O'],\n",
       " [' ', 20, 'O'],\n",
       " ['å ', 21, 'O'],\n",
       " ['â', 22, 'O'],\n",
       " ['ä½', 23, 'O'],\n",
       " ['æ£', 24, 'O'],\n",
       " ['å', 25, 'O'],\n",
       " ['ç°', 26, 'O'],\n",
       " ['å¤§', 27, 'O'],\n",
       " ['è ', 28, 'O'],\n",
       " ['å¤', 29, 'ç¾ç-B'],\n",
       " ['å', 30, 'ç¾ç-I'],\n",
       " ['æ¯', 31, 'ç¾ç-I'],\n",
       " ['è', 32, 'ç¾ç-E'],\n",
       " ['3', 33, 'ä¿®é¥°-B'],\n",
       " ['æ', 34, 'ä¿®é¥°-I'],\n",
       " ['ä½', 35, 'ä¿®é¥°-E'],\n",
       " ['â', 36, 'O'],\n",
       " ['å¥', 37, 'O'],\n",
       " ['é¢', 38, 'O'],\n",
       " ['ã', 39, 'O'],\n",
       " ['æ¥', 40, 'O'],\n",
       " ['ä½', 41, 'O'],\n",
       " [':', 42, 'O'],\n",
       " ['æ ', 43, 'ä¸ç¡®å®-B'],\n",
       " ['é³', 44, 'ä¸ç¡®å®-I'],\n",
       " ['æ§', 45, 'ä¸ç¡®å®-I'],\n",
       " ['ä½', 46, 'ä¸ç¡®å®-I'],\n",
       " ['å¾', 47, 'ä¸ç¡®å®-E'],\n",
       " ['ã', 48, 'O']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''ç»è å¤åæ¯èã\\næ£ä¸­èå¹´ç·æ§,æ¢æ§çç¨ã å âä½æ£åç°å¤§è å¤åæ¯è3æä½âå¥é¢ãæ¥ä½:æ é³æ§ä½å¾ã'''\n",
    "\n",
    "\n",
    "SSETText = [ ('å¤åæ¯è', 2, 6, 'ç¾ç'),\n",
    "             ('æ¢æ§', 15, 17, 'ä¿®é¥°'),\n",
    "             ('å¤åæ¯è', 29, 33, 'ç¾ç'),\n",
    "             ('3æä½', 33, 36, 'ä¿®é¥°'),\n",
    "             ('æ é³æ§ä½å¾', 43, 48, 'ä¸ç¡®å®')]\n",
    "\n",
    "def getCITText(strText, SSETText, TOKENLevel='char'):\n",
    "    len(SSETText) > 0 \n",
    "    if TOKENLevel == 'char':\n",
    "        for sset in SSETText:\n",
    "            try:\n",
    "                assert strText[sset[1]: sset[2]] == sset[0]\n",
    "            except:\n",
    "                print('strText:', strText[sset[1] : sset[2]])\n",
    "                print('SSETText:', sset[0])\n",
    "        CITAnnoText = []\n",
    "        for sset in SSETText:\n",
    "            # BIOES\n",
    "            strAnno, s, e, tag = sset\n",
    "            CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "            CIT[-1][2] = tag + '-E'\n",
    "            CIT[ 0][2] = tag + '-B'\n",
    "            if len(CIT) == 1:\n",
    "                CIT[0][2] = tag + '-S' \n",
    "            CITAnnoText.extend(CIT)\n",
    "\n",
    "        # print(strAnnoText)\n",
    "        CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "        for citAnno in CITAnnoText:\n",
    "            c, idx, t = citAnno\n",
    "            assert CITText[idx][0] == c\n",
    "            CITText[idx] = citAnno\n",
    "\n",
    "    elif TOKENLevel == 'word':\n",
    "        CITText = []\n",
    "        for idx, sset in enumerate(SSETText):\n",
    "            try:\n",
    "                assert sset[0] == strText[idx]\n",
    "            except:\n",
    "                print(strText)[idx]\n",
    "                print(sset[0])\n",
    "\n",
    "            CITText.append(sset)\n",
    "    return CITText\n",
    "\n",
    "\n",
    "CITText = getCITText(strText, SSETText, TOKENLevel='char')\n",
    "CITText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ç»', 0, 'O'],\n",
       "  ['è ', 1, 'O'],\n",
       "  ['å¤', 2, 'ç¾ç-B'],\n",
       "  ['å', 3, 'ç¾ç-I'],\n",
       "  ['æ¯', 4, 'ç¾ç-I'],\n",
       "  ['è', 5, 'ç¾ç-E'],\n",
       "  ['ã', 6, 'O']],\n",
       " [['æ£', 0, 'O'],\n",
       "  ['ä¸­', 1, 'O'],\n",
       "  ['è', 2, 'O'],\n",
       "  ['å¹´', 3, 'O'],\n",
       "  ['ç·', 4, 'O'],\n",
       "  ['æ§', 5, 'O'],\n",
       "  [',', 6, 'O'],\n",
       "  ['æ¢', 7, 'ä¿®é¥°-B'],\n",
       "  ['æ§', 8, 'ä¿®é¥°-E'],\n",
       "  ['ç', 9, 'O'],\n",
       "  ['ç¨', 10, 'O'],\n",
       "  ['ã', 11, 'O']],\n",
       " [['å ', 0, 'O'],\n",
       "  ['â', 1, 'O'],\n",
       "  ['ä½', 2, 'O'],\n",
       "  ['æ£', 3, 'O'],\n",
       "  ['å', 4, 'O'],\n",
       "  ['ç°', 5, 'O'],\n",
       "  ['å¤§', 6, 'O'],\n",
       "  ['è ', 7, 'O'],\n",
       "  ['å¤', 8, 'ç¾ç-B'],\n",
       "  ['å', 9, 'ç¾ç-I'],\n",
       "  ['æ¯', 10, 'ç¾ç-I'],\n",
       "  ['è', 11, 'ç¾ç-E'],\n",
       "  ['3', 12, 'ä¿®é¥°-B'],\n",
       "  ['æ', 13, 'ä¿®é¥°-I'],\n",
       "  ['ä½', 14, 'ä¿®é¥°-E'],\n",
       "  ['â', 15, 'O'],\n",
       "  ['å¥', 16, 'O'],\n",
       "  ['é¢', 17, 'O'],\n",
       "  ['ã', 18, 'O']],\n",
       " [['æ¥', 0, 'O'],\n",
       "  ['ä½', 1, 'O'],\n",
       "  [':', 2, 'O'],\n",
       "  ['æ ', 3, 'ä¸ç¡®å®-B'],\n",
       "  ['é³', 4, 'ä¸ç¡®å®-I'],\n",
       "  ['æ§', 5, 'ä¸ç¡®å®-I'],\n",
       "  ['ä½', 6, 'ä¸ç¡®å®-I'],\n",
       "  ['å¾', 7, 'ä¸ç¡®å®-E'],\n",
       "  ['ã', 8, 'O']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strSents = ['ç»è å¤åæ¯èã', 'æ£ä¸­èå¹´ç·æ§,æ¢æ§çç¨ã', 'å âä½æ£åç°å¤§è å¤åæ¯è3æä½âå¥é¢ã', 'æ¥ä½:æ é³æ§ä½å¾ã']\n",
    "\n",
    "CITText  =  [['ç»', 0, 'O'],\n",
    "             ['è ', 1, 'O'],\n",
    "             ['å¤', 2, 'ç¾ç-B'],\n",
    "             ['å', 3, 'ç¾ç-I'],\n",
    "             ['æ¯', 4, 'ç¾ç-I'],\n",
    "             ['è', 5, 'ç¾ç-E'],\n",
    "             ['ã', 6, 'O'],\n",
    "             ['\\n', 7, 'O'],\n",
    "             ['æ£', 8, 'O'],\n",
    "             ['ä¸­', 9, 'O'],\n",
    "             ['è', 10, 'O'],\n",
    "             ['å¹´', 11, 'O'],\n",
    "             ['ç·', 12, 'O'],\n",
    "             ['æ§', 13, 'O'],\n",
    "             [',', 14, 'O'],\n",
    "             ['æ¢', 15, 'ä¿®é¥°-B'],\n",
    "             ['æ§', 16, 'ä¿®é¥°-E'],\n",
    "             ['ç', 17, 'O'],\n",
    "             ['ç¨', 18, 'O'],\n",
    "             ['ã', 19, 'O'],\n",
    "             [' ', 20, 'O'],\n",
    "             ['å ', 21, 'O'],\n",
    "             ['â', 22, 'O'],\n",
    "             ['ä½', 23, 'O'],\n",
    "             ['æ£', 24, 'O'],\n",
    "             ['å', 25, 'O'],\n",
    "             ['ç°', 26, 'O'],\n",
    "             ['å¤§', 27, 'O'],\n",
    "             ['è ', 28, 'O'],\n",
    "             ['å¤', 29, 'ç¾ç-B'],\n",
    "             ['å', 30, 'ç¾ç-I'],\n",
    "             ['æ¯', 31, 'ç¾ç-I'],\n",
    "             ['è', 32, 'ç¾ç-E'],\n",
    "             ['3', 33, 'ä¿®é¥°-B'],\n",
    "             ['æ', 34, 'ä¿®é¥°-I'],\n",
    "             ['ä½', 35, 'ä¿®é¥°-E'],\n",
    "             ['â', 36, 'O'],\n",
    "             ['å¥', 37, 'O'],\n",
    "             ['é¢', 38, 'O'],\n",
    "             ['ã', 39, 'O'],\n",
    "             ['æ¥', 40, 'O'],\n",
    "             ['ä½', 41, 'O'],\n",
    "             [':', 42, 'O'],\n",
    "             ['æ ', 43, 'ä¸ç¡®å®-B'],\n",
    "             ['é³', 44, 'ä¸ç¡®å®-I'],\n",
    "             ['æ§', 45, 'ä¸ç¡®å®-I'],\n",
    "             ['ä½', 46, 'ä¸ç¡®å®-I'],\n",
    "             ['å¾', 47, 'ä¸ç¡®å®-E'],\n",
    "             ['ã', 48, 'O']]\n",
    "\n",
    "def getCITSents(strSents, CITText):\n",
    "    lenLastSent = 0\n",
    "    collapse    = 0 # don't need to move \n",
    "    CITSents = []\n",
    "    for strSent in strSents:\n",
    "        CITSent = []\n",
    "        for sentTokenIdx, c in enumerate(strSent):\n",
    "            # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            while c != cT and c != ' ':\n",
    "                collapse = collapse + 1\n",
    "                txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                cT, _, tT = CITText[txtTokenIdx]\n",
    "            CITSent.append([c,sentTokenIdx, tT])\n",
    "        lenLastSent = lenLastSent + len(strSent)\n",
    "        CITSents.append(CITSent)\n",
    "    # CITSents\n",
    "    # Here we get CITSents  \n",
    "    return CITSents\n",
    "       \n",
    "    \n",
    "CITSents = getCITSents(strSents, CITText)\n",
    "CITSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSSET_from_CIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ç»', 0, 'O'], ['è ', 1, 'O'], ['å¤', 2, 'ç¾ç-B'], ['å', 3, 'ç¾ç-I'], ['æ¯', 4, 'ç¾ç-I'], ['è', 5, 'ç¾ç-E'], ['ã', 6, 'O']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'ç¾ç-B', 'ç¾ç-I', 'ç¾ç-I', 'ç¾ç-E', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CITSent = CITSents[0]\n",
    "print(CITSent)\n",
    "tag_seq = [i[-1] for i in CITSent]\n",
    "tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSET_from_CIT(orig_seq = None, tag_seq = None, CIT = None, tag_seq_tagScheme = 'BIO', join_char = ''):\n",
    "    # orig_seq is sentence without start or end\n",
    "    # tag_seq may have start or end\n",
    "        \n",
    "    tagScheme = tag_seq_tagScheme\n",
    "    if tagScheme == 'BIOES':\n",
    "        tag_seq = [i.replace('-S', '-B').replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOE':\n",
    "        tag_seq = [i.replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOS':\n",
    "        tag_seq = [i.replace('-S', '-B') for i in tag_seq]\n",
    "    elif tagScheme == 'BIO':\n",
    "        pass\n",
    "    else:\n",
    "        print('The tagScheme', tagScheme, 'is not supported yet...')\n",
    "    \n",
    "    if not CIT:\n",
    "        # use BIO tagScheme\n",
    "        CIT = list(zip(orig_seq, range(len(orig_seq)), tag_seq))\n",
    "    taggedCIT = [cit for cit in CIT if cit[2]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedCIT)) if taggedCIT[idx][2][-2:] == '-B']\n",
    "    startIdx.append(len(taggedCIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedCIT[startIdx[i]: startIdx[i+1]]\n",
    "        string = join_char.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][1], entityAtom[-1][1] + 1\n",
    "        tag = entityAtom[0][2].split('-')[0]\n",
    "        entitiesList.append((string, start, end, tag))\n",
    "    return entitiesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get strText, strSents, and SSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
