{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.715 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "\n",
    "### Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CORPUSPath': 'corpus/wiki_cn_sample/',\n",
       " 'Data_Dir': 'data/wiki_cn_sample/char',\n",
       " 'EndIDXGroups': array([1]),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corpus2GroupMethod': '.txt',\n",
       " 'GroupType': 'File',\n",
       " 'group_names': ['corpus/wiki_cn_sample/sample_wiki_smp.txt'],\n",
       " 'EndIDXTexts': array([100]),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group2TextMethod': 'line',\n",
       " 'EndIDXSents': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  133,   309,   491,   681,   885,   989,  1082,  1139,  1239,\n",
       "         1383,  1571,  1890,  1972,  2121,  2321,  2505,  2737,  2942,\n",
       "         3156,  3335,  3604,  3936,  4119,  4146,  4241,  4307,  4421,\n",
       "         4599,  4777,  4817,  4838,  4906,  5262,  5511,  5718,  5783,\n",
       "         5993,  6284,  6420,  6543,  6723,  6738,  6791,  6876,  6936,\n",
       "         7002,  7076,  7114,  7143,  7203,  7313,  7467,  7594,  7755,\n",
       "         7878,  8165,  8214,  8396,  8607,  8824,  9009,  9197,  9253,\n",
       "         9347,  9487,  9624,  9675,  9953, 10027, 10241, 10479, 10634,\n",
       "        10868, 10939, 11167, 11249, 11257, 11446, 11515, 11633, 11904,\n",
       "        12068, 12144, 12261, 12525, 12603, 12754, 12878, 12895, 12947,\n",
       "        13070, 13099, 13153, 13246, 13365, 13456, 13540, 13616, 13813,\n",
       "        13878]),\n",
       " 'data/wiki_cn_sample/char/Pyramid/_file/token.txt': array([  526,  1212,  1924,  2632,  3412,  3770,  4136,  4362,  4754,\n",
       "         5316,  6002,  7246,  7568,  8148,  8928,  9658, 10572, 11374,\n",
       "        12184, 12871, 13898, 15198, 15912, 16018, 16390, 16642, 17092,\n",
       "        17790, 18458, 18614, 18696, 18936, 20317, 21295, 22107, 22355,\n",
       "        23183, 24282, 24812, 25236, 25901, 25959, 26165, 26499, 26737,\n",
       "        26997, 27273, 27423, 27539, 27777, 28206, 28812, 29294, 29922,\n",
       "        30406, 31536, 31726, 32442, 33266, 34122, 34852, 35594, 35810,\n",
       "        36178, 36728, 37268, 37466, 38555, 38845, 39689, 40627, 41240,\n",
       "        42153, 42433, 43335, 43659, 43689, 44409, 44681, 45148, 46208,\n",
       "        46853, 47153, 47612, 48645, 48948, 49540, 49986, 50052, 50258,\n",
       "        50740, 50852, 51064, 51428, 51890, 52248, 52576, 52874, 53644,\n",
       "        53892]),\n",
       " 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt': array([  461,  1073,  1702,  2383,  3104,  3477,  3799,  3992,  4343,\n",
       "         4831,  5459,  6556,  6840,  7352,  8033,  8660,  9455, 10147,\n",
       "        10893, 11518, 12439, 13583, 14207, 14301, 14620, 14833, 15228,\n",
       "        15845, 16459, 16603, 16676, 16912, 18165, 19019, 19710, 19929,\n",
       "        20671, 21662, 22120, 22548, 23173, 23224, 23407, 23704, 23910,\n",
       "        24136, 24393, 24518, 24614, 24821, 25202, 25728, 26176, 26745,\n",
       "        27188, 28185, 28360, 28998, 29742, 30491, 31150, 31795, 31985,\n",
       "        32316, 32818, 33300, 33476, 34472, 34727, 35458, 36296, 36844,\n",
       "        37643, 37890, 38664, 38959, 38986, 39638, 39868, 40276, 41221,\n",
       "        41791, 42043, 42448, 43370, 43644, 44163, 44575, 44634, 44825,\n",
       "        45260, 45363, 45550, 45874, 46288, 46591, 46879, 47144, 47825,\n",
       "        48059]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'iter',\n",
       " 'TOKENLevel': 'char',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki_cn_sample/char/Pyramid/_file/token.txt',\n",
       "  'pos': 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt'},\n",
       " 'length': 13878}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki_cn_sample/char/Pyramid/_file/token.txt',\n",
       " 'pos': 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简', '塞', '拿', '济', '医', '幅', '韶', '狭', '功', '综', '癸', '喧', '势', '趋', '享', '钻', '尘', '嚣', '走', '司', '僧', '净', '父', '迪', '穆', '迈', '跟', '替', '麦', '品', '择', '魔', '占', '星', '燃', '隐', '藏', '收', '获', '控', '剧', '衰', '参', '廉', '邓', '破', '师', '乱', '死', '岭', '水', '钱', '赚', '旅', '瑟', '惑', '伸', '财', '脚', '坏', '案', '顺', '准', '渐', '令', '闭', '余', '逃', '珊', '王', '陆', '罕', '排', '斥', '野', '蛮', '黄', '媲', '打', '涅', '辛', '割', '赞', '板', '双', '翰', '护', '厘', '故', '贡', '刺', '讽', '企', '束', '梭', '卢', '宣', '拒', '伏', '恒', '逾', '官', '折', 'I', '规', '凭', '予', '奴', '仆', '浪', '漫', '‘', '’', '桑', '套', '民', '倡', '朽', '剂', '催', '鉴', '徒', '遭', '玄', '霍', '晋', '魏', '秀', '莫', '央', '族', '追', '韩', '愈', '翱', '驱', '颐', '框', '巫', '绪', '摆', '突', '鲜', '巨', '干', '服', '鼻', '祖', '诘', '诉', '圆', '码', '免', '避', '好', '胚', '撑', '迅', '握', '径', '傅', '妙', '优', '爆', '途', '弦', '错', '仔', '蕴', '涵', '叙', '坦', '供', 'Z', '拟', '轻', '芬', '低', '估', '忽', '装', '促', '紧', '洞', '路', '光', '遗', 'á', 'ó', '六', '委', '撰', '忠', '拾', '五', '耕', '杜', '钥', '迁', '柯', '轨', '衡', '短', '季', '乘', '石', '碑', '泥', '税', '丰', '惠', 'M', 'B', 'v', 'r', 'u', '月', '籍', '置', '递', '悉', '阴', ']', '[', '坑', '焚', '声', '阳', '农', '详', '旃', '拘', '婆', '钦', '翅', '摩', '诗', '歌', '颖', '佐', '雏', '末', '张', '载', '均', '秩', '偶', '左', '右', '男', '女', '静', '曲', '駄', '笩', '牟', '沌', '孪', '八', '{', 'w', '}', '偏', '邻', '勾', '纤', '丛', '俄', '凯', '冈', '诞', '硬', '吠', '媒', '介', '熵', '朋', '友', '辨', '辞', '器', '适', '纲', '叫', '南', '北', '须', '恼']\n"
     ]
    }
   ],
   "source": [
    "print(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([671, 441, 409, ...,   1,   1,   1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.idx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.min_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.current_vocab_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.original_vocab_token_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization from Saved NLPText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tread from pickle file : data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tread from pickle file : data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "Data_Dir = 'data/wiki_cn_sample/char/'\n",
    "min_token_freq = 2\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq = min_token_freq)\n",
    "print(len(BasicObject.TokenVocab[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CORPUSPath': 'corpus/wiki_cn_sample/',\n",
       " 'Data_Dir': 'data/wiki_cn_sample/char',\n",
       " 'EndIDXGroups': array([1]),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corpus2GroupMethod': '.txt',\n",
       " 'GroupType': 'File',\n",
       " 'group_names': ['corpus/wiki_cn_sample/sample_wiki_smp.txt'],\n",
       " 'EndIDXTexts': array([100]),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group2TextMethod': 'line',\n",
       " 'EndIDXSents': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  133,   309,   491,   681,   885,   989,  1082,  1139,  1239,\n",
       "         1383,  1571,  1890,  1972,  2121,  2321,  2505,  2737,  2942,\n",
       "         3156,  3335,  3604,  3936,  4119,  4146,  4241,  4307,  4421,\n",
       "         4599,  4777,  4817,  4838,  4906,  5262,  5511,  5718,  5783,\n",
       "         5993,  6284,  6420,  6543,  6723,  6738,  6791,  6876,  6936,\n",
       "         7002,  7076,  7114,  7143,  7203,  7313,  7467,  7594,  7755,\n",
       "         7878,  8165,  8214,  8396,  8607,  8824,  9009,  9197,  9253,\n",
       "         9347,  9487,  9624,  9675,  9953, 10027, 10241, 10479, 10634,\n",
       "        10868, 10939, 11167, 11249, 11257, 11446, 11515, 11633, 11904,\n",
       "        12068, 12144, 12261, 12525, 12603, 12754, 12878, 12895, 12947,\n",
       "        13070, 13099, 13153, 13246, 13365, 13456, 13540, 13616, 13813,\n",
       "        13878]),\n",
       " 'data/wiki_cn_sample/char/Pyramid/_file/token.txt': array([  526,  1212,  1924,  2632,  3412,  3770,  4136,  4362,  4754,\n",
       "         5316,  6002,  7246,  7568,  8148,  8928,  9658, 10572, 11374,\n",
       "        12184, 12871, 13898, 15198, 15912, 16018, 16390, 16642, 17092,\n",
       "        17790, 18458, 18614, 18696, 18936, 20317, 21295, 22107, 22355,\n",
       "        23183, 24282, 24812, 25236, 25901, 25959, 26165, 26499, 26737,\n",
       "        26997, 27273, 27423, 27539, 27777, 28206, 28812, 29294, 29922,\n",
       "        30406, 31536, 31726, 32442, 33266, 34122, 34852, 35594, 35810,\n",
       "        36178, 36728, 37268, 37466, 38555, 38845, 39689, 40627, 41240,\n",
       "        42153, 42433, 43335, 43659, 43689, 44409, 44681, 45148, 46208,\n",
       "        46853, 47153, 47612, 48645, 48948, 49540, 49986, 50052, 50258,\n",
       "        50740, 50852, 51064, 51428, 51890, 52248, 52576, 52874, 53644,\n",
       "        53892]),\n",
       " 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt': array([  461,  1073,  1702,  2383,  3104,  3477,  3799,  3992,  4343,\n",
       "         4831,  5459,  6556,  6840,  7352,  8033,  8660,  9455, 10147,\n",
       "        10893, 11518, 12439, 13583, 14207, 14301, 14620, 14833, 15228,\n",
       "        15845, 16459, 16603, 16676, 16912, 18165, 19019, 19710, 19929,\n",
       "        20671, 21662, 22120, 22548, 23173, 23224, 23407, 23704, 23910,\n",
       "        24136, 24393, 24518, 24614, 24821, 25202, 25728, 26176, 26745,\n",
       "        27188, 28185, 28360, 28998, 29742, 30491, 31150, 31795, 31985,\n",
       "        32316, 32818, 33300, 33476, 34472, 34727, 35458, 36296, 36844,\n",
       "        37643, 37890, 38664, 38959, 38986, 39638, 39868, 40276, 41221,\n",
       "        41791, 42043, 42448, 43370, 43644, 44163, 44575, 44634, 44825,\n",
       "        45260, 45363, 45550, 45874, 46288, 46591, 46879, 47144, 47825,\n",
       "        48059]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'iter',\n",
       " 'TOKENLevel': 'char',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki_cn_sample/char/Pyramid/_file/token.txt',\n",
       "  'pos': 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt'},\n",
       " 'length': 13878}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki_cn_sample/char/Pyramid/_file/token.txt',\n",
       " 'pos': 'data/wiki_cn_sample/char/Pyramid/_file/pos-bioes.txt'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简']\n"
     ]
    }
   ],
   "source": [
    "print(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([671, 441, 409, 321, 225, 164, 164, 157, 152, 150, 146, 134, 133,\n",
       "       115, 107,  97,  88,  78,  78,  77,  76,  71,  71,  67,  65,  64,\n",
       "        64,  64,  64,  64,  63,  62,  61,  60,  59,  58,  57,  56,  56,\n",
       "        56,  55,  54,  53,  53,  53,  52,  52,  52,  52,  51,  51,  51,\n",
       "        50,  49,  49,  48,  48,  47,  47,  46,  44,  44,  44,  43,  43,\n",
       "        43,  42,  42,  42,  42,  42,  41,  40,  40,  40,  40,  39,  39,\n",
       "        38,  38,  38,  37,  37,  37,  37,  37,  36,  36,  36,  36,  35,\n",
       "        35,  34,  34,  34,  34,  34,  33,  33,  33,  33,  33,  33,  33,\n",
       "        32,  32,  32,  32,  31,  31,  30,  30,  30,  29,  29,  29,  29,\n",
       "        29,  29,  29,  29,  28,  28,  28,  27,  27,  27,  27,  26,  26,\n",
       "        26,  26,  26,  26,  26,  25,  25,  25,  25,  25,  25,  25,  24,\n",
       "        24,  24,  24,  24,  24,  24,  23,  23,  23,  23,  23,  23,  22,\n",
       "        22,  22,  22,  22,  22,  22,  21,  21,  21,  21,  21,  21,  21,\n",
       "        21,  21,  20,  20,  20,  20,  20,  20,  20,  19,  19,  19,  19,\n",
       "        19,  19,  19,  19,  19,  19,  19,  18,  18,  18,  18,  18,  18,\n",
       "        18,  18,  18,  18,  18,  17,  17,  17,  17,  17,  17,  16,  16,\n",
       "        16,  16,  16,  16,  16,  16,  16,  16,  15,  15,  15,  15,  15,\n",
       "        15,  15,  15,  15,  15,  14,  14,  14,  14,  14,  14,  14,  14,\n",
       "        14,  14,  14,  14,  14,  13,  13,  13,  13,  13,  13,  13,  13,\n",
       "        13,  13,  13,  13,  13,  13,  13,  13,  13,  12,  12,  12,  12,\n",
       "        12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,\n",
       "        12,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "        11,  11,  11,  11,  11,  11,  11,  10,  10,  10,  10,  10,  10,\n",
       "        10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
       "        10,  10,  10,  10,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "         9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "         9,   9,   9,   9,   9,   8,   8,   8,   8,   8,   8,   8,   8,\n",
       "         8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
       "         8,   8,   8,   8,   8,   8,   8,   7,   7,   7,   7,   7,   7,\n",
       "         7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,\n",
       "         7,   7,   7,   7,   7,   7,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.idx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.min_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.current_vocab_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.original_vocab_token_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getGrainVocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TU = BasicObject.getGrainVocab('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简']\n"
     ]
    }
   ],
   "source": [
    "# idx2token\n",
    "print(TU[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Overview of Pyramid\n",
    "\n",
    "\n",
    "Show the pyramid structures: Corpus, Folder, Text, Sentence, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1.病史：患者为63岁女性，慢性病程，急性加重。', '既往有“高脂血症”病史。', '2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
       " ['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。', '双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
       " ['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
       "  '心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
       "  '腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
       "  '肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
       "  '双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    # count the string's byte number\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '.': 1, '病': 2, '史': 3, '：': 4, '患': 5, '者': 6, '为': 7, '6': 8, '3': 9, '岁': 10, '女': 11, '性': 12, '，': 13, '慢': 14, '程': 15, '急': 16, '加': 17, '重': 18, '。': 19, '既': 20, '往': 21, '有': 22, '“': 23, '高': 24, '脂': 25, '血': 26, '症': 27, '”': 28, '2': 29, '因': 30, '反': 31, '复': 32, '脐': 33, '周': 34, '疼': 35, '痛': 36, '年': 37, '余': 38, '再': 39, '发': 40, '并': 41, '入': 42, '院': 43, '体': 44, '查': 45, '压': 46, '8': 47, '/': 48, 'm': 49, 'H': 50, 'g': 51, '神': 52, '志': 53, '清': 54, '楚': 55, '浅': 56, '表': 57, '淋': 58, '巴': 59, '结': 60, '无': 61, '肿': 62, '大': 63, '口': 64, '唇': 65, '苍': 66, '白': 67, '双': 68, '侧': 69, '扁': 70, '桃': 71, '、': 72, '充': 73, '咽': 74, '颈': 75, '静': 76, '脉': 77, '怒': 78, '张': 79, '肺': 80, '呼': 81, '吸': 82, '音': 83, '晰': 84, '未': 85, '闻': 86, '及': 87, '干': 88, '湿': 89, '啰': 90, '胸': 91, '膜': 92, '摩': 93, '擦': 94, '心': 95, '率': 96, '次': 97, '分': 98, '律': 99, '齐': 100, '各': 101, '瓣': 102, '区': 103, '理': 104, '杂': 105, '腹': 106, '部': 107, '平': 108, '坦': 109, '见': 110, '胃': 111, '肠': 112, '型': 113, '蠕': 114, '动': 115, '波': 116, '壁': 117, '柔': 118, '软': 119, '跳': 120, '扪': 121, '包': 122, '块': 123, '肝': 124, '脾': 125, '肋': 126, '下': 127, 'M': 128, 'u': 129, 'r': 130, 'p': 131, 'h': 132, 'y': 133, '征': 134, '（': 135, '-': 136, '）': 137, '肾': 138, '叩': 139, '击': 140, '移': 141, '浊': 142, '鸣': 143, '4': 144, '肢': 145, '浮': 146, '四': 147, '肌': 148, '力': 149, '正': 150, '常': 151, '生': 152, '射': 153, '存': 154, '在': 155, '引': 156, '出': 157}\n",
      "['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '程', '急', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '2', '因', '反', '复', '脐', '周', '疼', '痛', '年', '余', '再', '发', '并', '入', '院', '体', '查', '压', '8', '/', 'm', 'H', 'g', '神', '志', '清', '楚', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '口', '唇', '苍', '白', '双', '侧', '扁', '桃', '、', '充', '咽', '颈', '静', '脉', '怒', '张', '肺', '呼', '吸', '音', '晰', '未', '闻', '及', '干', '湿', '啰', '胸', '膜', '摩', '擦', '心', '率', '次', '分', '律', '齐', '各', '瓣', '区', '理', '杂', '腹', '部', '平', '坦', '见', '胃', '肠', '型', '蠕', '动', '波', '壁', '柔', '软', '跳', '扪', '包', '块', '肝', '脾', '肋', '下', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肾', '叩', '击', '移', '浊', '鸣', '4', '肢', '浮', '四', '肌', '力', '正', '常', '生', '射', '存', '在', '引', '出']\n",
      "[3, 3, 5, 2, 2, 1, 1, 1, 3, 3, 1, 1, 6, 21, 1, 1, 1, 2, 2, 12, 1, 1, 1, 2, 1, 1, 4, 1, 2, 4, 1, 4, 1, 2, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 9, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 6, 1, 6, 2, 5, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SENT = []\n",
    "\n",
    "processed_data_path = 'processed_data.txt'\n",
    "\n",
    "\n",
    "DTU = {}\n",
    "LTU = []\n",
    "index2freq = []\n",
    "# data = np.zeros(5000, dtype= np.uint32)\n",
    "token_num_in_corpus = 0\n",
    "\n",
    "if os.path.isfile(processed_data_path):\n",
    "    os.remove(processed_data_path)\n",
    "        \n",
    "for text in corpus:\n",
    "    for strSent in text:\n",
    "        strTokens = [i for i in strSent]\n",
    "        for token in strTokens:\n",
    "            if token not in DTU:\n",
    "                # deal with new words\n",
    "                token_idx  = len(DTU)\n",
    "                DTU[token] = token_idx\n",
    "                index2freq.append(1)\n",
    "                LTU.append(token)\n",
    "            else:\n",
    "                # deal with old words\n",
    "                token_idx = DTU[token]\n",
    "                index2freq[token_idx] += 1\n",
    "            # data.append(token_idx)\n",
    "            # data[token_num_in_corpus] = token_idx\n",
    "            token_num_in_corpus = token_num_in_corpus + 1\n",
    "            \n",
    "        with open(processed_data_path, 'a') as f:\n",
    "            line_sentence = ' '.join(strTokens) + '\\n'\n",
    "            f.write(line_sentence)\n",
    "            SENT.append(utf8len(line_sentence))\n",
    "            \n",
    "            \n",
    "            \n",
    "# data = data[: token_num_in_corpus]\n",
    "# print(len(data))\n",
    "# print(data)\n",
    "print(DTU)\n",
    "print(LTU)\n",
    "print(index2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  88,  136,  228,  356,  452,  548,  638,  840,  934, 1066])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentIdx = np.cumsum(SENT)\n",
    "sentIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "sent_idx = 2 # the three line\n",
    "sent_pos_start = sentIdx[sent_idx - 1]\n",
    "print(sent_pos_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_with_position(processed_data_path,start_position):\n",
    "    with open(processed_data_path, 'r') as f:\n",
    "        f.seek(start_position)\n",
    "        line = f.readline() # there is no 's' in f.readline()\n",
    "        # last_pos = f.tell()\n",
    "        # f.close()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 12,  9,  6,  6,  6,  5,  5,  4,  4,  4,  4,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index2freq = np.sort(index2freq)[::-1]\n",
    "new_index2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,  19,  61,  83,  12,  85,  87,   2,  29,  31,  36,  26,   0,\n",
       "        62,   9,   1,  34,  68,  48, 104,   8, 115,  63, 112, 106, 103,\n",
       "        54,  98,  44,  46,  92,  49,  28,  72,  73,  86,  79,  97,  33,\n",
       "       135, 149, 136, 121, 145,  17,  18, 127, 148, 137,  23,   4, 153,\n",
       "         3, 124,  14,  55,   6,   7,  53,  59,   5,  52,  57,  51,  10,\n",
       "        58,  11,  60,  56,  27,  50,  47,  30,  25,  24,  32,  65,  35,\n",
       "        22,  37,  38,  21,  20,  39,  40,  41,  42,  43,  16,  45,  15,\n",
       "        64, 157,  66, 114, 117, 118, 119, 120, 122, 123, 125, 126, 128,\n",
       "       129, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144,\n",
       "       146, 147, 150, 151, 152, 154, 155, 116, 113,  67, 111,  69,  70,\n",
       "        71,  74,  75,  76,  77, 156,  80,  81,  82,  84,  88,  89,  90,\n",
       "        91,  93,  94,  95,  96,  99, 100, 101, 102, 105, 107, 108, 109,\n",
       "       110,  78])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newidx2oldidx = np.argsort(index2freq)[::-1]\n",
    "newidx2oldidx# [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  15,   7,  52,  50,  60,  56,  57,  20,  14,  64,  66,   4,\n",
       "         0,  54,  90,  88,  44,  45,   1,  82,  81,  78,  49,  74,  73,\n",
       "        11,  69,  32,   8,  72,   9,  75,  38,  16,  77,  10,  79,  80,\n",
       "        83,  84,  85,  86,  87,  28,  89,  29,  71,  18,  31,  70,  63,\n",
       "        61,  58,  26,  55,  68,  62,  65,  59,  67,   2,  13,  22,  91,\n",
       "        76,  93, 126,  17, 128, 129, 130,  33,  34, 131, 132, 133, 134,\n",
       "       157,  36, 136, 137, 138,   3, 139,   5,  35,   6, 140, 141, 142,\n",
       "       143,  30, 144, 145, 146, 147,  37,  27, 148, 149, 150, 151,  25,\n",
       "        19, 152,  24, 153, 154, 155, 156, 127,  23, 125,  94,  21, 124,\n",
       "        95,  96,  97,  98,  42,  99, 100,  53, 101, 102,  46, 103, 104,\n",
       "       105, 106, 107, 108, 109,  39,  41,  48, 110, 111, 112, 113, 114,\n",
       "       115, 116,  43, 117, 118,  47,  40, 119, 120, 121,  51, 122, 123,\n",
       "       135,  92])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldidx2newidx = np.zeros(len(newidx2oldidx), dtype= int)\n",
    "\n",
    "for new_idx, old_idx in enumerate(newidx2oldidx):\n",
    "    oldidx2newidx[old_idx] = new_idx\n",
    "    \n",
    "oldidx2newidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['，', '。', '无', '音', '性', '未', '及', '病', '2', '反', '痛', '血', '1', '肿', '3', '.', '周', '双', '/', '理', '6', '动', '大', '肠', '腹', '区', '清', '分', '体', '压', '膜', 'm', '”', '、', '充', '闻', '张', '次', '脐', '（', '力', '-', '扪', '肢', '加', '重', '下', '肌', '）', '“', '：', '射', '史', '肝', '慢', '楚', '者', '为', '志', '巴', '患', '神', '表', 'g', '岁', '淋', '女', '结', '浅', '症', 'H', '8', '因', '脂', '高', '复', '唇', '疼', '有', '年', '余', '往', '既', '再', '发', '并', '入', '院', '急', '查', '程', '口', '出', '苍', '蠕', '壁', '柔', '软', '跳', '包', '块', '脾', '肋', 'M', 'u', 'r', 'p', 'h', 'y', '征', '肾', '叩', '击', '移', '浊', '鸣', '4', '浮', '四', '正', '常', '生', '存', '在', '波', '型', '白', '胃', '侧', '扁', '桃', '咽', '颈', '静', '脉', '引', '肺', '呼', '吸', '晰', '干', '湿', '啰', '胸', '摩', '擦', '心', '率', '律', '齐', '各', '瓣', '杂', '部', '平', '坦', '见', '怒']\n",
      "{'，': 0, '。': 1, '无': 2, '音': 3, '性': 4, '未': 5, '及': 6, '病': 7, '2': 8, '反': 9, '痛': 10, '血': 11, '1': 12, '肿': 13, '3': 14, '.': 15, '周': 16, '双': 17, '/': 18, '理': 19, '6': 20, '动': 21, '大': 22, '肠': 23, '腹': 24, '区': 25, '清': 26, '分': 27, '体': 28, '压': 29, '膜': 30, 'm': 31, '”': 32, '、': 33, '充': 34, '闻': 35, '张': 36, '次': 37, '脐': 38, '（': 39, '力': 40, '-': 41, '扪': 42, '肢': 43, '加': 44, '重': 45, '下': 46, '肌': 47, '）': 48, '“': 49, '：': 50, '射': 51, '史': 52, '肝': 53, '慢': 54, '楚': 55, '者': 56, '为': 57, '志': 58, '巴': 59, '患': 60, '神': 61, '表': 62, 'g': 63, '岁': 64, '淋': 65, '女': 66, '结': 67, '浅': 68, '症': 69, 'H': 70, '8': 71, '因': 72, '脂': 73, '高': 74, '复': 75, '唇': 76, '疼': 77, '有': 78, '年': 79, '余': 80, '往': 81, '既': 82, '再': 83, '发': 84, '并': 85, '入': 86, '院': 87, '急': 88, '查': 89, '程': 90, '口': 91, '出': 92, '苍': 93, '蠕': 94, '壁': 95, '柔': 96, '软': 97, '跳': 98, '包': 99, '块': 100, '脾': 101, '肋': 102, 'M': 103, 'u': 104, 'r': 105, 'p': 106, 'h': 107, 'y': 108, '征': 109, '肾': 110, '叩': 111, '击': 112, '移': 113, '浊': 114, '鸣': 115, '4': 116, '浮': 117, '四': 118, '正': 119, '常': 120, '生': 121, '存': 122, '在': 123, '波': 124, '型': 125, '白': 126, '胃': 127, '侧': 128, '扁': 129, '桃': 130, '咽': 131, '颈': 132, '静': 133, '脉': 134, '引': 135, '肺': 136, '呼': 137, '吸': 138, '晰': 139, '干': 140, '湿': 141, '啰': 142, '胸': 143, '摩': 144, '擦': 145, '心': 146, '率': 147, '律': 148, '齐': 149, '各': 150, '瓣': 151, '杂': 152, '部': 153, '平': 154, '坦': 155, '见': 156, '怒': 157}\n"
     ]
    }
   ],
   "source": [
    "new_LTU = []\n",
    "for new_idx in range(len(LTU)):\n",
    "    new_LTU.append(LTU[newidx2oldidx[new_idx]])\n",
    "    \n",
    "print(new_LTU)\n",
    "new_DTU = {}\n",
    "for new_idx, token in enumerate(new_LTU):\n",
    "    new_DTU[token] = new_idx\n",
    "    \n",
    "print(new_DTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3d276600edff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moldidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldidx2newidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moldidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, oldidx in enumerate(data):\n",
    "    data[idx] = oldidx2newidx[oldidx]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  Level Dictionary\n",
      "{'NUMSents': [3, 2, 5], 'EndIDXSents': [3, 63, 128]}\n",
      "\n",
      "Sent  Level Dictionary\n",
      "{'NUMTokens': [24, 12, 25, 38, 24, 24, 24, 54, 25, 33], 'EndIDXTokens': [24, 36, 61, 99, 123, 147, 171, 225, 250, 283]}\n",
      "\n",
      "Token Level Dictionary\n",
      "{'DATAToken': ['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '性', '病', '程', '，', '急', '性', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '病', '史', '。', '2', '.', '因', '“', '反', '复', '脐', '周', '疼', '痛', '2', '年', '余', '，', '再', '发', '并', '加', '重', '1', '周', '”', '入', '院', '。', '3', '.', '体', '查', '：', '血', '压', '1', '2', '8', '/', '6', '3', 'm', 'm', 'H', 'g', '，', '神', '志', '清', '楚', '，', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '，', '口', '唇', '无', '苍', '白', '。', '双', '侧', '扁', '桃', '体', '无', '肿', '大', '、', '充', '血', '。', '咽', '无', '充', '血', '。', '颈', '静', '脉', '无', '怒', '张', '。', '双', '肺', '呼', '吸', '音', '清', '晰', '，', '未', '闻', '及', '干', '湿', '性', '啰', '音', '，', '无', '胸', '膜', '摩', '擦', '音', '。', '心', '率', '6', '2', '次', '/', '分', '，', '律', '齐', '，', '各', '瓣', '膜', '区', '未', '闻', '及', '病', '理', '性', '杂', '音', '。', '腹', '部', '平', '坦', '，', '未', '见', '胃', '肠', '型', '及', '蠕', '动', '波', '，', '腹', '壁', '柔', '软', '，', '脐', '周', '压', '痛', '，', '无', '反', '跳', '痛', '，', '未', '扪', '及', '包', '块', '，', '肝', '脾', '肋', '下', '未', '扪', '及', '，', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肝', '肾', '区', '无', '叩', '击', '痛', '，', '移', '动', '性', '浊', '音', '（', '-', '）', '，', '肠', '鸣', '音', '4', '次', '/', '分', '。', '双', '下', '肢', '无', '浮', '肿', '。', '四', '肢', '肌', '力', '、', '肌', '张', '力', '正', '常', '，', '生', '理', '反', '射', '存', '在', '，', '病', '理', '反', '射', '未', '引', '出', '。']}\n"
     ]
    }
   ],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n"
     ]
    }
   ],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus, Group and from Corpus to Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus/clinical_ner_sample/MEntity': ['patient4367.txt',\n",
       "  'patient4367.Entity',\n",
       "  'patient4378.txt',\n",
       "  'patient4378.Entity',\n",
       "  'patient4382.txt',\n",
       "  'patient4382.Entity']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "    \n",
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group, Text and from Group to Texts\n",
    "\n",
    "Each text is a tuple for strText, SSET, orig_file_name, anno_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': <function nlptext.utils.pyramid.textFileReader(folderPath, fileNames, anno=False, **kwargs)>,\n",
       " 'line': <function nlptext.utils.pyramid.textLineReader(folderPath, fileNames, anno=False, **kwargs)>,\n",
       " 'block': <function nlptext.utils.pyramid.textBlockReader(folderPath, fileNames, anno='conll_block', change_tags=False, connector='', suffix=True, anno_sep=' ', **kwargs)>,\n",
       " 'element': <function nlptext.utils.pyramid.textElementReader(folderPath, fileNames, anno=False, **kwargs)>,\n",
       " 'json': <function nlptext.utils.pyramid.textJsonReader(folderPath, fileNames, anno='json_annotation', strText='content', labels='annotation', **kwargs)>,\n",
       " 'csv': <function nlptext.utils.pyramid.textCSVReader(folderPath, fileNames, anno='csv_annotation', ANNOIden='.csv', **kwargs)>}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.pyramid import CorpusGroupsReader, FolderTextsReaders\n",
    "\n",
    "FolderTextsReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus/clinical_ner_sample/MEntity': ['patient4367.txt', 'patient4367.Entity', 'patient4378.txt', 'patient4378.Entity', 'patient4382.txt', 'patient4382.Entity']}\n",
      "1.咳嗽查因:咳嗽变异性哮喘?迁延性支气管炎?,2.高血压病?\n",
      "中年男性,亚急性病程。因“咳嗽、咳痰1月余”入院。既往史:既往健康。有吸烟史20年,量约10支/天,偶有饮酒。入院查体:体温36.3℃,脉搏80次/分,呼吸20次/分,血压151/108mmHg,血氧饱和度99%。咽部轻度充血,扁桃体无明显肿大,肺叩诊清音,双肺呼吸音清晰,双肺未闻及明显干湿性罗音和胸膜摩擦音。心率80次/分,心律齐,未闻及心脏杂音。辅助检查:2016-01-11我院血常规示:嗜酸粒细胞比值8.3%,嗜酸细胞绝对数0.76x10^9/L,CRP正常。\n",
      "[['咳嗽', 2, 4, '症状'], ['咳嗽变异性哮喘', 7, 14, '疾病'], ['迁延性支气管炎', 15, 22, '疾病'], ['高血压病', 26, 30, '疾病'], ['亚急性', 37, 40, '修饰'], ['咳嗽', 45, 47, '症状'], ['咳痰', 48, 50, '症状'], ['1月余', 50, 53, '修饰'], ['吸烟史', 67, 70, '疾病'], ['20年', 70, 73, '修饰'], ['量约10支/天', 74, 81, '修饰'], ['偶有', 82, 84, '修饰'], ['饮酒', 84, 86, '疾病'], ['体温', 92, 94, '检查'], ['36.3℃', 94, 99, '修饰'], ['脉搏', 100, 102, '检查'], ['80次/分', 102, 107, '修饰'], ['呼吸', 108, 110, '检查'], ['20次/分', 110, 115, '修饰'], ['血压', 116, 118, '检查'], ['151/108mmHg', 118, 129, '修饰'], ['血氧饱和度', 130, 135, '检查'], ['99%', 135, 138, '修饰'], ['轻度', 141, 143, '修饰'], ['充血', 143, 145, '症状'], ['明显', 150, 152, '修饰'], ['肿大', 152, 154, '症状'], ['叩诊', 156, 158, '检查'], ['呼吸音', 163, 166, '检查'], ['明显', 174, 176, '修饰'], ['干湿性罗音', 176, 181, '症状'], ['摩擦音', 184, 187, '症状'], ['心率', 188, 190, '检查'], ['80次/分', 190, 195, '修饰'], ['心律', 196, 198, '检查'], ['杂音', 205, 207, '症状'], ['2016-01-11', 213, 223, '修饰'], ['血常规', 225, 228, '检查'], ['嗜酸粒细胞比值', 230, 237, '检查'], ['8.3%', 237, 241, '修饰'], ['嗜酸细胞绝对数', 242, 249, '检查'], ['0.76x10^9/L', 249, 260, '修饰'], ['CRP', 261, 264, '检查']]\n",
      "patient4367.txt\n",
      "patient4367.Entity\n"
     ]
    }
   ],
   "source": [
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "CorpusGroups = CorpusGroupsReader(CORPUSPath, iden = Corpus2GroupMethod)[0]\n",
    "\n",
    "print(CorpusGroups)\n",
    "\n",
    "group_name =  list(CorpusGroups.keys())[0]\n",
    "text_names = CorpusGroups[group_name]\n",
    "\n",
    "\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.Entity',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 1, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "GroupTexts = FolderTextsReaders[Group2TextMethod](group_name, text_names, anno, **anno_keywords)\n",
    "\n",
    "strText_SSET_O_A = list(GroupTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus/medical_pos_sample/batch2': ['patient5221-sent1.UMLSTag', 'patient5221.txt', 'patient5221-sent7.UMLSTag', 'patient5212-sent3.UMLSTag', 'patient5221-sent3.UMLSTag', 'patient5211-sent7.UMLSTag', 'patient5221-sent9.UMLSTag', 'patient5218-sent1.UMLSTag', 'patient5212.txt', 'patient5199-sent1.UMLSTag', 'patient5211-sent6.UMLSTag', 'patient5212-sent1.UMLSTag', 'patient5199-sent5.UMLSTag', 'patient5214.txt', 'patient5188-sent3.UMLSTag', 'patient5188-sent10.UMLSTag', 'patient5211-sent1.UMLSTag', 'patient5218-sent5.UMLSTag', 'patient5214-sent0.UMLSTag', 'patient5211-sent0.UMLSTag', 'patient5212-sent0.UMLSTag', 'patient5219-sent4.UMLSTag', 'patient5219-sent6.UMLSTag', 'patient5212-sent4.UMLSTag', 'patient5188-sent7.UMLSTag', 'patient5188-sent2.UMLSTag', 'patient5188-sent9.UMLSTag', 'patient5178-sent4.UMLSTag', 'patient5218-sent2.UMLSTag', 'patient5221-sent2.UMLSTag', 'patient5214-sent4.UMLSTag', 'patient5186-sent1.UMLSTag', 'patient5199-sent3.UMLSTag', 'patient5199-sent2.UMLSTag', 'patient5188-sent1.UMLSTag', 'patient5221-sent6.UMLSTag', 'patient5214-sent3.UMLSTag', 'patient5219-sent10.UMLSTag', 'patient5188-sent0.UMLSTag', 'patient5188-sent5.UMLSTag', 'patient5186.txt', 'patient5212-sent2.UMLSTag', 'patient5178-sent2.UMLSTag', 'patient5211-sent4.UMLSTag', 'patient5199-sent7.UMLSTag', 'patient5218-sent0.UMLSTag', 'patient5212-sent5.UMLSTag', 'patient5219-sent5.UMLSTag', 'patient5211-sent3.UMLSTag', 'patient5211-sent2.UMLSTag', 'patient5211-sent5.UMLSTag', 'patient5199-sent4.UMLSTag', 'patient5219-sent11.UMLSTag', 'patient5218.txt', 'patient5188-sent8.UMLSTag', 'patient5219-sent3.UMLSTag', 'patient5221-sent8.UMLSTag', 'patient5219.txt', 'patient5214-sent5.UMLSTag', 'patient5221-sent5.UMLSTag', 'patient5178-sent0.UMLSTag', 'patient5199.txt', 'patient5186-sent2.UMLSTag', 'patient5219-sent2.UMLSTag', 'patient5219-sent9.UMLSTag', 'patient5186-sent0.UMLSTag', 'patient5211.txt', 'patient5219-sent7.UMLSTag', 'patient5188.txt', 'patient5199-sent0.UMLSTag', 'patient5211-sent8.UMLSTag', 'patient5199-sent8.UMLSTag', 'patient5214-sent1.UMLSTag', 'patient5178.txt', 'patient5178-sent1.UMLSTag', 'patient5214-sent2.UMLSTag', 'patient5188-sent12.UMLSTag', 'patient5199-sent6.UMLSTag', 'patient5188-sent4.UMLSTag', 'patient5221-sent0.UMLSTag', 'patient5188-sent13.UMLSTag', 'patient5218-sent3.UMLSTag', 'patient5188-sent11.UMLSTag', 'patient5219-sent8.UMLSTag', 'patient5221-sent4.UMLSTag', 'patient5219-sent0.UMLSTag', 'patient5219-sent1.UMLSTag', 'patient5178-sent3.UMLSTag', 'patient5188-sent6.UMLSTag'], 'corpus/medical_pos_sample/batch1': ['patient4855-sent4.UMLSTag', 'patient4857-sent1.UMLSTag', 'patient4857-sent2.UMLSTag', 'patient4892.txt', 'patient4855-sent2.UMLSTag', 'patient4818-sent8.UMLSTag', 'patient4841-sent7.UMLSTag', 'patient5172-sent5.UMLSTag', 'patient4818-sent4.UMLSTag', 'patient4818-sent2.UMLSTag', 'patient4857-sent7.UMLSTag', 'patient4892-sent0.UMLSTag', 'patient4835-sent6.UMLSTag', 'patient4879-sent12.UMLSTag', 'patient4818-sent9.UMLSTag', 'patient4855-sent8.UMLSTag', 'patient4841-sent3.UMLSTag', 'patient4835-sent3.UMLSTag', 'patient4835-sent12.UMLSTag', 'patient4818.txt', 'patient4841.txt', 'patient4835-sent1.UMLSTag', 'patient4841-sent0.UMLSTag', 'patient4857-sent16.UMLSTag', 'patient4855-sent6.UMLSTag', 'patient4879-sent5.UMLSTag', 'patient4879-sent3.UMLSTag', 'patient4879-sent4.UMLSTag', 'patient4857-sent8.UMLSTag', 'patient4855.txt', 'patient5172-sent3.UMLSTag', 'patient4857-sent4.UMLSTag', 'patient4818-sent0.UMLSTag', 'patient4835-sent8.UMLSTag', 'patient4818-sent10.UMLSTag', 'patient4855-sent11.UMLSTag', 'patient4857-sent15.UMLSTag', 'patient4841-sent5.UMLSTag', 'patient4818-sent5.UMLSTag', 'patient4857-sent9.UMLSTag', 'patient4818-sent3.UMLSTag', 'patient4879-sent13.UMLSTag', 'patient4855-sent10.UMLSTag', 'patient4835.txt', 'patient5172.txt', 'patient4841-sent2.UMLSTag', 'patient4879-sent0.UMLSTag', 'patient4835-sent9.UMLSTag', 'patient4879-sent7.UMLSTag', 'patient4835-sent10.UMLSTag', 'patient4879-sent8.UMLSTag', 'patient4857-sent6.UMLSTag', 'patient4825-sent1.UMLSTag', 'patient4855-sent5.UMLSTag', 'patient4857-sent12.UMLSTag', 'patient4857-sent0.UMLSTag', 'patient4892-sent4.UMLSTag', 'patient4841-sent6.UMLSTag', 'patient5175.txt', 'patient4879-sent9.UMLSTag', 'patient4879-sent11.UMLSTag', 'patient4857-sent13.UMLSTag', 'patient5172-sent2.UMLSTag', 'patient4835-sent5.UMLSTag', 'patient5172-sent0.UMLSTag', 'patient4855-sent0.UMLSTag', 'patient4879-sent1.UMLSTag', 'patient4841-sent1.UMLSTag', 'patient4857.txt', 'patient5172-sent1.UMLSTag', 'patient5175-sent1.UMLSTag', 'patient5175-sent3.UMLSTag', 'patient4818-sent7.UMLSTag', 'patient4879-sent2.UMLSTag', 'patient4857-sent11.UMLSTag', 'patient5175-sent0.UMLSTag', 'patient4835-sent4.UMLSTag', 'patient4855-sent3.UMLSTag', 'patient4841-sent8.UMLSTag', 'patient4879-sent6.UMLSTag', 'patient4857-sent5.UMLSTag', 'patient4835-sent13.UMLSTag', 'patient4855-sent9.UMLSTag', 'patient4825-sent0.UMLSTag', 'patient4879-sent10.UMLSTag', 'patient4855-sent7.UMLSTag', 'patient4879.txt', 'patient4818-sent1.UMLSTag', 'patient4892-sent1.UMLSTag', 'patient4835-sent2.UMLSTag', 'patient4825.txt', 'patient5172-sent4.UMLSTag', 'patient4857-sent14.UMLSTag', 'patient4835-sent7.UMLSTag', 'patient4857-sent3.UMLSTag', 'patient4841-sent4.UMLSTag', 'patient4835-sent11.UMLSTag', 'patient4892-sent2.UMLSTag', 'patient4818-sent6.UMLSTag', 'patient4835-sent0.UMLSTag', 'patient4855-sent1.UMLSTag', 'patient4892-sent3.UMLSTag']}\n",
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "急性脑梗死,高血压病3级 高危\n",
      "患者为中年女性,急性起病,因“突发左侧肢体无力3天”入院。既往有高血压病4年余,近期未服用药物治疗,10余年前因“肾结石”在外院行“取石术”。查体:T:36.3℃,P:60次/分,呼吸:20次/分,血压:152/86mmHg。神志清楚,言语清晰,双侧瞳孔等大同圆,直径3mm,对光反射灵敏,双眼各向活动不受限,右侧鼻唇沟变浅,口角左歪,伸舌居中。颈软、无抵抗。左侧肢体肌力4+级,四肢肌张力对称正常,四肢腱反射正常,双侧病理征未引出。双肺呼吸音清,双肺未闻及干湿性啰音,心率60次/分,心律齐,各瓣膜听诊区未闻及病理性杂音。腹平软,无压痛,肝脾未及,双下肢无水肿。辅助检查:2014-07-26我院门诊查头颅CT示“未见明显异常”。\n",
      "[['急', 0, 1, '定性-Qualitative_Concep-Ql'], ['性', 1, 2, '定性-Qualitative_Concep-Ql'], ['脑', 2, 3, '身体部位-Body_Part-BP'], ['梗死', 3, 5, '病理功能-Pathologic_Function-PF'], [',', 5, 6, '标点-Punctuation-Punc'], ['高', 6, 7, '定性-Qualitative_Concep-Ql'], ['血', 7, 8, '身体物质-BodySubstance-BS'], ['压', 8, 9, '临床属性-Clinical_Attribute-CA'], ['病', 9, 10, '疾病-Disease-Di'], ['3', 10, 11, '数字-Number-N'], ['级', 11, 12, '单位-Unit-U'], [' ', 12, 13, '标点-Punctuation-Punc'], ['高', 13, 14, '定性-Qualitative_Concep-Ql'], ['危', 14, 15, '定性-Qualitative_Concep-Ql'], ['患者', 16, 18, '人群-Group-Gr'], ['为', 18, 19, '发现-Finding-Find'], ['中年', 19, 21, '人群-Group-Gr'], ['女性', 21, 23, '人群-Group-Gr'], [',', 23, 24, '标点-Punctuation-Punc'], ['急', 24, 25, '定性-Qualitative_Concep-Ql'], ['性', 25, 26, '定性-Qualitative_Concep-Ql'], ['起病', 26, 28, '病理功能-Pathologic_Function-PF'], [',', 28, 29, '标点-Punctuation-Punc'], ['因', 29, 30, '因果-Causal_Or_Effect-CE'], ['“', 30, 31, '发现-Finding-Find'], ['突发', 31, 33, '定性-Qualitative_Concep-Ql'], ['左', 33, 34, '空间概念-Spatial_Concept-Sp'], ['侧', 34, 35, '空间概念-Spatial_Concept-Sp'], ['肢体', 35, 37, '身体部位-Body_Part-BP'], ['无力', 37, 39, '定性-Qualitative_Concep-Ql'], ['3', 39, 40, '数字-Number-N'], ['天', 40, 41, '时间单位-Time_Unit-TU'], ['”', 41, 42, '发现-Finding-Find'], ['入院', 42, 44, '事件-Event-E'], ['。', 44, 45, '标点-Punctuation-Punc'], ['既往', 45, 47, '时间概念-Temporal_Concept-T'], ['有', 47, 48, '有-Present-Pre'], ['高', 48, 49, '定性-Qualitative_Concep-Ql'], ['血', 49, 50, '身体物质-BodySubstance-BS'], ['压', 50, 51, '临床属性-Clinical_Attribute-CA'], ['病', 51, 52, '疾病-Disease-Di'], ['4', 52, 53, '数字-Number-N'], ['年', 53, 54, '时间单位-Time_Unit-TU'], ['余', 54, 55, '数字-Number-N'], [',', 55, 56, '标点-Punctuation-Punc'], ['近期', 56, 58, '时间概念-Temporal_Concept-T'], ['未', 58, 59, '无-Absent-Ab'], ['服用', 59, 61, '治疗行为-Treatment_Behavior-TB'], ['药物', 61, 63, '药物-Clinical_Drug-Drug'], ['治疗', 63, 65, '治疗行为-Treatment_Behavior-TB'], [',', 65, 66, '标点-Punctuation-Punc'], ['10', 66, 68, '数字-Number-N'], ['余', 68, 69, '数字-Number-N'], ['年', 69, 70, '时间单位-Time_Unit-TU'], ['前', 70, 71, '时间概念-Temporal_Concept-T'], ['因', 71, 72, '因果-Causal_Or_Effect-CE'], ['“', 72, 73, '发现-Finding-Find'], ['肾', 73, 74, '身体部位-Body_Part-BP'], ['结石', 74, 76, '病理功能-Pathologic_Function-PF'], ['”', 76, 77, '发现-Finding-Find'], ['在', 77, 78, '介词-Preposition-Prep'], ['外院', 78, 80, '地名机构名-Location_Organization-LO'], ['行', 80, 81, '治疗行为-Treatment_Behavior-TB'], ['“', 81, 82, '发现-Finding-Find'], ['取', 82, 83, '治疗行为-Treatment_Behavior-TB'], ['石', 83, 84, '身体物质-BodySubstance-BS'], ['术', 84, 85, '治疗项目-Treatment_Project-TP'], ['”', 85, 86, '发现-Finding-Find'], ['。', 86, 87, '标点-Punctuation-Punc'], ['查', 87, 88, '检查行为-Examination_Behavior-EB'], ['体', 88, 89, '身体部位-Body_Part-BP'], [':', 89, 90, '发现-Finding-Find'], ['T', 90, 91, '临床属性-Clinical_Attribute-CA'], [':', 91, 92, '发现-Finding-Find'], ['36.3', 92, 96, '数字-Number-N'], ['℃', 96, 97, '单位-Unit-U'], [',', 97, 98, '标点-Punctuation-Punc'], ['P', 98, 99, '临床属性-Clinical_Attribute-CA'], [':', 99, 100, '发现-Finding-Find'], ['60', 100, 102, '数字-Number-N'], ['次', 102, 103, '单位-Unit-U'], ['/', 103, 104, '数学符号-Math_Symbols-MS'], ['分', 104, 105, '时间单位-Time_Unit-TU'], [',', 105, 106, '标点-Punctuation-Punc'], ['呼吸', 106, 108, '身体功能-Body_Function-BF'], [':', 108, 109, '发现-Finding-Find'], ['20', 109, 111, '数字-Number-N'], ['次', 111, 112, '单位-Unit-U'], ['/', 112, 113, '数学符号-Math_Symbols-MS'], ['分', 113, 114, '时间单位-Time_Unit-TU'], [',', 114, 115, '标点-Punctuation-Punc'], ['血', 115, 116, '身体物质-BodySubstance-BS'], ['压', 116, 117, '临床属性-Clinical_Attribute-CA'], [':', 117, 118, '发现-Finding-Find'], ['152', 118, 121, '数字-Number-N'], ['/', 121, 122, '数学符号-Math_Symbols-MS'], ['86', 122, 124, '数字-Number-N'], ['mmHg', 124, 128, '单位-Unit-U'], ['。', 128, 129, '标点-Punctuation-Punc'], ['神志', 129, 131, '身体功能-Body_Function-BF'], ['清楚', 131, 133, '定性-Qualitative_Concep-Ql'], [',', 133, 134, '标点-Punctuation-Punc'], ['言语', 134, 136, '身体功能-Body_Function-BF'], ['清晰', 136, 138, '定性-Qualitative_Concep-Ql'], [',', 138, 139, '标点-Punctuation-Punc'], ['双', 139, 140, '数字-Number-N'], ['侧', 140, 141, '空间概念-Spatial_Concept-Sp'], ['瞳孔', 141, 143, '身体部位-Body_Part-BP'], ['等', 143, 144, '定性-Qualitative_Concep-Ql'], ['大', 144, 145, '定性-Qualitative_Concep-Ql'], ['同', 145, 146, '介词-Preposition-Prep'], ['圆', 146, 147, '定性-Qualitative_Concep-Ql'], [',', 147, 148, '标点-Punctuation-Punc'], ['直径', 148, 150, '临床属性-Clinical_Attribute-CA'], ['3', 150, 151, '数字-Number-N'], ['mm', 151, 153, '单位-Unit-U'], [',', 153, 154, '标点-Punctuation-Punc'], ['对光', 154, 156, '检查行为-Examination_Behavior-EB'], ['反射', 156, 158, '身体功能-Body_Function-BF'], ['灵敏', 158, 160, '定性-Qualitative_Concep-Ql'], [',', 160, 161, '标点-Punctuation-Punc'], ['双', 161, 162, '数字-Number-N'], ['眼', 162, 163, '身体部位-Body_Part-BP'], ['各向', 163, 165, '空间概念-Spatial_Concept-Sp'], ['活动', 165, 167, '身体功能-Body_Function-BF'], ['不', 167, 168, '无-Absent-Ab'], ['受限', 168, 170, '定性-Qualitative_Concep-Ql'], [',', 170, 171, '标点-Punctuation-Punc'], ['右', 171, 172, '空间概念-Spatial_Concept-Sp'], ['侧', 172, 173, '空间概念-Spatial_Concept-Sp'], ['鼻', 173, 174, '身体部位-Body_Part-BP'], ['唇', 174, 175, '身体部位-Body_Part-BP'], ['沟', 175, 176, '空间概念-Spatial_Concept-Sp'], ['变浅', 176, 178, '定性-Qualitative_Concep-Ql'], [',', 178, 179, '标点-Punctuation-Punc'], ['口角', 179, 181, '身体部位-Body_Part-BP'], ['左', 181, 182, '空间概念-Spatial_Concept-Sp'], ['歪', 182, 183, '定性-Qualitative_Concep-Ql'], [',', 183, 184, '标点-Punctuation-Punc'], ['伸', 184, 185, '身体功能-Body_Function-BF'], ['舌', 185, 186, '身体部位-Body_Part-BP'], ['居中', 186, 188, '空间概念-Spatial_Concept-Sp'], ['。', 188, 189, '标点-Punctuation-Punc'], ['颈', 189, 190, '身体部位-Body_Part-BP'], ['软', 190, 191, '定性-Qualitative_Concep-Ql'], ['、', 191, 192, '连词-Conjunction-Conj'], ['无', 192, 193, '无-Absent-Ab'], ['抵抗', 193, 195, '身体功能-Body_Function-BF'], ['。', 195, 196, '标点-Punctuation-Punc'], ['左', 196, 197, '空间概念-Spatial_Concept-Sp'], ['侧', 197, 198, '空间概念-Spatial_Concept-Sp'], ['肢体', 198, 200, '身体部位-Body_Part-BP'], ['肌力', 200, 202, '身体功能-Body_Function-BF'], ['4', 202, 203, '数字-Number-N'], ['+', 203, 204, '数学符号-Math_Symbols-MS'], ['级', 204, 205, '单位-Unit-U'], [',', 205, 206, '标点-Punctuation-Punc'], ['四肢', 206, 208, '身体部位-Body_Part-BP'], ['肌', 208, 209, '身体部位-Body_Part-BP'], ['张力', 209, 211, '身体功能-Body_Function-BF'], ['对称', 211, 213, '定性-Qualitative_Concep-Ql'], ['正常', 213, 215, '定性-Qualitative_Concep-Ql'], [',', 215, 216, '标点-Punctuation-Punc'], ['四肢', 216, 218, '身体部位-Body_Part-BP'], ['腱', 218, 219, '身体部位-Body_Part-BP'], ['反射', 219, 221, '身体功能-Body_Function-BF'], ['正常', 221, 223, '定性-Qualitative_Concep-Ql'], [',', 223, 224, '标点-Punctuation-Punc'], ['双', 224, 225, '数字-Number-N'], ['侧', 225, 226, '空间概念-Spatial_Concept-Sp'], ['病理', 226, 228, '病理功能-Pathologic_Function-PF'], ['征', 228, 229, '体征与症状-Sign_Or_Symptom-SOS'], ['未', 229, 230, '无-Absent-Ab'], ['引出', 230, 232, '发现-Finding-Find'], ['。', 232, 233, '标点-Punctuation-Punc'], ['双', 233, 234, '数字-Number-N'], ['肺', 234, 235, '身体部位-Body_Part-BP'], ['呼吸', 235, 237, '身体功能-Body_Function-BF'], ['音', 237, 238, '临床属性-Clinical_Attribute-CA'], ['清', 238, 239, '定性-Qualitative_Concep-Ql'], [',', 239, 240, '标点-Punctuation-Punc'], ['双', 240, 241, '数字-Number-N'], ['肺', 241, 242, '身体部位-Body_Part-BP'], ['未', 242, 243, '无-Absent-Ab'], ['闻', 243, 244, '检查行为-Examination_Behavior-EB'], ['及', 244, 245, '连词-Conjunction-Conj'], ['干', 245, 246, '定性-Qualitative_Concep-Ql'], ['湿', 246, 247, '定性-Qualitative_Concep-Ql'], ['性', 247, 248, '定性-Qualitative_Concep-Ql'], ['啰', 248, 249, '定性-Qualitative_Concep-Ql'], ['音', 249, 250, '临床属性-Clinical_Attribute-CA'], [',', 250, 251, '标点-Punctuation-Punc'], ['心', 251, 252, '身体部位-Body_Part-BP'], ['率', 252, 253, '临床属性-Clinical_Attribute-CA'], ['60', 253, 255, '数字-Number-N'], ['次', 255, 256, '单位-Unit-U'], ['/', 256, 257, '数学符号-Math_Symbols-MS'], ['分', 257, 258, '时间单位-Time_Unit-TU'], [',', 258, 259, '标点-Punctuation-Punc'], ['心', 259, 260, '身体部位-Body_Part-BP'], ['律', 260, 261, '临床属性-Clinical_Attribute-CA'], ['齐', 261, 262, '定性-Qualitative_Concep-Ql'], [',', 262, 263, '标点-Punctuation-Punc'], ['各', 263, 264, '定性-Qualitative_Concep-Ql'], ['瓣膜', 264, 266, '身体部位-Body_Part-BP'], ['听诊', 266, 268, '检查行为-Examination_Behavior-EB'], ['区', 268, 269, '空间概念-Spatial_Concept-Sp'], ['未', 269, 270, '无-Absent-Ab'], ['闻', 270, 271, '检查行为-Examination_Behavior-EB'], ['及', 271, 272, '连词-Conjunction-Conj'], ['病理', 272, 274, '病理功能-Pathologic_Function-PF'], ['性', 274, 275, '定性-Qualitative_Concep-Ql'], ['杂', 275, 276, '定性-Qualitative_Concep-Ql'], ['音', 276, 277, '临床属性-Clinical_Attribute-CA'], ['。', 277, 278, '标点-Punctuation-Punc'], ['腹', 278, 279, '身体部位-Body_Part-BP'], ['平软', 279, 281, '定性-Qualitative_Concep-Ql'], [',', 281, 282, '标点-Punctuation-Punc'], ['无', 282, 283, '无-Absent-Ab'], ['压', 283, 284, '检查行为-Examination_Behavior-EB'], ['痛', 284, 285, '病理功能-Pathologic_Function-PF'], [',', 285, 286, '标点-Punctuation-Punc'], ['肝', 286, 287, '身体部位-Body_Part-BP'], ['脾', 287, 288, '身体部位-Body_Part-BP'], ['未', 288, 289, '无-Absent-Ab'], ['及', 289, 290, '连词-Conjunction-Conj'], [',', 290, 291, '标点-Punctuation-Punc'], ['双', 291, 292, '数字-Number-N'], ['下肢', 292, 294, '身体部位-Body_Part-BP'], ['无', 294, 295, '无-Absent-Ab'], ['水肿', 295, 297, '病理功能-Pathologic_Function-PF'], ['。', 297, 298, '标点-Punctuation-Punc'], ['辅助', 298, 300, '定性-Qualitative_Concep-Ql'], ['检查', 300, 302, '检查项目-Examination_Project-EP'], [':', 302, 303, '发现-Finding-Find'], ['2014', 303, 307, '数字-Number-N'], ['-', 307, 308, '时间单位-Time_Unit-TU'], ['07', 308, 310, '数字-Number-N'], ['-', 310, 311, '时间单位-Time_Unit-TU'], ['26', 311, 313, '数字-Number-N'], ['我院', 313, 315, '地名机构名-Location_Organization-LO'], ['门诊', 315, 317, '地名机构名-Location_Organization-LO'], ['查', 317, 318, '检查行为-Examination_Behavior-EB'], ['头颅', 318, 320, '身体部位-Body_Part-BP'], ['CT', 320, 322, '检查项目-Examination_Project-EP'], ['示', 322, 323, '发现-Finding-Find'], ['“', 323, 324, '发现-Finding-Find'], ['未', 324, 325, '无-Absent-Ab'], ['见', 325, 326, '检查行为-Examination_Behavior-EB'], ['明显', 326, 328, '定性-Qualitative_Concep-Ql'], ['异常', 328, 330, '定性-Qualitative_Concep-Ql'], ['”', 330, 331, '发现-Finding-Find'], ['。', 331, 332, '标点-Punctuation-Punc']]\n",
      "patient5221.txt\n",
      "['patient5221-sent0.UMLSTag', 'patient5221-sent1.UMLSTag', 'patient5221-sent2.UMLSTag', 'patient5221-sent3.UMLSTag', 'patient5221-sent4.UMLSTag', 'patient5221-sent5.UMLSTag', 'patient5221-sent6.UMLSTag', 'patient5221-sent7.UMLSTag', 'patient5221-sent8.UMLSTag', 'patient5221-sent9.UMLSTag']\n"
     ]
    }
   ],
   "source": [
    "CORPUSPath = 'corpus/medical_pos_sample/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = Corpus2GroupMethod)[0]\n",
    "\n",
    "CorpusGroups = CorpusGroupsReader(CORPUSPath, iden = Corpus2GroupMethod)[0]\n",
    "\n",
    "print(CorpusGroups)\n",
    "\n",
    "group_name =  list(CorpusGroups.keys())[0]\n",
    "text_names = CorpusGroups[group_name]\n",
    "\n",
    "\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "anno = 'annofile4sent'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.UMLSTag',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "GroupTexts = FolderTextsReaders[Group2TextMethod](group_name, text_names, anno, **anno_keywords)\n",
    "\n",
    "strText_SSET_O_A = list(GroupTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strText is then be spilted into sentences.\n",
    "\n",
    "If SSET exists, we need to match SSET and the splited sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text, Sentence and from Text to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙江在线杭州4月25日讯(记者施宇翔 通讯员 方英)毒贩很“时髦”,用微信交易毒品。没料想警方也很“潮”,将计就计,一举将其擒获。记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。钱某不寻常的特征,引起了警方注意。禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['浙江在线杭州4月25日讯(记者施宇翔通讯员方英)毒贩很“时髦”,用微信交易毒品。',\n",
       " '没料想警方也很“潮”,将计就计,一举将其擒获。',\n",
       " '记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。',\n",
       " '黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。',\n",
       " '他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。',\n",
       " '钱某不寻常的特征,引起了警方注意。',\n",
       " '禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "##################################################################################################TEXT-SENT\n",
    "def reCutText2Sent(text, useSep = False):\n",
    "    \n",
    "    ###################### Remove some weird chars #######################\n",
    "    text = re.sub('\\xa0', '', text)\n",
    "    \n",
    "    ############# The Issue of Spaces\n",
    "    ###################### Convert the Spaces between two English Letters to 'ⴷ' #################\n",
    "    # Take care of Spaces\n",
    "    text = re.sub(r'(?<=[A-Za-z])\\s+(?=[|A-Za-z])', 'ⴷ',  text)\n",
    "    \n",
    "    ###################### Convert the S+ spaces to '〰' #################\n",
    "    text = re.sub(' {2}', '〰', text ).strip()\n",
    "    if useSep == ' ':\n",
    "        # if using space to sep the words\n",
    "        text = text.replace('\\t','').replace('〰', ' ')\n",
    "    elif useSep == '\\t':\n",
    "        # if using tab to sep the words, removing all spaces\n",
    "        text = text.replace(' ','').replace('〰', '')\n",
    "    else:\n",
    "        # if there is no sep char for Chinese, remove single space, and then convert space+ to single space\n",
    "        text = text.replace('\\t','').replace(' ', '',).replace('〰', ' ')\n",
    "        \n",
    "    # convert the spaces between English letters to single spaces\n",
    "    text = text.replace('ⴷ', ' ')\n",
    "    \n",
    "    # Other Things\n",
    "    text = re.sub('([。！!;；])([^”])', r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])',    r\"\\1\\n\\2\",text)\n",
    "    \n",
    "    # The \\n within \" \" is not considered\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [sent.strip() for sent in text]\n",
    "    # text = [sent.replace(' ', '').replace('\\\\n', '') for sent in text]\n",
    "    return [sent for sent in text if len(sent)>=2]\n",
    "\n",
    "\n",
    "text = '浙江在线杭州4月25日讯(记者施宇翔 通讯员 方英)毒贩很“时髦”,用微信交易毒品。没料想警方也很“潮”,将计就计,一举将其擒获。记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。钱某不寻常的特征,引起了警方注意。禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。'\n",
    "print(text)\n",
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole', **kwargs):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath. 2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.        \n",
    "    '''\n",
    "    if os.path.isfile(text):\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    elif method == 're':\n",
    "        return reCutText2Sent(text, **kwargs)\n",
    "    else:\n",
    "        return method(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence, Token and from Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data\n",
    "\n",
    "## From Corpus to Texts\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Corpus to Tokens\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtained Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n",
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3825\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3825\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CORPUSPath': 'corpus/boson/', 'Data_Dir': 'data/boson/char', 'EndIDXGroups': array([1]), 'length': 1}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Corpus2GroupMethod': '.txt', 'GroupType': 'File', 'group_names': ['corpus/boson/bosonNER.txt'], 'EndIDXTexts': array([1961]), 'length': 1}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Group2TextMethod': 'line', 'EndIDXSents': array([    7,    12,    14, ..., 10207, 10211, 10214]), 'length': 1961}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text2SentMethod': 're', 'EndIDXTokens': array([    40,     63,    138, ..., 533314, 533377, 533491]), 'data/boson/char/Pyramid/_file/token.txt': array([    148,     236,     508, ..., 1995569, 1995785, 1996219]), 'data/boson/char/Pyramid/_file/pos-bioes.txt': array([    136,     216,     479, ..., 1830495, 1830703, 1831082]), 'data/boson/char/Pyramid/_file/annoE-bioes.txt': array([     93,     139,     289, ..., 1124081, 1124210, 1124438]), 'length': 10214}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus.SENT['EndIDXTokens'][0])\n",
    "\n",
    "l = [1,2,3,4]\n",
    "import numpy as np\n",
    "\n",
    "type(np.array(l)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sent2TokenMethod': 'iter', 'TOKENLevel': 'char', 'Channel_Hyper_Path': {'token': 'data/boson/char/Pyramid/_file/token.txt', 'pos': 'data/boson/char/Pyramid/_file/pos-bioes.txt', 'annoE': 'data/boson/char/Pyramid/_file/annoE-bioes.txt'}, 'length': 533491}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1280, 1507, 1860, 1511, 1064, 236, 912, 1655, 1628, 1662]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <txt New> \n",
      "\n",
      "0 --> [ 导 读 ] 一 汽 丰 田 分 别 推 出 了 三 款 主 力 车 型 R E I Z 锐 志 、 C O R O L L A 卡 罗 拉 和 R A V 4 的 深 棕 云 母 金 属 色 炫 装 版 新 车 。\n",
      "1 --> 此 次 推 出 的 炫 装 版 车 型 共 包 括 3 款 车 型 9 个 级 别 , 除 在 商 品 性 上 有 所 提 升 外 , 性 价 比 均 有 不 同 程 度 的 提 升 。\n",
      "2 --> 3 月 8 日 , 一 汽 丰 田 汽 车 销 售 有 限 公 司 ( 以 下 简 称 “ 一 汽 丰 田 ” ) 联 手 时 尚 传 媒 集 团 在 北 京 7 9 8 园 区 D - P A R K 北 京 会 所 举 办 “ 跟 ‘ 棕 ’ 时 尚 有 型 有 色 ” 三 车 型 炫 装 版 联 合 发 布 会 , 一 汽 丰 田 领 导 高 放 副 总 经 理 、 北 京 服 装 设 计 师 协 会 刘 亚 桐 秘 书 长 、 中 国 先 锋 设 计 师 张 驰 、 著 名 造 型 师 小 P 老 师 、 著 名 影 星 冯 绍 峰 、 黄 海 波 以 及 各 界 时 尚 名 流 和 媒 体 嘉 宾 共 约 2 0 0 余 人 出 席 了 此 次 活 动 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "3 --> 新 西 兰 公 司 恒 天 然 ( F o n t e r r a C o - O p e r a t i v e G r o u p L t d . ) 周 五 表 示 , 正 寻 求 在 香 港 发 行 债 券 , 融 资 人 民 币 3 亿 元 ( 合 4 , 6 0 0 万 美 元 ) 。\n",
      "4 --> 该 公 司 库 务 总 经 理 S t e p h a n D e s c h a m p s 称 , 进 入 香 港 市 场 的 决 策 反 映 出 中 国 市 场 对 恒 天 然 业 务 日 益 凸 现 的 重 要 意 义 。\n",
      "5 --> 恒 天 然 称 , 他 们 是 首 家 涉 足 中 国 人 民 币 市 场 的 大 洋 洲 企 业 。\n",
      "6 --> 恒 天 然 中 国 总 裁 P h i l i p T u r n e r 表 示 , 此 次 发 债 所 得 将 用 于 支 持 该 公 司 中 国 业 务 的 发 展 。\n",
      "7 --> 中 金 在 线 声 明 : 中 金 在 线 转 载 上 述 内 容 , 不 表 明 证 实 其 描 述 , 仅 供 投 资 者 参 考 , 并 不 构 成 投 资 建 议 。\n",
      "8 --> 投 资 者 据 此 操 作 , 风 险 自 担 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "9 --> 明 星 们 的 激 情 戏 , 总 是 让 观 众 们 热 血 澎 湃 。\n",
      "10 --> 范 冰 冰 、 刘 德 华 、 舒 淇 、 林 心 如 、 韩 庚 , 看 看 明 星 们 在 镜 头 前 的 那 些 激 情 镜 头 吧 。\n",
      "11 --> 提 起 马 景 涛 , 大 家 首 先 想 到 的 肯 定 是 他 那 如 雷 贯 耳 滴 “ 马 氏 狂 吼 功 ” , 那 家 伙 , 每 回 吼 起 来 总 是 地 动 山 摇 、 江 河 倒 流 。\n",
      "12 --> 但 殊 不 知 景 涛 兄 的 激 情 戏 也 是 有 目 共 睹 , 每 回 总 能 看 得 观 众 心 惊 肉 跳 脸 红 脖 子 粗 。\n",
      "13 --> 有 一 回 蔡 康 永 问 他 “ 演 戏 这 么 用 力 会 不 会 累 啊 ? ” 马 景 涛 却 说 , “ 我 觉 得 用 力 演 戏 , 对 我 是 一 种 享 受 ! ” 据 说 马 景 涛 在 某 部 戏 中 与 陈 德 容 、 胡 可 等 女 星 拍 过 床 戏 , 结 果 硬 是 搞 塌 了 好 几 张 道 具 床 ! 连 工 作 人 员 都 叹 他 “ 床 上 功 夫 了 得 ! ”\n",
      "\n",
      " <txt New> \n",
      "\n",
      "14 --> 消 除 游 戏 玩 的 好 , 奖 励 就 收 集 的 多 , 感 觉 你 微 信 里 面 已 经 排 名 靠 前 了 ? 有 本 事 来 玩 @ 逆 转 三 国 挑 战 一 下 消 除 游 戏 的 极 限 , 让 你 知 道 我 玩 到 现 在 多 不 容 易 !\n",
      "\n",
      " <txt New> \n",
      "\n",
      "15 --> 而 对 于 消 费 者 最 为 关 注 的 2 1 . 5 寸 液 晶 显 示 器 中 , H K C 新 推 出 的 T 2 2 1 7 L “ 酷 变 小 N ” 最 为 吸 引 眼 球 。\n",
      "16 --> 它 是 来 自 亚 平 宁 半 岛 顶 尖 设 计 师 的 恢 宏 之 作 , 外 观 时 尚 可 与 显 示 器 相 媲 美 。\n",
      "17 --> 的 纤 薄 机 身 、 水 晶 般 璀 璨 的 飘 逸 外 形 、 可 折 叠 的 铝 合 金 底 座 、 优 雅 钢 琴 黑 与 科 技 太 空 银 的 搭 配 色 调 将 艺 术 与 功 能 完 美 的 结 合 在 一 起 。\n",
      "18 --> 并 且 其 底 座 采 用 独 具 匠 心 的 “ ” 双 重 使 用 模 式 设 计 , 立 卧 姿 态 随 心 调 节 , 轻 松 满 足 多 样 视 觉 观 看 需 求 , 追 求 精 致 的 使 用 体 验 , 放 置 更 为 随 意 自 由 。\n",
      "19 --> 高 端 的 L E D 1 0 8 0 P 全 面 板 屏 , 更 好 的 亮 度 均 匀 性 使 得 显 示 更 加 清 晰 、 亮 丽 , 无 论 你 是 游 戏 、 娱 乐 还 是 工 作 都 能 享 受 到 最 真 实 至 美 的 显 示 效 果 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "20 --> 最 后 来 围 观 一 下 “ 京 东 苏 宁 2 9 0 个 商 品 比 价 一 览 表 ” 里 面 的 爆 走 漫 画 。\n",
      "21 --> 商 城 比 价 , 最 后 受 益 的 还 是 某 些 爱 网 购 的 消 费 者 。\n",
      "22 --> 商 城 比 价 大 战 开 始 至 截 稿 时 已 经 过 了 约 5 个 半 小 时 , 很 多 低 价 商 品 已 经 跟 预 料 中 一 样 瞬 间 没 货 , 近 期 想 买 大 家 电 的 朋 友 还 是 捉 紧 下 单 吧 。\n",
      "23 --> 希 望 W P S O f f i c e 2 0 1 2 的 “ 京 东 苏 宁 2 9 0 个 商 品 比 价 一 览 表 ” 可 以 帮 到 大 家 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "24 --> 此 外 , 该 支 行 与 武 进 区 政 府 、 市 科 技 局 合 作 , 由 武 进 区 财 政 局 拿 出 1 0 0 0 万 元 作 为 科 技 性 企 业 信 贷 的 风 险 补 偿 资 金 , 支 持 科 技 支 行 投 放 1 亿 元 的 科 技 型 企 业 信 贷 资 金 。\n",
      "25 --> 如 果 申 请 贷 款 的 企 业 项 目 优 质 , 银 行 还 可 考 虑 增 加 更 多 信 贷 额 度 投 入 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "26 --> 小 汉 目 前 的 女 友 是 菲 律 宾 裔 美 国 歌 星 妮 可 ・ 舒 可 辛 格 , 她 是 前 著 名 女 子 团 体 小 野 猫 的 成 员 之 一 。\n",
      "27 --> 2 0 1 0 年 小 野 猫 解 散 后 , 舒 可 辛 格 单 飞 。\n",
      "28 --> 她 与 小 汉 就 是 在 2 0 0 7 年 慕 尼 黑 M T V 颁 奖 礼 上 相 遇 并 相 爱 的 。\n",
      "29 --> 也 许 因 为 本 身 是 歌 手 的 原 因 , 她 对 于 男 友 希 望 在 音 乐 道 路 上 发 展 的 计 划 一 直 给 予 支 持 , 但 是 也 并 未 太 过 积 极 , 插 手 参 与 他 的 歌 曲 录 制 。\n",
      "30 --> 至 于 小 汉 的 歌 星 之 路 是 否 能 像 他 的 F 1 之 路 那 么 成 功 , 车 迷 们 可 以 等 待 他 的 首 张 专 辑 发 行 后 再 做 评 价 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "31 --> 对 于 一 个 企 业 家 来 讲 算 这 样 一 个 帐 也 无 可 非 议 , 更 重 要 的 是 我 们 的 政 府 到 底 做 一 个 什 么 样 的 判 断 , 做 什 么 样 的 政 策 导 向 , 才 使 得 中 国 更 加 加 强 我 们 实 体 经 济 的 发 展 。\n",
      "32 --> 因 为 从 中 国 来 讲 , 1 3 亿 人 口 发 展 中 国 家 , 如 果 不 搞 经 济 可 能 出 大 的 问 题 。\n",
      "33 --> 特 别 是 当 前 面 临 的 一 个 很 大 问 题 , 就 是 实 现 经 济 转 型 。\n",
      "34 --> 经 济 转 型 的 内 涵 很 多 , 也 很 丰 富 。\n",
      "35 --> 可 以 说 对 于 不 同 发 展 阶 段 的 省 市 区 来 讲 , 经 济 发 展 的 内 涵 不 一 样 。\n",
      "36 --> 比 如 说 对 于 东 北 亚 地 区 主 要 的 内 容 之 一 就 是 进 行 更 深 入 的 经 济 结 构 调 整 , 培 育 一 批 战 略 新 兴 产 业 , 培 养 一 些 新 的 经 济 增 长 点 。\n",
      "37 --> 对 于 中 西 部 地 区 来 讲 还 是 怎 么 样 更 好 的 发 展 传 统 制 造 业 , 发 展 农 业 , 使 传 统 的 制 造 业 发 生 更 高 的 附 加 值 , 提 高 整 个 制 造 业 的 水 平 。\n",
      "\n",
      " <txt New> \n",
      "\n",
      "38 --> 毛 毛 有 定 时 开 窗 透 风 的 习 惯 , 夏 天 甚 至 整 晚 开 着 窗 。\n",
      "39 --> 对 毛 毛 来 说 , 租 住 在 这 个 仅 能 放 下 一 张 床 和 一 台 电 脑 桌 的 房 间 , 如 果 不 开 窗 , 她 会 有 一 种 窒 息 感 , “ 这 是 个 寸 土 寸 金 的 城 市 , 幸 好 空 气 不 要 钱 , 也 不 分 三 环 内 三 环 外 。 ”\n",
      "40 --> 但 这 些 天 持 续 笼 罩 京 城 的 雾 霾 , 让 毛 毛 不 禁 为 了 “ 开 窗 ” 而 纠 结 。\n",
      "41 --> 开 还 是 不 开 ? 是 屋 里 的 空 气 好 些 还 是 屋 外 的 空 气 好 些 ?\n",
      "42 --> 9 日 以 来 , 全 国 中 东 部 地 区 陷 入 严 重 的 雾 霾 和 污 染 天 中 , 中 央 气 象 台 将 大 雾 蓝 色 预 警 升 级 至 黄 色 预 警 , 环 保 部 门 的 数 据 显 示 , 从 东 北 到 到 西 北 , 从 华 北 到 中 部 导 致 黄 淮 、 江 南 地 区 , 都 出 现 了 大 范 围 的 重 度 和 严 重 污 染 。\n"
     ]
    }
   ],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "s,e = corpus.IdxFolderStartEnd\n",
    "print(s,e)\n",
    "s = corpus.GROUP['EndIDXTexts'][s-1] if s != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(e - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = corpus.GROUP['EndIDXTexts'][e-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.GROUP['EndIDXTexts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'注 意 观 察 会 发 现 , 这 两 个 版 本 的 平 板 代 号 前 缀 都 为 S M ;'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/boson/char'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Data_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<st 40 (tokenNum: 51) >,\n",
       " <st 41 (tokenNum: 52) >,\n",
       " <st 42 (tokenNum: 54) >,\n",
       " <st 43 (tokenNum: 55) >,\n",
       " <st 44 (tokenNum: 58) >,\n",
       " <st 45 (tokenNum: 77) >]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.text import Text\n",
    "loc_idx = 9  # how to interpret loc_idx: support there are n texts in the whole corpus, get the loc_idx th text\n",
    "txt = Text(loc_idx)\n",
    "txt.Sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.26px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
