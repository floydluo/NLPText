{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki/sample_wiki_smp.txt\n",
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "TOKEN\tis Dumped into file: data/wiki/char/Vocab/token.voc\n",
      "TOKEN\tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = False\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, annoKW = {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "\n",
    "### Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CORPUSPath': 'corpus/wiki/',\n",
       " 'Data_Dir': 'data/wiki/char',\n",
       " 'EndIDXGroups': array([1], dtype=uint32),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corpus2GroupMethod': '.txt',\n",
       " 'GroupType': 'File',\n",
       " 'group_names': ['corpus/wiki/sample_wiki_smp.txt'],\n",
       " 'EndIDXTexts': array([100], dtype=uint32),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group2TextMethod': 'line',\n",
       " 'EndIDXSents': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100], dtype=uint32),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  133,   309,   491,   681,   885,   989,  1082,  1139,  1239,\n",
       "         1383,  1571,  1890,  1972,  2121,  2321,  2505,  2737,  2942,\n",
       "         3156,  3335,  3604,  3936,  4119,  4146,  4241,  4307,  4421,\n",
       "         4599,  4777,  4817,  4838,  4906,  5262,  5511,  5718,  5783,\n",
       "         5993,  6284,  6420,  6543,  6723,  6738,  6791,  6876,  6936,\n",
       "         7002,  7076,  7114,  7143,  7203,  7313,  7467,  7594,  7755,\n",
       "         7878,  8165,  8214,  8396,  8607,  8824,  9009,  9197,  9253,\n",
       "         9347,  9487,  9624,  9675,  9953, 10027, 10241, 10479, 10634,\n",
       "        10868, 10939, 11167, 11249, 11257, 11446, 11515, 11633, 11904,\n",
       "        12068, 12144, 12261, 12525, 12603, 12754, 12878, 12895, 12947,\n",
       "        13070, 13099, 13153, 13246, 13365, 13456, 13540, 13616, 13813,\n",
       "        13878], dtype=uint32),\n",
       " 'data/wiki/char/Pyramid/_file/token.txt': array([  526,  1212,  1924,  2632,  3412,  3770,  4136,  4362,  4754,\n",
       "         5316,  6002,  7246,  7568,  8148,  8928,  9658, 10572, 11374,\n",
       "        12184, 12871, 13898, 15198, 15912, 16018, 16390, 16642, 17092,\n",
       "        17790, 18458, 18614, 18696, 18936, 20317, 21295, 22107, 22355,\n",
       "        23183, 24282, 24812, 25236, 25901, 25959, 26165, 26499, 26737,\n",
       "        26997, 27273, 27423, 27539, 27777, 28206, 28812, 29294, 29922,\n",
       "        30406, 31536, 31726, 32442, 33266, 34122, 34852, 35594, 35810,\n",
       "        36178, 36728, 37268, 37466, 38555, 38845, 39689, 40627, 41240,\n",
       "        42153, 42433, 43335, 43659, 43689, 44409, 44681, 45148, 46208,\n",
       "        46853, 47153, 47612, 48645, 48948, 49540, 49986, 50052, 50258,\n",
       "        50740, 50852, 51064, 51428, 51890, 52248, 52576, 52874, 53644,\n",
       "        53892], dtype=uint32),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'iter',\n",
       " 'TOKENLevel': 'char',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki/char/Pyramid/_file/token.txt'},\n",
       " 'length': 13878}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki/char/Pyramid/_file/token.txt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简', '塞', '拿', '济', '医', '幅', '韶', '狭', '功', '综', '癸', '喧', '势', '趋', '享', '钻', '尘', '嚣', '走', '司', '僧', '净', '父', '迪', '穆', '迈', '跟', '替', '麦', '品', '择', '魔', '占', '星', '燃', '隐', '藏', '收', '获', '控', '剧', '衰', '参', '廉', '邓', '破', '师', '乱', '死', '岭', '水', '钱', '赚', '旅', '瑟', '惑', '伸', '财', '脚', '坏', '案', '顺', '准', '渐', '令', '闭', '余', '逃', '珊', '王', '陆', '罕', '排', '斥', '野', '蛮', '黄', '媲', '打', '涅', '辛', '割', '赞', '板', '双', '翰', '护', '厘', '故', '贡', '刺', '讽', '企', '束', '梭', '卢', '宣', '拒', '伏', '恒', '逾', '官', '折', 'I', '规', '凭', '予', '奴', '仆', '浪', '漫', '‘', '’', '桑', '套', '民', '倡', '朽', '剂', '催', '鉴', '徒', '遭', '玄', '霍', '晋', '魏', '秀', '莫', '央', '族', '追', '韩', '愈', '翱', '驱', '颐', '框', '巫', '绪', '摆', '突', '鲜', '巨', '干', '服', '鼻', '祖', '诘', '诉', '圆', '码', '免', '避', '好', '胚', '撑', '迅', '握', '径', '傅', '妙', '优', '爆', '途', '弦', '错', '仔', '蕴', '涵', '叙', '坦', '供', 'Z', '拟', '轻', '芬', '低', '估', '忽', '装', '促', '紧', '洞', '路', '光', '遗', 'á', 'ó', '六', '委', '撰', '忠', '拾', '五', '耕', '杜', '钥', '迁', '柯', '轨', '衡', '短', '季', '乘', '石', '碑', '泥', '税', '丰', '惠', 'M', 'B', 'v', 'r', 'u', '月', '籍', '置', '递', '悉', '阴', ']', '[', '坑', '焚', '声', '阳', '农', '详', '旃', '拘', '婆', '钦', '翅', '摩', '诗', '歌', '颖', '佐', '雏', '末', '张', '载', '均', '秩', '偶', '左', '右', '男', '女', '静', '曲', '駄', '笩', '牟', '沌', '孪', '八', '{', 'w', '}', '偏', '邻', '勾', '纤', '丛', '俄', '凯', '冈', '诞', '硬', '吠', '媒', '介', '熵', '朋', '友', '辨', '辞', '器', '适', '纲', '叫', '南', '北', '须', '恼']\n"
     ]
    }
   ],
   "source": [
    "print(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([671, 441, 409, ...,   1,   1,   1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.idx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.min_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.current_vocab_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.original_vocab_token_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization from Saved NLPText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/wiki/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/wiki/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/wiki/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tread from pickle file : data/wiki/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tread from pickle file : data/wiki/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "Data_Dir = 'data/wiki/char/'\n",
    "min_token_freq = 2\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq = min_token_freq)\n",
    "print(len(BasicObject.TokenVocab[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CORPUSPath': 'corpus/wiki/',\n",
       " 'Data_Dir': 'data/wiki/char',\n",
       " 'EndIDXGroups': array([1], dtype=uint32),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corpus2GroupMethod': '.txt',\n",
       " 'GroupType': 'File',\n",
       " 'group_names': ['corpus/wiki/sample_wiki_smp.txt'],\n",
       " 'EndIDXTexts': array([100], dtype=uint32),\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group2TextMethod': 'line',\n",
       " 'EndIDXSents': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100], dtype=uint32),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  133,   309,   491,   681,   885,   989,  1082,  1139,  1239,\n",
       "         1383,  1571,  1890,  1972,  2121,  2321,  2505,  2737,  2942,\n",
       "         3156,  3335,  3604,  3936,  4119,  4146,  4241,  4307,  4421,\n",
       "         4599,  4777,  4817,  4838,  4906,  5262,  5511,  5718,  5783,\n",
       "         5993,  6284,  6420,  6543,  6723,  6738,  6791,  6876,  6936,\n",
       "         7002,  7076,  7114,  7143,  7203,  7313,  7467,  7594,  7755,\n",
       "         7878,  8165,  8214,  8396,  8607,  8824,  9009,  9197,  9253,\n",
       "         9347,  9487,  9624,  9675,  9953, 10027, 10241, 10479, 10634,\n",
       "        10868, 10939, 11167, 11249, 11257, 11446, 11515, 11633, 11904,\n",
       "        12068, 12144, 12261, 12525, 12603, 12754, 12878, 12895, 12947,\n",
       "        13070, 13099, 13153, 13246, 13365, 13456, 13540, 13616, 13813,\n",
       "        13878], dtype=uint32),\n",
       " 'data/wiki/char/Pyramid/_file/token.txt': array([  526,  1212,  1924,  2632,  3412,  3770,  4136,  4362,  4754,\n",
       "         5316,  6002,  7246,  7568,  8148,  8928,  9658, 10572, 11374,\n",
       "        12184, 12871, 13898, 15198, 15912, 16018, 16390, 16642, 17092,\n",
       "        17790, 18458, 18614, 18696, 18936, 20317, 21295, 22107, 22355,\n",
       "        23183, 24282, 24812, 25236, 25901, 25959, 26165, 26499, 26737,\n",
       "        26997, 27273, 27423, 27539, 27777, 28206, 28812, 29294, 29922,\n",
       "        30406, 31536, 31726, 32442, 33266, 34122, 34852, 35594, 35810,\n",
       "        36178, 36728, 37268, 37466, 38555, 38845, 39689, 40627, 41240,\n",
       "        42153, 42433, 43335, 43659, 43689, 44409, 44681, 45148, 46208,\n",
       "        46853, 47153, 47612, 48645, 48948, 49540, 49986, 50052, 50258,\n",
       "        50740, 50852, 51064, 51428, 51890, 52248, 52576, 52874, 53644,\n",
       "        53892], dtype=uint32),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'iter',\n",
       " 'TOKENLevel': 'char',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki/char/Pyramid/_file/token.txt'},\n",
       " 'length': 13878}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki/char/Pyramid/_file/token.txt'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简']\n"
     ]
    }
   ],
   "source": [
    "print(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([671, 441, 409, 321, 225, 164, 164, 157, 152, 150, 146, 134, 133,\n",
       "       115, 107,  97,  88,  78,  78,  77,  76,  71,  71,  67,  65,  64,\n",
       "        64,  64,  64,  64,  63,  62,  61,  60,  59,  58,  57,  56,  56,\n",
       "        56,  55,  54,  53,  53,  53,  52,  52,  52,  52,  51,  51,  51,\n",
       "        50,  49,  49,  48,  48,  47,  47,  46,  44,  44,  44,  43,  43,\n",
       "        43,  42,  42,  42,  42,  42,  41,  40,  40,  40,  40,  39,  39,\n",
       "        38,  38,  38,  37,  37,  37,  37,  37,  36,  36,  36,  36,  35,\n",
       "        35,  34,  34,  34,  34,  34,  33,  33,  33,  33,  33,  33,  33,\n",
       "        32,  32,  32,  32,  31,  31,  30,  30,  30,  29,  29,  29,  29,\n",
       "        29,  29,  29,  29,  28,  28,  28,  27,  27,  27,  27,  26,  26,\n",
       "        26,  26,  26,  26,  26,  25,  25,  25,  25,  25,  25,  25,  24,\n",
       "        24,  24,  24,  24,  24,  24,  23,  23,  23,  23,  23,  23,  22,\n",
       "        22,  22,  22,  22,  22,  22,  21,  21,  21,  21,  21,  21,  21,\n",
       "        21,  21,  20,  20,  20,  20,  20,  20,  20,  19,  19,  19,  19,\n",
       "        19,  19,  19,  19,  19,  19,  19,  18,  18,  18,  18,  18,  18,\n",
       "        18,  18,  18,  18,  18,  17,  17,  17,  17,  17,  17,  16,  16,\n",
       "        16,  16,  16,  16,  16,  16,  16,  16,  15,  15,  15,  15,  15,\n",
       "        15,  15,  15,  15,  15,  14,  14,  14,  14,  14,  14,  14,  14,\n",
       "        14,  14,  14,  14,  14,  13,  13,  13,  13,  13,  13,  13,  13,\n",
       "        13,  13,  13,  13,  13,  13,  13,  13,  13,  12,  12,  12,  12,\n",
       "        12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,\n",
       "        12,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "        11,  11,  11,  11,  11,  11,  11,  10,  10,  10,  10,  10,  10,\n",
       "        10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
       "        10,  10,  10,  10,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "         9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "         9,   9,   9,   9,   9,   8,   8,   8,   8,   8,   8,   8,   8,\n",
       "         8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
       "         8,   8,   8,   8,   8,   8,   8,   7,   7,   7,   7,   7,   7,\n",
       "         7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,\n",
       "         7,   7,   7,   7,   7,   7,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         2,   2,   2,   2,   2,   2,   2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.idx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.min_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.current_vocab_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.original_vocab_token_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getGrainVocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TU = BasicObject.getGrainVocab('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '学', ',', '。', '数', '为', '一', '是', '在', '和', '、', '理', '哲', '有', '中', '论', '了', '而', '不', '他', '其', '上', '时', '主', '对', '以', '题', '家', '认', '个', '之', '多', '被', '义', '及', '问', '科', '·', '代', '性', '世', '于', '人', '这', '尔', '与', '用', '自', '方', '拉', '”', '“', '德', '著', '斯', '系', '分', '定', '思', '现', '究', '可', '研', '来', '发', '实', '出', '然', '统', '形', '物', '们', ')', '域', '(', '要', '期', '大', '些', '如', '到', '化', '间', '本', '基', '作', '纪', '体', '领', '想', '古', '教', '包', '等', '同', '此', '希', '者', '展', '法', '明', '所', '地', '关', '成', '新', '文', '知', '存', '最', '并', '结', ':', '证', '经', '从', '识', '生', '公', '里', '且', '解', '量', '西', '神', '过', '念', '概', '更', '许', '重', '它', '都', '语', '图', '式', '「', '称', '相', '」', '即', '后', '质', '开', '年', '亚', '复', '立', '种', '或', '腊', '合', '常', '也', '始', '内', '特', '就', '国', '验', '变', '派', '含', '构', '由', '得', '使', '1', '度', '因', '意', '算', '响', '但', '至', '应', '则', '\"', '近', '影', '柏', '亦', '兴', '逻', '辑', '观', '元', '前', '计', '会', '那', '史', '格', '源', '括', '两', ';', '产', '普', '卡', '典', '加', '确', '几', '界', '当', '述', '心', '马', '名', '士', '罗', '何', '通', '精', '限', '部', '说', '术', '0', '词', '象', '创', '础', '空', '直', '点', '奖', '另', '《', '维', '进', '道', '三', '整', '尼', '建', '》', '将', '推', '日', '唯', '般', '笛', '起', '做', '提', '达', '历', '身', '早', '今', '6', '-', '艺', '决', '无', '动', '程', '运', '表', '答', '子', '言', '例', '事', '广', '美', '已', '析', '真', '诺', '还', '遍', '觉', '严', '脑', '哥', '二', '纯', '治', '只', '很', '面', '工', '别', '伊', 'h', '克', '指', '符', '信', '步', '描', '先', '感', '疑', '电', '能', '阿', '看', '欧', '9', '字', '抽', '高', '机', '传', '—', '份', '布', '康', '比', 'i', '集', '政', '才', '较', '把', '智', '角', '致', '争', '宋', '2', '谨', '导', '天', '.', '十', '果', '正', '外', '调', '反', '猜', '莱', '类', '号', '东', '伦', '需', '具', '印', '利', '托', '趣', '书', '样', '怀', '互', '流', '越', '万', '非', '院', '处', '四', '测', '第', '单', '伯', '受', '初', '又', '入', '费', '着', '佛', '难', '贝', '微', '毕', '善', '没', '苏', '专', '批', '曼', '际', '爱', '下', '依', '绝', '仍', '力', '极', '督', '诸', '原', '底', '黑', '积', '转', '百', '3', '件', '组', '注', '连', '某', '8', '索', '门', '归', '假', '演', '接', '记', '洲', '便', '易', '丁', '谟', '色', '拓', '7', '根', '切', '什', '…', '取', 'a', '每', '总', '九', '活', '灵', '辩', '我', '考', '行', '千', '函', '放', '儒', '向', 'e', '容', '判', '必', '示', 'm', '却', '望', 'o', '资', '据', '密', '远', '显', '波', '否', '杂', '约', '探', '试', '少', '快', '位', '预', 'n', '皮', '未', '宗', '宾', '议', '范', 'P', '深', '完', '蒙', '序', '萨', '全', '续', '客', '脱', '慧', '离', '独', '泰', '宙', '宇', '么', '属', '群', '回', '莎', '勒', '话', '5', '小', '裂', '超', '断', '助', '土', '视', '持', '讯', '引', '鸠', '鲁', '朝', '业', '见', '启', '帝', '段', '速', '设', '随', '扮', '评', '模', '乐', '英', '周', '社', '己', 'l', \"'\", '命', '既', '革', '仰', '夫', '混', 'p', '差', '扑', '写', '习', '往', 's', '洛', '终', '皆', '管', '值', '标', 't', '汉', '牛', '察', '查', '顿', '核', '若', '泛', '械', '造', '休', '讨', '去', '困', '俗', '埃', '奥', '译', '坚', '除', '清', '威', '让', '巴', '兰', '逐', '料', '涉', '再', '七', '库', '墨', '区', '育', '畴', '众', '尤', '吉', '秘', '奎', '长', '环', '林', '团', 'k', '权', '移', '志', '朗', '秦', '尚', '略', '承', '培', '安', '强', '条', '奇', '曾', '兹', '带', '共', '葛', '备', '换', '阶', '键', '求', '延', '富', '迦', '永', '项', '次', 'c', '哈', '白', '足', '架', '散', '制', '技', '弗', '型', '禧', '尽', '熟', '帮', '各', '支', '价', '赫', '列', '戈', '庞', '舍', '细', '该', '股', '情', '误', '太', '粹', '纳', '联', '首', '沃', '效', '金', '谓', '虽', '雅', '版', '修', '尝', '扩', '绎', '缺', '孟', '固', '姆', '渡', '谐', '划', '丹', '串', 'J', '赋', '章', '谢', '允', '辅', '节', 'N', '减', '仅', '蒂', 'G', 'z', '齐', 'F', '线', 'W', '敦', '异', '潮', '狄', '透', '目', '商', '儿', '茨', '阐', '审', '状', '员', '况', '借', '唐', '皇', '伽', '久', '/', '靠', '伴', '选', '契', '帕', '焦', '博', '努', '艾', '任', '譬', '态', '献', '律', '改', '晚', '犹', '圣', '谈', '4', '巧', '卫', '颁', '叶', '菲', '刊', '花', '迷', 'y', '检', '恶', 'S', '交', '够', '彼', '孔', '跃', '平', '融', '释', '陀', '织', '耆', '申', '似', '给', '凡', '寻', '胡', '素', '翻', '老', '像', '海', '歧', '增', '米', '溯', '掌', '候', '肯', '贸', '务', '壁', '殊', '雷', '佩', '弃', '旧', '李', '盛', '封', '储', '木', '围', '录', '暗', '陷', '缩', '激', '率', '良', '找', '盾', '落', '压', '驳', '息', '音', '编', '黎', '矛', '奠', '简']\n"
     ]
    }
   ],
   "source": [
    "# idx2token\n",
    "print(TU[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Overview of Pyramid\n",
    "\n",
    "\n",
    "Show the pyramid structures: Corpus, Folder, Text, Sentence, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1.病史：患者为63岁女性，慢性病程，急性加重。', '既往有“高脂血症”病史。', '2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
       " ['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。', '双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
       " ['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
       "  '心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
       "  '腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
       "  '肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
       "  '双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    # count the string's byte number\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "{'1': 0, '.': 1, '病': 2, '史': 3, '：': 4, '患': 5, '者': 6, '为': 7, '6': 8, '3': 9, '岁': 10, '女': 11, '性': 12, '，': 13, '慢': 14, '程': 15, '急': 16, '加': 17, '重': 18, '。': 19, '既': 20, '往': 21, '有': 22, '“': 23, '高': 24, '脂': 25, '血': 26, '症': 27, '”': 28, '2': 29, '因': 30, '反': 31, '复': 32, '脐': 33, '周': 34, '疼': 35, '痛': 36, '年': 37, '余': 38, '再': 39, '发': 40, '并': 41, '入': 42, '院': 43, '体': 44, '查': 45, '压': 46, '8': 47, '/': 48, 'm': 49, 'H': 50, 'g': 51, '神': 52, '志': 53, '清': 54, '楚': 55, '浅': 56, '表': 57, '淋': 58, '巴': 59, '结': 60, '无': 61, '肿': 62, '大': 63, '口': 64, '唇': 65, '苍': 66, '白': 67, '双': 68, '侧': 69, '扁': 70, '桃': 71, '、': 72, '充': 73, '咽': 74, '颈': 75, '静': 76, '脉': 77, '怒': 78, '张': 79, '肺': 80, '呼': 81, '吸': 82, '音': 83, '晰': 84, '未': 85, '闻': 86, '及': 87, '干': 88, '湿': 89, '啰': 90, '胸': 91, '膜': 92, '摩': 93, '擦': 94, '心': 95, '率': 96, '次': 97, '分': 98, '律': 99, '齐': 100, '各': 101, '瓣': 102, '区': 103, '理': 104, '杂': 105, '腹': 106, '部': 107, '平': 108, '坦': 109, '见': 110, '胃': 111, '肠': 112, '型': 113, '蠕': 114, '动': 115, '波': 116, '壁': 117, '柔': 118, '软': 119, '跳': 120, '扪': 121, '包': 122, '块': 123, '肝': 124, '脾': 125, '肋': 126, '下': 127, 'M': 128, 'u': 129, 'r': 130, 'p': 131, 'h': 132, 'y': 133, '征': 134, '（': 135, '-': 136, '）': 137, '肾': 138, '叩': 139, '击': 140, '移': 141, '浊': 142, '鸣': 143, '4': 144, '肢': 145, '浮': 146, '四': 147, '肌': 148, '力': 149, '正': 150, '常': 151, '生': 152, '射': 153, '存': 154, '在': 155, '引': 156, '出': 157}\n",
      "['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '程', '急', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '2', '因', '反', '复', '脐', '周', '疼', '痛', '年', '余', '再', '发', '并', '入', '院', '体', '查', '压', '8', '/', 'm', 'H', 'g', '神', '志', '清', '楚', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '口', '唇', '苍', '白', '双', '侧', '扁', '桃', '、', '充', '咽', '颈', '静', '脉', '怒', '张', '肺', '呼', '吸', '音', '晰', '未', '闻', '及', '干', '湿', '啰', '胸', '膜', '摩', '擦', '心', '率', '次', '分', '律', '齐', '各', '瓣', '区', '理', '杂', '腹', '部', '平', '坦', '见', '胃', '肠', '型', '蠕', '动', '波', '壁', '柔', '软', '跳', '扪', '包', '块', '肝', '脾', '肋', '下', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肾', '叩', '击', '移', '浊', '鸣', '4', '肢', '浮', '四', '肌', '力', '正', '常', '生', '射', '存', '在', '引', '出']\n",
      "[3, 3, 5, 2, 2, 1, 1, 1, 3, 3, 1, 1, 6, 21, 1, 1, 1, 2, 2, 12, 1, 1, 1, 2, 1, 1, 4, 1, 2, 4, 1, 4, 1, 2, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 9, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 6, 1, 6, 2, 5, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SENT = []\n",
    "\n",
    "processed_data_path = 'processed_data.txt'\n",
    "\n",
    "\n",
    "DTU = {}\n",
    "LTU = []\n",
    "index2freq = []\n",
    "# data = np.zeros(5000, dtype= np.uint32)\n",
    "token_num_in_corpus = 0\n",
    "\n",
    "if os.path.isfile(processed_data_path):\n",
    "    os.remove(processed_data_path)\n",
    "        \n",
    "for text in corpus:\n",
    "    for strSent in text:\n",
    "        strTokens = [i for i in strSent]\n",
    "        for token in strTokens:\n",
    "            if token not in DTU:\n",
    "                # deal with new words\n",
    "                token_idx  = len(DTU)\n",
    "                DTU[token] = token_idx\n",
    "                index2freq.append(1)\n",
    "                LTU.append(token)\n",
    "            else:\n",
    "                # deal with old words\n",
    "                token_idx = DTU[token]\n",
    "                index2freq[token_idx] += 1\n",
    "            # data.append(token_idx)\n",
    "            # data[token_num_in_corpus] = token_idx\n",
    "            token_num_in_corpus = token_num_in_corpus + 1\n",
    "            \n",
    "        with open(processed_data_path, 'a') as f:\n",
    "            line_sentence = ' '.join(strTokens) + '\\n'\n",
    "            f.write(line_sentence)\n",
    "            SENT.append(utf8len(line_sentence))\n",
    "            \n",
    "            \n",
    "            \n",
    "data = data[: token_num_in_corpus]\n",
    "print(len(data))\n",
    "# print(data)\n",
    "print(DTU)\n",
    "print(LTU)\n",
    "print(index2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  88,  136,  228,  356,  452,  548,  638,  840,  934, 1066])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentIdx = np.cumsum(SENT)\n",
    "sentIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "sent_idx = 2 # the three line\n",
    "sent_pos_start = sentIdx[sent_idx - 1]\n",
    "print(sent_pos_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_with_position(processed_data_path,start_position):\n",
    "    with open(processed_data_path, 'r') as f:\n",
    "        f.seek(start_position)\n",
    "        line = f.readline() # there is no 's' in f.readline()\n",
    "        # last_pos = f.tell()\n",
    "        # f.close()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 12,  9,  6,  6,  6,  5,  5,  4,  4,  4,  4,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index2freq = np.sort(index2freq)[::-1]\n",
    "new_index2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,  19,  61,  83,  12,  85,  87,   2,  29,  31,  36,  26,   0,\n",
       "        62,   9,   1,  34,  68,  48, 104,   8, 115,  63, 112, 106, 103,\n",
       "        54,  98,  44,  46,  92,  49,  28,  72,  73,  86,  79,  97,  33,\n",
       "       135, 149, 136, 121, 145,  17,  18, 127, 148, 137,  23,   4, 153,\n",
       "         3, 124,  14,  55,   6,   7,  53,  59,   5,  52,  57,  51,  10,\n",
       "        58,  11,  60,  56,  27,  50,  47,  30,  25,  24,  32,  65,  35,\n",
       "        22,  37,  38,  21,  20,  39,  40,  41,  42,  43,  16,  45,  15,\n",
       "        64, 157,  66, 114, 117, 118, 119, 120, 122, 123, 125, 126, 128,\n",
       "       129, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144,\n",
       "       146, 147, 150, 151, 152, 154, 155, 116, 113,  67, 111,  69,  70,\n",
       "        71,  74,  75,  76,  77, 156,  80,  81,  82,  84,  88,  89,  90,\n",
       "        91,  93,  94,  95,  96,  99, 100, 101, 102, 105, 107, 108, 109,\n",
       "       110,  78])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newidx2oldidx = np.argsort(index2freq)[::-1]\n",
    "newidx2oldidx# [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  15,   7,  52,  50,  60,  56,  57,  20,  14,  64,  66,   4,\n",
       "         0,  54,  90,  88,  44,  45,   1,  82,  81,  78,  49,  74,  73,\n",
       "        11,  69,  32,   8,  72,   9,  75,  38,  16,  77,  10,  79,  80,\n",
       "        83,  84,  85,  86,  87,  28,  89,  29,  71,  18,  31,  70,  63,\n",
       "        61,  58,  26,  55,  68,  62,  65,  59,  67,   2,  13,  22,  91,\n",
       "        76,  93, 126,  17, 128, 129, 130,  33,  34, 131, 132, 133, 134,\n",
       "       157,  36, 136, 137, 138,   3, 139,   5,  35,   6, 140, 141, 142,\n",
       "       143,  30, 144, 145, 146, 147,  37,  27, 148, 149, 150, 151,  25,\n",
       "        19, 152,  24, 153, 154, 155, 156, 127,  23, 125,  94,  21, 124,\n",
       "        95,  96,  97,  98,  42,  99, 100,  53, 101, 102,  46, 103, 104,\n",
       "       105, 106, 107, 108, 109,  39,  41,  48, 110, 111, 112, 113, 114,\n",
       "       115, 116,  43, 117, 118,  47,  40, 119, 120, 121,  51, 122, 123,\n",
       "       135,  92])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldidx2newidx = np.zeros(len(newidx2oldidx), dtype= int)\n",
    "\n",
    "for new_idx, old_idx in enumerate(newidx2oldidx):\n",
    "    oldidx2newidx[old_idx] = new_idx\n",
    "    \n",
    "oldidx2newidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['，', '。', '无', '音', '性', '未', '及', '病', '2', '反', '痛', '血', '1', '肿', '3', '.', '周', '双', '/', '理', '6', '动', '大', '肠', '腹', '区', '清', '分', '体', '压', '膜', 'm', '”', '、', '充', '闻', '张', '次', '脐', '（', '力', '-', '扪', '肢', '加', '重', '下', '肌', '）', '“', '：', '射', '史', '肝', '慢', '楚', '者', '为', '志', '巴', '患', '神', '表', 'g', '岁', '淋', '女', '结', '浅', '症', 'H', '8', '因', '脂', '高', '复', '唇', '疼', '有', '年', '余', '往', '既', '再', '发', '并', '入', '院', '急', '查', '程', '口', '出', '苍', '蠕', '壁', '柔', '软', '跳', '包', '块', '脾', '肋', 'M', 'u', 'r', 'p', 'h', 'y', '征', '肾', '叩', '击', '移', '浊', '鸣', '4', '浮', '四', '正', '常', '生', '存', '在', '波', '型', '白', '胃', '侧', '扁', '桃', '咽', '颈', '静', '脉', '引', '肺', '呼', '吸', '晰', '干', '湿', '啰', '胸', '摩', '擦', '心', '率', '律', '齐', '各', '瓣', '杂', '部', '平', '坦', '见', '怒']\n",
      "{'，': 0, '。': 1, '无': 2, '音': 3, '性': 4, '未': 5, '及': 6, '病': 7, '2': 8, '反': 9, '痛': 10, '血': 11, '1': 12, '肿': 13, '3': 14, '.': 15, '周': 16, '双': 17, '/': 18, '理': 19, '6': 20, '动': 21, '大': 22, '肠': 23, '腹': 24, '区': 25, '清': 26, '分': 27, '体': 28, '压': 29, '膜': 30, 'm': 31, '”': 32, '、': 33, '充': 34, '闻': 35, '张': 36, '次': 37, '脐': 38, '（': 39, '力': 40, '-': 41, '扪': 42, '肢': 43, '加': 44, '重': 45, '下': 46, '肌': 47, '）': 48, '“': 49, '：': 50, '射': 51, '史': 52, '肝': 53, '慢': 54, '楚': 55, '者': 56, '为': 57, '志': 58, '巴': 59, '患': 60, '神': 61, '表': 62, 'g': 63, '岁': 64, '淋': 65, '女': 66, '结': 67, '浅': 68, '症': 69, 'H': 70, '8': 71, '因': 72, '脂': 73, '高': 74, '复': 75, '唇': 76, '疼': 77, '有': 78, '年': 79, '余': 80, '往': 81, '既': 82, '再': 83, '发': 84, '并': 85, '入': 86, '院': 87, '急': 88, '查': 89, '程': 90, '口': 91, '出': 92, '苍': 93, '蠕': 94, '壁': 95, '柔': 96, '软': 97, '跳': 98, '包': 99, '块': 100, '脾': 101, '肋': 102, 'M': 103, 'u': 104, 'r': 105, 'p': 106, 'h': 107, 'y': 108, '征': 109, '肾': 110, '叩': 111, '击': 112, '移': 113, '浊': 114, '鸣': 115, '4': 116, '浮': 117, '四': 118, '正': 119, '常': 120, '生': 121, '存': 122, '在': 123, '波': 124, '型': 125, '白': 126, '胃': 127, '侧': 128, '扁': 129, '桃': 130, '咽': 131, '颈': 132, '静': 133, '脉': 134, '引': 135, '肺': 136, '呼': 137, '吸': 138, '晰': 139, '干': 140, '湿': 141, '啰': 142, '胸': 143, '摩': 144, '擦': 145, '心': 146, '率': 147, '律': 148, '齐': 149, '各': 150, '瓣': 151, '杂': 152, '部': 153, '平': 154, '坦': 155, '见': 156, '怒': 157}\n"
     ]
    }
   ],
   "source": [
    "new_LTU = []\n",
    "for new_idx in range(len(LTU)):\n",
    "    new_LTU.append(LTU[newidx2oldidx[new_idx]])\n",
    "    \n",
    "print(new_LTU)\n",
    "new_DTU = {}\n",
    "for new_idx, token in enumerate(new_LTU):\n",
    "    new_DTU[token] = new_idx\n",
    "    \n",
    "print(new_DTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  15,   7,  52,  50,  60,  56,  57,  20,  14,  64,  66,   4,\n",
       "         0,  54,   4,   7,  90,   0,  88,   4,  44,  45,   1,  82,  81,\n",
       "        78,  49,  74,  73,  11,  69,  32,   7,  52,   1,   8,  15,  72,\n",
       "        49,   9,  75,  38,  16,  77,  10,   8,  79,  80,   0,  83,  84,\n",
       "        85,  44,  45,  12,  16,  32,  86,  87,   1,  14,  15,  28,  89,\n",
       "        50,  11,  29,  12,   8,  71,  18,  20,  14,  31,  31,  70,  63,\n",
       "         0,  61,  58,  26,  55,   0,  68,  62,  65,  59,  67,   2,  13,\n",
       "        22,   0,  91,  76,   2,  93, 126,   1,  17, 128, 129, 130,  28,\n",
       "         2,  13,  22,  33,  34,  11,   1, 131,   2,  34,  11,   1, 132,\n",
       "       133, 134,   2, 157,  36,   1,  17, 136, 137, 138,   3,  26, 139,\n",
       "         0,   5,  35,   6, 140, 141,   4, 142,   3,   0,   2, 143,  30,\n",
       "       144, 145,   3,   1, 146, 147,  20,   8,  37,  18,  27,   0, 148,\n",
       "       149,   0, 150, 151,  30,  25,   5,  35,   6,   7,  19,   4, 152,\n",
       "         3,   1,  24, 153, 154, 155,   0,   5, 156, 127,  23, 125,   6,\n",
       "        94,  21, 124,   0,  24,  95,  96,  97,   0,  38,  16,  29,  10,\n",
       "         0,   2,   9,  98,  10,   0,   5,  42,   6,  99, 100,   0,  53,\n",
       "       101, 102,  46,   5,  42,   6,   0, 103, 104, 105, 106, 107, 108,\n",
       "       109,  39,  41,  48,  53, 110,  25,   2, 111, 112,  10,   0, 113,\n",
       "        21,   4, 114,   3,  39,  41,  48,   0,  23, 115,   3, 116,  37,\n",
       "        18,  27,   1,  17,  46,  43,   2, 117,  13,   1, 118,  43,  47,\n",
       "        40,  33,  47,  36,  40, 119, 120,   0, 121,  19,   9,  51, 122,\n",
       "       123,   0,   7,  19,   9,  51,   5, 135,  92,   1], dtype=uint32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, oldidx in enumerate(data):\n",
    "    data[idx] = oldidx2newidx[oldidx]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  Level Dictionary\n",
      "{'NUMSents': [3, 2, 5], 'EndIDXSents': [3, 63, 128]}\n",
      "\n",
      "Sent  Level Dictionary\n",
      "{'NUMTokens': [24, 12, 25, 38, 24, 24, 24, 54, 25, 33], 'EndIDXTokens': [24, 36, 61, 99, 123, 147, 171, 225, 250, 283]}\n",
      "\n",
      "Token Level Dictionary\n",
      "{'DATAToken': ['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '性', '病', '程', '，', '急', '性', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '病', '史', '。', '2', '.', '因', '“', '反', '复', '脐', '周', '疼', '痛', '2', '年', '余', '，', '再', '发', '并', '加', '重', '1', '周', '”', '入', '院', '。', '3', '.', '体', '查', '：', '血', '压', '1', '2', '8', '/', '6', '3', 'm', 'm', 'H', 'g', '，', '神', '志', '清', '楚', '，', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '，', '口', '唇', '无', '苍', '白', '。', '双', '侧', '扁', '桃', '体', '无', '肿', '大', '、', '充', '血', '。', '咽', '无', '充', '血', '。', '颈', '静', '脉', '无', '怒', '张', '。', '双', '肺', '呼', '吸', '音', '清', '晰', '，', '未', '闻', '及', '干', '湿', '性', '啰', '音', '，', '无', '胸', '膜', '摩', '擦', '音', '。', '心', '率', '6', '2', '次', '/', '分', '，', '律', '齐', '，', '各', '瓣', '膜', '区', '未', '闻', '及', '病', '理', '性', '杂', '音', '。', '腹', '部', '平', '坦', '，', '未', '见', '胃', '肠', '型', '及', '蠕', '动', '波', '，', '腹', '壁', '柔', '软', '，', '脐', '周', '压', '痛', '，', '无', '反', '跳', '痛', '，', '未', '扪', '及', '包', '块', '，', '肝', '脾', '肋', '下', '未', '扪', '及', '，', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肝', '肾', '区', '无', '叩', '击', '痛', '，', '移', '动', '性', '浊', '音', '（', '-', '）', '，', '肠', '鸣', '音', '4', '次', '/', '分', '。', '双', '下', '肢', '无', '浮', '肿', '。', '四', '肢', '肌', '力', '、', '肌', '张', '力', '正', '常', '，', '生', '理', '反', '射', '存', '在', '，', '病', '理', '反', '射', '未', '引', '出', '。']}\n"
     ]
    }
   ],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '性', '病', '程', '，', '急', '性', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '病', '史', '。', '2', '.', '因', '“', '反', '复', '脐', '周', '疼', '痛', '2', '年', '余', '，', '再', '发', '并', '加', '重', '1', '周', '”', '入', '院', '。', '3', '.', '体', '查', '：', '血', '压', '1', '2', '8', '/', '6', '3', 'm', 'm', 'H', 'g', '，', '神', '志', '清', '楚', '，', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '，', '口', '唇', '无', '苍', '白', '。', '双', '侧', '扁', '桃', '体', '无', '肿', '大', '、', '充', '血', '。', '咽', '无', '充', '血', '。', '颈', '静', '脉', '无', '怒', '张', '。', '双', '肺', '呼', '吸', '音', '清', '晰', '，', '未', '闻', '及', '干', '湿', '性', '啰', '音', '，', '无', '胸', '膜', '摩', '擦', '音', '。', '心', '率', '6', '2', '次', '/', '分', '，', '律', '齐', '，', '各', '瓣', '膜', '区', '未', '闻', '及', '病', '理', '性', '杂', '音', '。', '腹', '部', '平', '坦', '，', '未', '见', '胃', '肠', '型', '及', '蠕', '动', '波', '，', '腹', '壁', '柔', '软', '，', '脐', '周', '压', '痛', '，', '无', '反', '跳', '痛', '，', '未', '扪', '及', '包', '块', '，', '肝', '脾', '肋', '下', '未', '扪', '及', '，', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肝', '肾', '区', '无', '叩', '击', '痛', '，', '移', '动', '性', '浊', '音', '（', '-', '）', '，', '肠', '鸣', '音', '4', '次', '/', '分', '。', '双', '下', '肢', '无', '浮', '肿', '。', '四', '肢', '肌', '力', '、', '肌', '张', '力', '正', '常', '，', '生', '理', '反', '射', '存', '在', '，', '病', '理', '反', '射', '未', '引', '出', '。']\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "for newidx in data:\n",
    "    token_list.append(new_LTU[newidx])\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n"
     ]
    }
   ],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus, Group and from Corpus to Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "    \n",
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group, Text and from Group to Texts\n",
    "\n",
    "Each text is a tuple for strText, SSET, orig_file_name, anno_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "FolderTextsReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.Entity'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/medpos/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/boson/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = '.txt')[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'line'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "     \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strText is then be spilted into sentences.\n",
    "\n",
    "If SSET exists, we need to match SSET and the splited sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text, Sentence and from Text to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##################################################################################################TEXT-SENT\n",
    "def reCutText2Sent(text, useSep = False):\n",
    "    \n",
    "    ###################### Remove some weird chars #######################\n",
    "    text = re.sub('\\xa0', '', text)\n",
    "    \n",
    "    ############# The Issue of Spaces\n",
    "    ###################### Convert the Spaces between two English Letters to 'ⴷ' #################\n",
    "    # Take care of Spaces\n",
    "    text = re.sub(r'(?<=[A-Za-z])\\s+(?=[|A-Za-z])', 'ⴷ',  text)\n",
    "    \n",
    "    ###################### Convert the S+ spaces to '〰' #################\n",
    "    text = re.sub(' {2}', '〰', text ).strip()\n",
    "    if useSep == ' ':\n",
    "        # if using space to sep the words\n",
    "        text = text.replace('\\t','').replace('〰', ' ')\n",
    "    elif useSep == '\\t':\n",
    "        # if using tab to sep the words, removing all spaces\n",
    "        text = text.replace(' ','').replace('〰', '')\n",
    "    else:\n",
    "        # if there is no sep char for Chinese, remove single space, and then convert space+ to single space\n",
    "        text = text.replace('\\t','').replace(' ', '',).replace('〰', ' ')\n",
    "        \n",
    "    # convert the spaces between English letters to single spaces\n",
    "    text = text.replace('ⴷ', ' ')\n",
    "    \n",
    "    # Other Things\n",
    "    text = re.sub('([。！!;；])([^”])', r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])',    r\"\\1\\n\\2\",text)\n",
    "    \n",
    "    # The \\n within \" \" is not considered\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [sent.strip() for sent in text]\n",
    "    # text = [sent.replace(' ', '').replace('\\\\n', '') for sent in text]\n",
    "    return [sent for sent in text if len(sent)>=2]\n",
    "\n",
    "\n",
    "text = '浙江在线杭州4月25日讯(记者施宇翔 通讯员 方英)毒贩很“时髦”,用微信交易毒品。没料想警方也很“潮”,将计就计,一举将其擒获。记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。钱某不寻常的特征,引起了警方注意。禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。'\n",
    "print(text)\n",
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole', **kwargs):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath. 2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.        \n",
    "    '''\n",
    "    if os.path.isfile(text):\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    elif method == 're':\n",
    "        return reCutText2Sent(text, **kwargs)\n",
    "    else:\n",
    "        return method(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence, Token and from Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data\n",
    "\n",
    "## From Corpus to Texts\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "Folders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "\n",
    "pprint(Folders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "\n",
    "for folderPath in Folders:\n",
    "    print(folderPath)\n",
    "    fileNames = Folders[folderPath]\n",
    "    \n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        print(textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Corpus to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "# STAGE 2\n",
    "from nlptext.utils.pyramid import reCutText2Sent\n",
    "from nlptext.utils.pyramid import segText2Sents, segSent2Tokens# (text, method = 'whole')\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'corpus/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed' # TODO\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "################   Things to Save   ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['corpusFileIden'] = corpusFileIden # None if Dir else\n",
    "CORPUS['CORPUSType']     = 'File' if corpusFileIden else 'Dir'\n",
    "CORPUS['textType'] = textType\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['folderPaths'] = [] \n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "        \n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['Text2SentMethod'] = Text2SentMethod\n",
    "if textType == 'file':\n",
    "    TEXT['ORIGFileName'] = []\n",
    "if anno:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "    \n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "SENT['Sent2TokenMethod'] = Sent2TokenMethod\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "TOKEN['TOKENLevel'] = TOKENLevel\n",
    "if anno:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "ANNO = {}\n",
    "ANNO['anno'] = anno\n",
    "ANNO['annoKW'] = annoKW\n",
    "\n",
    "    \n",
    "    \n",
    "########################################################\n",
    "##################     CHAINES      ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "###--> CHAIN: from Corpus to Folders <--###\n",
    "\n",
    "CorpusFolders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "assert CORPUS['CORPUSType'] == CORPUSType\n",
    "pprint(CorpusFolders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "for folderIdx, folderPath in enumerate(CorpusFolders):\n",
    "    print(folderPath)\n",
    "    fileNames = CorpusFolders[folderPath]\n",
    "    \n",
    "    ###--> CHAIN: from Folder to Texts <--###\n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        \n",
    "        print('\\n', textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        \n",
    "        ###--> CHAIN: from strText to strSents <--###\n",
    "        strSents = segText2Sents(strText, method = Text2SentMethod) # fixed\n",
    "        \n",
    "        for strSent in strSents:\n",
    "            #- print(strSent)\n",
    "            ###--> CHAIN: from strSent to strTokens <--###\n",
    "            strTokens = segSent2Tokens(strSent, method = Sent2TokenMethod)\n",
    "            \n",
    "            ###--> CHAIN's End: Token itself <--###\n",
    "            #- print(strTokens)\n",
    "            TOKEN['ORIGToken'].extend(strTokens)\n",
    "            \n",
    "            lenSent = len(strTokens)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "            \n",
    "        \n",
    "        lenText = len(strSents)\n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "            \n",
    "        if origTextName:\n",
    "            TEXT['ORIGFileName'].append(origTextName)\n",
    "            \n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "    \n",
    "    # Back to Folder\n",
    "    lenFolder = textIdx\n",
    "    FOLDER['folderPaths'].append(folderPath)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder) # to remove\n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "        \n",
    "# End here\n",
    "lenCorpus = folderIdx\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtained Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text\n",
    "loc_idx = 9  # how to interpret loc_idx: support there are n texts in the whole corpus, get the loc_idx th text\n",
    "txt = Text(loc_idx)\n",
    "txt.Tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
