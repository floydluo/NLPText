{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Init DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nlptext.base import BasicObject\n",
    "# BOB = 'data/LuohuCorpus/char/Token3546/Pyramid'\n",
    "# LGU = 'data/LuohuCorpus/char/Token3546/GrainUnique'\n",
    "# BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "    \n",
    "    \n",
    "    \n",
    "# corpus = Corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders = corpus.Folders\n",
    "# folders\n",
    "# f = folders[0]\n",
    "# f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chief_complaints = [f for f in folders if 'RANinfos_ChiefComplaint' in f.name or 'FirstCorinfos_Basicinfo' in f.name]\n",
    "# pprint(chief_complaints)\n",
    "# def get_sent_len(f):\n",
    "#     s,e = f.IdxSentStartEnd\n",
    "#     return e-s\n",
    "\n",
    "# [get_sent_len(f) for f in chief_complaints]\n",
    "\n",
    "\n",
    "\n",
    "# from nlptext.sentence import Sentence\n",
    "# f  = chief_complaints[0]\n",
    "\n",
    "# s, e = f.IdxSentStartEnd\n",
    "# all_sentence = [Sentence(i)  for i in range(s, e)]\n",
    "# all_sentence = [st for st in all_sentence if len(st.sentence) >10]\n",
    "\n",
    "# # f2 = chief_complaints[1]\n",
    "\n",
    "# # s, e = f2.IdxSentStartEnd\n",
    "# # all_sentence2 = [Sentence(i)  for i in range(s, e)]\n",
    "# # all_sentence2 = [st for st in all_sentence if len(st.sentence) >10]\n",
    "# # all_sentence = all_sentence + all_sentence2\n",
    "# print(len(all_sentence))\n",
    "\n",
    "\n",
    "# selected_idx = np.random.randint(len(all_sentence), size=5000)\n",
    "\n",
    "# selected_st = []\n",
    "\n",
    "# for idx in selected_idx:\n",
    "#     selected_st.append(all_sentence[idx].sentence.replace('主诉:', ''))\n",
    "    \n",
    "# for st in selected_st[:10]:\n",
    "#     print(st)\n",
    "    \n",
    "# print(len(selected_st))\n",
    "\n",
    "# import pickle \n",
    "\n",
    "\n",
    "# with open('Chief_Complaint.p', 'wb') as handle:\n",
    "#     pickle.dump(selected_st, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_history = [f for f in folders if 'Past' in f.name]\n",
    "# pprint(past_history)\n",
    "\n",
    "# [get_sent_len(f) for f in past_history]\n",
    "\n",
    "\n",
    "# from nlptext.sentence import Sentence\n",
    "# f = past_history[0]\n",
    "\n",
    "# s, e = f.IdxSentStartEnd\n",
    "# all_sentence = [Sentence(i)  for i in range(s, e)]\n",
    "# all_sentence = [st for st in all_sentence if len(st.sentence) >10]\n",
    "# print(len(all_sentence))\n",
    "\n",
    "\n",
    "# selected_idx = np.random.randint(len(all_sentence), size=5000)\n",
    "\n",
    "# selected_st = []\n",
    "\n",
    "# for idx in selected_idx:\n",
    "#     selected_st.append(all_sentence[idx].sentence)\n",
    "    \n",
    "# for st in selected_st[:10]:\n",
    "#     print(st)\n",
    "    \n",
    "# print(len(selected_st))\n",
    "\n",
    "# import pickle \n",
    "\n",
    "\n",
    "# with open('Past_History.p', 'wb') as handle:\n",
    "#     pickle.dump(selected_st, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examination = [f for f in folders if 'RANinfos_PhysicalExamination' in f.name]\n",
    "# pprint(examination)\n",
    "\n",
    "# [get_sent_len(f) for f in examination]\n",
    "\n",
    "# from nlptext.sentence import Sentence\n",
    "# f = examination[0]\n",
    "\n",
    "# s, e = f.IdxSentStartEnd\n",
    "# all_sentence = [Sentence(i)  for i in range(s, e)]\n",
    "# all_sentence = [st for st in all_sentence if len(st.sentence) >10]\n",
    "# print(len(all_sentence))\n",
    "\n",
    "\n",
    "# selected_idx = np.random.randint(len(all_sentence), size=5000)\n",
    "\n",
    "# selected_st = []\n",
    "\n",
    "# for idx in selected_idx:\n",
    "#     selected_st.append(all_sentence[idx].sentence)\n",
    "    \n",
    "# for st in selected_st[:10]:\n",
    "#     print(st)\n",
    "    \n",
    "# print(len(selected_st))\n",
    "\n",
    "# import pickle \n",
    "\n",
    "\n",
    "# with open('Examination.p', 'wb') as handle:\n",
    "#     pickle.dump(selected_st, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis = [f for f in folders if 'FirstCorinfos_Diagdiscern' in f.name or 'GetDisRecordinfos_Outdiag' in f.name ]\n",
    "# pprint(diagnosis)\n",
    "# [get_sent_len(f) for f in diagnosis]\n",
    "\n",
    "\n",
    "\n",
    "# from nlptext.sentence import Sentence\n",
    "# f = diagnosis[0]\n",
    "\n",
    "# s, e = f.IdxSentStartEnd\n",
    "# all_sentence = [Sentence(i)  for i in range(s, e)]\n",
    "# all_sentence = [st for st in all_sentence if len(st.sentence) >10]\n",
    "# print(len(all_sentence))\n",
    "\n",
    "\n",
    "# selected_idx = np.random.randint(len(all_sentence), size=5000)\n",
    "\n",
    "# selected_st = []\n",
    "\n",
    "# for idx in selected_idx:\n",
    "#     selected_st.append(all_sentence[idx].sentence)\n",
    "    \n",
    "# for st in selected_st[:10]:\n",
    "#     print(st)\n",
    "    \n",
    "# print(len(selected_st))\n",
    "\n",
    "# import pickle \n",
    "\n",
    "\n",
    "# with open('Diagnosis.p', 'wb') as handle:\n",
    "#     pickle.dump(selected_st, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment = [f for f in folders if 'TreatMent' in f.name  ]\n",
    "# pprint(treatment)\n",
    "# [get_sent_len(f) for f in treatment]\n",
    "\n",
    "\n",
    "# from nlptext.sentence import Sentence\n",
    "# f = treatment[0]\n",
    "\n",
    "# s, e = f.IdxSentStartEnd\n",
    "# all_sentence = [Sentence(i)  for i in range(s, e)]\n",
    "# all_sentence = [st for st in all_sentence if len(st.sentence) >10]\n",
    "# print(len(all_sentence))\n",
    "\n",
    "\n",
    "# selected_idx = np.random.randint(len(all_sentence), size=5000)\n",
    "\n",
    "# selected_st = []\n",
    "\n",
    "# for idx in selected_idx:\n",
    "#     selected_st.append(all_sentence[idx].sentence)\n",
    "    \n",
    "# for st in selected_st[:15]:\n",
    "#     print(st)\n",
    "    \n",
    "# print(len(selected_st))\n",
    "\n",
    "# import pickle \n",
    "\n",
    "\n",
    "# with open('Treatment.p', 'wb') as handle:\n",
    "#     pickle.dump(selected_st, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.657 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_OperName.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/DailyRecDatas_Text.p\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_Text.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "Total Num of All    Tokens 36771299\n",
      "The Total Number of Tokens: 36771299\n",
      "Counting the number unique Tokens...          \t 2019-04-30 14:28:26.811847\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-04-30 14:28:37.016267\n",
      "\t\tThe length of DTU is: 3546 \t 2019-04-30 14:28:37.016896\n",
      "Generating the ORIGTokenIndex...       \t 2019-04-30 14:28:37.016931\n",
      "\t\tThe idx of token is: 0 \t 2019-04-30 14:28:37.017151\n",
      "\t\tThe idx of token is: 5000000 \t 2019-04-30 14:28:39.176864\n",
      "\t\tThe idx of token is: 10000000 \t 2019-04-30 14:28:41.410522\n",
      "\t\tThe idx of token is: 15000000 \t 2019-04-30 14:28:43.641935\n",
      "\t\tThe idx of token is: 20000000 \t 2019-04-30 14:28:45.804148\n",
      "\t\tThe idx of token is: 25000000 \t 2019-04-30 14:28:47.967319\n",
      "\t\tThe idx of token is: 30000000 \t 2019-04-30 14:28:50.186588\n",
      "\t\tThe idx of token is: 35000000 \t 2019-04-30 14:28:52.348733\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 3546\n",
      "\t\tWrite to: data/LuohuCorpus/char/Token3546/token.tsv\n",
      "\t\tWrite to: data/LuohuCorpus/char/Token3546/pos-es.tsv\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Token3546/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/LuohuCorpus/char/Token3546/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 44\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Token3546/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 227876\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Token3546/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1213094\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Token3546/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36771299\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/LuohuCorpus/char/Token3546/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3546\n",
      "pos-es\tis Dumped into file: data/LuohuCorpus/char/Token3546/GrainUnique/pos-es.p\n",
      "pos-es\tthe length of it is   : 232\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "# ########### BOSON ###########\n",
    "# CORPUSPath = 'corpus/boson/'\n",
    "# corpusFileIden = '.txt'\n",
    "# textType   = 'line'\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'iter'\n",
    "# TOKENLevel = 'char'\n",
    "# anno = 'embed'\n",
    "# annoKW = {}\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "corpusFileIden = '.p'\n",
    "textType   = 'element'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = False # TODO\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "# ########### ResumeNER ###########\n",
    "# CORPUSPath = 'corpus/ResumeNER/'\n",
    "# corpusFileIden = '.bmes'\n",
    "# textType   = 'block'\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'iter'\n",
    "# TOKENLevel = 'char'\n",
    "# anno = 'embed' # TODO\n",
    "# annoKW = {}\n",
    "\n",
    "# MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/ResumeNER/word/Token7137/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/ResumeNER/word/Token7137/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 3\n",
      "TEXT\tread from pickle file : data/ResumeNER/word/Token7137/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4617\n",
      "SENT\tread from pickle file : data/ResumeNER/word/Token7137/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4617\n",
      "TOKEN\tread from pickle file : data/ResumeNER/word/Token7137/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 74258\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/ResumeNER/word/Token7137/GrainUnique/token.p\n",
      "token\tthe length of it is   : 7137\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Path2Pyramid = 'data/ResumeNER/word/Token7137/Pyramid/'\n",
    "Path2LGUnique = 'data/ResumeNER/word/Token7137/GrainUnique/'\n",
    "\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "# BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CORPUSPath': 'corpus/ResumeNER/', 'corpusFileIden': '.bmes', 'CORPUSType': 'File', 'textType': 'block', 'EndIDXFolders': array([3]), 'TokenNum_Dir': 'data/ResumeNER/word/Token7137', 'length': 1}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'folderPaths': ['corpus/ResumeNER/test.char.bmes', 'corpus/ResumeNER/train.char.bmes', 'corpus/ResumeNER/dev.char.bmes'], 'EndIDXTexts': array([ 461, 4179, 4617]), 'length': 3}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndIDXSents': array([   1,    2,    3, ..., 4615, 4616, 4617]), 'Text2SentMethod': 're', 'length': 4617}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndIDXTokens': array([    4,    18,    59, ..., 74219, 74253, 74258]), 'Sent2TokenMethod': 'iter', 'length': 4617}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TOKENLevel': 'word', 'posTokenIndex': array([ 99, 215,  91, ..., 191,  91, 215]), 'ANNOTokenIndex': array([19,  3,  3, ..., 32, 33,  3]), 'length': 74258, 'ORIGTokenIndex': array([3075,    4,   21, ...,   36,   12,    7])}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/word/Token34139/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/word/Token34139/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/word/Token34139/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/word/Token34139/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/word/Token34139/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 313402\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/word/Token34139/GrainUnique/token.p\n",
      "token\tthe length of it is   : 34139\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Path2Pyramid  = 'data/boson/word/Token34139/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/word/Token34139/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "# BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Check Text and Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1344, 733, 1289, 1949, 527, 1520, 1393, 1147, 1180, 1149]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "print(txtIdxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1344 <txt New> \n",
      "\n",
      "0 --> 分税制实施后,一方面,从财政收入来看,实行的是“虹吸”政策,即将基层财力大量地逐级往上吸取,集中到上级财政特别是中央财政。\n",
      "1 --> 另一方面,从财政的再分配来看,实行的是“喷灌”政策。\n",
      "2 --> 这种分税制改革的实质是各级政府在财政主体利益的激励下,将各种好处尽量收到自己的盘子之中,乡镇及县的财权大部分被上收,而乡镇及县的事权则大大增加。\n",
      "3 --> 中央财政占了七成,却只需要做三成的事,地方财政区区三成,却要做七成的事。\n",
      "4 --> 地方政府没有钱,如何让他们想到发展?更别提还要全面城镇化了!\n",
      "\n",
      " 733 <txt New> \n",
      "\n",
      "5 --> 燕赵都市网讯(记者刘彬、通讯员刘永会)10月17日晚,京石高速徐水站救助了一位因急于寻找工作而陷入传销组织的青年大学毕业生,并为他免费提供食宿,联系上了苦苦寻找他17天的家人,送他踏上了回家的路。\n",
      "6 --> 据被救助人员王健介绍,他来自湖北,大学毕业后跟随哥哥在北京中关村经营电子产品,在“十一”放假期间,他背着家人应邀外出和网友见面,却不想是一场骗局,陷入了传销组织。\n",
      "7 --> 在传销期间他自己的身份证、手机、钱物都被扣押,每天陷入无止境的压抑中,无时无刻不想着逃脱。\n",
      "8 --> 终于在17天后机会来临,他设法摆脱传销组织后感觉到浑身的轻松和自由,但马上又陷入了茫然,自己身上身无分文又忘记了与家人的联系方式,于是他先找了一个没人地方躲藏了起来,等到天黑以后在惊恐未定中跑到收费站求救。\n",
      "\n",
      " 1289 <txt New> \n",
      "\n",
      "9 --> 此外,根据此前江淮发布的双品牌运营战略,“瑞风”、“和悦”将由车型品牌提升为产品子品牌,未来车型都统一采用“品牌+字母+数字”的命名方式,具体分工方面,和悦主打家用以及小型车市场,而瑞风将面向商务以及城市个性化。\n",
      "\n",
      " 1949 <txt New> \n",
      "\n",
      "10 --> 休闲风NO.200浏览链接1.长袖衬衫://t.cn/8sUOq542.针织开衫://t.cn/8sUHj5b3.铅笔裤://t.cn/8sUYuWd4.单鞋://t.cn/8sUWnrf5.单肩包://t.cn/8sUYuna\n",
      "\n",
      " 527 <txt New> \n",
      "\n",
      "11 --> 10月1日-7日,顾客(需关注百脑汇官方微信)到店即有机会抽取购物免单资格(每天2个),还有免费iPhone5S免费抽。\n",
      "12 --> 活动详情请至百脑汇官网查询。\n",
      "13 --> 活动四:关注微博、微信,好礼多重得! 即日起,关注百脑汇官方微博、微信,各种好礼缤纷送,9月28日-10月14日,巴西国家队10月15日鸟巢门票微信送出,10月1日-31日,电动车、阳光美食音乐嘉年华门票、加湿器等好礼赠出。\n",
      "14 --> 新浪微博@百脑汇北京旗舰店,微信公众号:buynow_bj活动五:微信粉丝国庆来就送 10月1日-7日,微信粉丝至百脑汇旗舰店即可免费领取加多宝饮料一瓶或手机支架等小礼品一个。\n",
      "\n",
      " 1520 <txt New> \n",
      "\n",
      "15 --> (麦媛/文)日本国民男星福山雅治6月26日(当地时间)抵达中国香港国际机场,他这次访港是为宣传主演的电影《真夏的方程式》。\n",
      "16 --> 当天有超过1000名粉丝和媒体在机场接驾,场面壮观。\n",
      "17 --> 该影片和日本同是在本月29日上映,福山将出席27日的特别试映会。\n",
      "18 --> 福山主演的《神探伽利略》系列在香港有着绝大人气,同系列的前作电影《容疑者X的献身》也曾在当地上映,获得异常的票房佳绩。\n",
      "19 --> 在《真夏的方程式》的香港发行方的强烈邀请下,福山实现了此次的访港之行。\n",
      "\n",
      " 1393 <txt New> \n",
      "\n",
      "20 --> 燕赵都市网衡水电(通讯员马丽则、刘恩、记者李海菊)一年一度的端午节即将来临,为让敬老院的老人们度过一个欢乐的节日,6月21日上午,河北省衡水市安平县五保供养中心迎来了一批特殊的客人——县妇联组织的20多名爱心志愿者,她们带着包粽子的原料和老人们一起来过端午节,让原本寂静的敬老院顿时充满了节日的气氛。\n",
      "21 --> 这些志愿者来自各行各业,她们都是通过观看近期央视四套播出的安平孝德文化专题片《孝德之乡》后才了解到这里生活着一群无儿无女的老人,她们带着精心为老人们准备了纯牛奶、水果等营养品和县妇联准备的糯米、粽叶和红枣来到了敬老院的厨房,和敬老院的老人们围在一起包粽子,一边包粽子还一边拉家常。\n",
      "22 --> 有的志愿者动作不太熟练,但是都学得很认真,不一会功夫,一盆糯米在这些“能工巧匠”的精心制作下变成了一个个棱角分明的粽子。\n",
      "\n",
      " 1147 <txt New> \n",
      "\n",
      "23 --> 瑞麒G3的1.3S/1.6/1.8三款动力配置在技术型和丰富程度上均要优于旗云E5和奇瑞A3。\n",
      "24 --> 尤其是1.3SCI机械增压发动机,最大功率达到了95kw/5200rpm,最大输出扭矩182Nm/3200rpm,即便放到同级合资车型中,也丝毫不落下风。\n",
      "25 --> 另外两款自然吸气发动机的差距实际上并不是很大,其中1.6DVVT发动机最大功率达到了93kw/6150rpm,最大输出扭矩160Nm/3900rpm;\n",
      "26 --> 而1.8L发动机的参数分别是最大功率97Kw/5750rpm,最大输出扭矩170Nm/4300-4500rpm。\n",
      "27 --> 不过1.3S机型的经济性要更加突出,对比已经曝光的1.3S手动车型和1.8CVT车型油耗信息就可以看出,前者的综合工况油耗为7.8L/100km,而后者的综合工况油耗为8.3L/100km。\n",
      "28 --> 底盘结构方面,新车依然采用了前麦弗逊和后多连杆悬挂组合,配备四轮盘式制动系统,整体上偏向舒适性,也比较符合其家用车定位。\n",
      "\n",
      " 1180 <txt New> \n",
      "\n",
      "29 --> 在大力推广企业和农户的产业化合作的同时,宜宾兴宜村镇银行的风险也得到了有效控制。\n",
      "30 --> 办理中绿林业的银行客户经理称,整个贷款过程有专门的账户监管,从种植到收购和加工的整个产业链过程中产生的现金流都在银行的监控之中,全面的信息覆盖大幅降低了银行的贷款风险。\n",
      "31 --> 国开行一位人士对记者表示,宜宾兴宜村镇银行通过这种农业产业链的方式,在支持了三农服务的同时还获得财务模式的可持续性,已经作为村镇银行的样本吸引了国开行和多家外资行前往调研。\n",
      "32 --> 但上述国开行人士称,没有找到合理的盈利模式和村镇银行的一些限制导致了开设的动力不足,目前绝大部分村镇银行还处于生存困境中。\n",
      "33 --> “很多村镇银行偏离了原来的三农服务方向,80%以上的村镇银行做的还是县域的中小企业贷款服务,但县域资质好的中小企业毕竟有限,有一半的村镇银行生存告急”。\n",
      "\n",
      " 1149 <txt New> \n",
      "\n",
      "34 --> “我们准备带离婴儿时,院子里突然跑来两名男子阻止我们。”田育兵说,经询问得知是朱某打电话叫来的两个哥哥。\n",
      "35 --> 虽然向对方亮明了身份,但是对方很不配合,双方还发生拉扯。\n",
      "36 --> 在经过民警一番法律教育后,朱某三兄弟极不情愿地看着民警将婴儿带离。\n",
      "37 --> 生了仨妞后想买个娃“续香火”“好好的家庭,怎么会想起来买婴儿呢?”起初,田育兵感到不理解,通过查询朱某的户口,发现32岁的朱某户口上有两个女儿,还有一个女儿没有上户口。\n",
      "38 --> “他连续生了3个女儿,心里很郁闷,也怕被人瞧不起,夫妻俩也多次吵架,心里一直想生个儿子。”经过一番思索后,他产生了买一个儿子“续香火”的想法。\n",
      "39 --> 一次,朱某在聊天中得知本村嫁到滑县的姑娘黄某有“门路”,便向黄某说出了自己的想法,黄某也表示帮忙“操心”。\n"
     ]
    }
   ],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "sentIdx = 0\n",
    "\n",
    "for txtIdx in txtIdxes:\n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n',txtIdx, txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tk 外站 >, <tk Phone >, <tk ・ >, <tk Arean >, <tk 表示 >, <tk , >, <tk 也许 >, <tk 前缀 >, <tk SM >, <tk 不 >, <tk 一定 >, <tk 表示 >, <tk 平板 >, <tk , >, <tk T >, <tk 作为 >, <tk 平板 >, <tk 代号 >, <tk 也 >, <tk 有 >, <tk 一定 >, <tk 可能性 >, <tk 。 >]\n"
     ]
    }
   ],
   "source": [
    "st = corpus.Sentences[34]\n",
    "print(st.Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tk 同时 >, <tk , >, <tk 我们 >, <tk 还 >, <tk 了解 >, <tk 到 >, <tk , >, <tk 福克斯 >, <tk 系列 >, <tk 还 >, <tk 将 >, <tk 引入 >, <tk 一款 >, <tk ST >, <tk 高性能 >, <tk 版本 >, <tk , >, <tk 该车 >, <tk 将 >, <tk 与 >, <tk 高尔夫 >, <tk GTI >, <tk 直接 >, <tk 竞争 >, <tk , >, <tk 将 >, <tk 搭载 >, <tk 蒙迪欧 >, <tk 上所 >, <tk 使用 >, <tk 的 >, <tk 2.0 >, <tk T >, <tk 发动机 >, <tk , >, <tk 匹配 >, <tk Powershift6 >, <tk 挡 >, <tk 双 >, <tk 离合 >, <tk 变速箱 >, <tk 。 >, <tk 但 >, <tk 该车 >, <tk 的 >, <tk 详细 >, <tk 参数 >, <tk 厂方 >, <tk 并未 >, <tk 透露 >, <tk , >, <tk 但 >, <tk 根据 >, <tk 海外 >, <tk 车型 >, <tk 的 >, <tk 参数 >, <tk , >, <tk 我们 >, <tk 猜测 >, <tk 有 >, <tk 可能 >, <tk 是 >, <tk 高功率 >, <tk 240 >, <tk 马力 >, <tk 版本 >, <tk , >, <tk 但 >, <tk 为了 >, <tk 适应 >, <tk 国内 >, <tk 的 >, <tk 燃油 >, <tk 标准 >, <tk , >, <tk 200 >, <tk 马力 >, <tk 版本 >, <tk 的 >, <tk 发动机 >, <tk 也 >, <tk 可能 >, <tk 成为 >, <tk 国产 >, <tk 福克斯 >, <tk ST >, <tk 所 >, <tk 采用 >, <tk 的 >, <tk 动力 >, <tk 配置 >, <tk 。 >, <tk 据 >, <tk 报道 >, <tk , >, <tk 这 >, <tk 款车 >, <tk 的 >, <tk 引入 >, <tk 虽 >, <tk 已 >, <tk 排上 >, <tk 日程 >, <tk , >, <tk 但 >, <tk 具体 >, <tk 上市 >, <tk 时间 >, <tk 还 >, <tk 未确定 >, <tk , >, <tk 大体 >, <tk 的 >, <tk 日程 >, <tk 在 >, <tk 今年 >, <tk 年末 >, <tk 或 >, <tk 明年 >, <tk 年初 >, <tk 。 >]\n"
     ]
    }
   ],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "txt = Text(13)\n",
    "print(txt.Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. getChannelGrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/word/Token34139/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/word/Token34139/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/word/Token34139/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/word/Token34139/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/word/Token34139/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 313402\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/word/Token34139/GrainUnique/token.p\n",
      "token\tthe length of it is   : 34139\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path2Pyramid  = 'data/boson/char/Token3870/Pyramid/'\n",
    "# Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Path2Pyramid  = 'data/boson/word/Token34139/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/word/Token34139/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Path2Pyramid = 'data/ResumeNER/char/Token1891/Pyramid/'\n",
    "# Path2LGUnique = 'data/ResumeNER/char/Token1891/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Path2Pyramid = 'data/ResumeNER/word/Token7137/Pyramid/'\n",
    "# Path2LGUnique = 'data/ResumeNER/word/Token7137/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['囗', '方']\n",
      "[['</start>'], ['心', '玄'], ['6', '2'], ['欠'], ['/'], ['刀'], ['，'], ['彳'], ['齐'], ['，'], ['口'], ['瓜', '月'], ['匚'], ['木', '门'], ['又'], ['疒', '王', '忄'], ['木', '音'], ['。'], ['</end>']]\n"
     ]
    }
   ],
   "source": [
    "PAD   = '</pad>'\n",
    "START = '</start>'\n",
    "END   = '</end>'\n",
    "UNK   = '</unk>'\n",
    "\n",
    "specialTokens     = [ PAD, UNK, START, END]\n",
    "specialTokensDict = {PAD: 0, START: 1, END: 2, UNK : 3, }\n",
    "\n",
    "from nlptext.utils.grain import getChannelGrain4Token, getChannelGrain4Sent\n",
    "\n",
    "channel = 'radical'\n",
    "token = '四方'\n",
    "\n",
    "print(getChannelGrain4Token(token, channel))\n",
    "\n",
    "channel = 'radical'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(getChannelGrain4Sent(sent, channel) )             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tk 将 >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = corpus.Tokens[37]\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['寸']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'radical'\n",
    "tk.getChannelGrain(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c69', 'c3', 'c234', 'c113', 'c0']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'subcomp'\n",
    "tk.getChannelGrain(channel, end_grain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s4', 's1', 's2', 's3', 's5', 's4', 's1', 's2', 's4', 's4-s1', 's1-s2', 's2-s3', 's3-s5', 's5-s4', 's4-s1', 's1-s2', 's2-s4']\n"
     ]
    }
   ],
   "source": [
    "channel = 'stroke'\n",
    "print(tk.getChannelGrain(channel, Max_Ngram=2, end_grain=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tk 村 >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "tk = Token(token='村')\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['木']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'radical'\n",
    "tk.getChannelGrain(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c185', 'c113', 'c0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'subcomp'\n",
    "tk.getChannelGrain(channel, end_grain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1', 's2', 's3', 's4', 's1', 's2', 's4', 's1-s2', 's2-s3', 's3-s4', 's4-s1', 's1-s2', 's2-s4']\n"
     ]
    }
   ],
   "source": [
    "channel = 'stroke'\n",
    "print(tk.getChannelGrain(channel, Max_Ngram=2, end_grain=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(来源:腾讯科技文:中涛)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<st 19 (tokenNum: 9) >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = corpus.Sentences[19]\n",
    "print(st.sentence)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['('],\n",
       " ['c67', 'c405', 'c277', 'c22', 'c331', 'c159'],\n",
       " [':'],\n",
       " ['c238',\n",
       "  'c70',\n",
       "  'c176',\n",
       "  'c1',\n",
       "  'c3',\n",
       "  'c1111',\n",
       "  'c1',\n",
       "  'c6',\n",
       "  'c1',\n",
       "  'c1111',\n",
       "  'c13',\n",
       "  'c20'],\n",
       " ['c326', 'c71', 'c20', 'c220', 'c20', 'c84'],\n",
       " ['c247'],\n",
       " [':'],\n",
       " ['c214', 'c277', 'c1', 'c98', 'c113'],\n",
       " [')']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'subcomp'\n",
    "\n",
    "st.getChannelGrain(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos-es'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelName\n",
    "channel = 'pos'\n",
    "channel_name = 'pos-es'\n",
    "end_grain = False\n",
    "tagScheme = 'BIOES'\n",
    "Max_Ngram = 1\n",
    "getChannelName(channel, Max_Ngram = Max_Ngram, end_grain = end_grain, tagScheme = tagScheme)\n",
    "\n",
    "# getChannelName(channel=channel, tagScheme = tagScheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['</start>'],\n",
       " ['x-S'],\n",
       " ['n-S'],\n",
       " ['x-S'],\n",
       " ['nz-S'],\n",
       " ['n-S'],\n",
       " ['n-S'],\n",
       " ['x-S'],\n",
       " ['n-S'],\n",
       " ['x-S'],\n",
       " ['</end>']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'pos'\n",
    "tagScheme = 'BIOES' # 'BIOE'\n",
    "useStartEnd = True\n",
    "st.getChannelGrain(channel, tagScheme = tagScheme, useStartEnd = useStartEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', ['x-S']),\n",
       " ('来源', ['n-S']),\n",
       " (':', ['x-S']),\n",
       " ('腾讯', ['nz-S']),\n",
       " ('科技', ['n-S']),\n",
       " ('文', ['n-S']),\n",
       " (':', ['x-S']),\n",
       " ('中涛', ['n-S']),\n",
       " (')', ['x-S'])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'pos'\n",
    "tagScheme = 'BIOES' # 'BIOE'\n",
    "useStartEnd = False\n",
    "\n",
    "pos = st.getChannelGrain(channel, tagScheme = tagScheme, useStartEnd = useStartEnd)\n",
    "\n",
    "list(zip([tk.token for tk in st.Tokens], pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', ['O']),\n",
       " ('来源', ['O']),\n",
       " (':', ['O']),\n",
       " ('腾讯', ['product_name-B']),\n",
       " ('科技', ['product_name-E']),\n",
       " ('文', ['O']),\n",
       " (':', ['O']),\n",
       " ('中涛', ['person_name-S']),\n",
       " (')', ['O'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 'annoE'\n",
    "tagScheme = 'BIOES' # 'BIOES'\n",
    "useStartEnd = False\n",
    "\n",
    "annoE = st.getChannelGrain(channel, tagScheme = tagScheme, useStartEnd = useStartEnd)\n",
    "list(zip([tk.token for tk in st.Tokens], annoE))\n",
    "# TODO, think more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build LGUnique and LookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/word/Token34139/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/word/Token34139/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/word/Token34139/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/word/Token34139/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/word/Token34139/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 313402\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/word/Token34139/GrainUnique/token.p\n",
      "token\tthe length of it is   : 34139\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Path2Pyramid  = 'data/boson/word/Token34139/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/word/Token34139/GrainUnique/'\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/boson/word/Token34139\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: basic\n",
      "Current Channel is        \t basic\n",
      "Current Channel Max_Ngram \t 2\n",
      "Deal with the Channel: medical\n",
      "Current Channel is        \t medical\n",
      "Current Channel Max_Ngram \t 2\n",
      "Deal with the Channel: radical\n",
      "Current Channel is        \t radical\n",
      "Current Channel Max_Ngram \t 2\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 3\n",
      "Deal with the Channel: pos\n",
      "Current Channel is        \t pos\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: annoR\n",
      "Current Channel is        \t annoR\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: annoE\n",
      "Current Channel is        \t annoE\n",
      "Current Channel Max_Ngram \t 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus()\n",
    "print(corpus.TokenNum_Dir)\n",
    "\n",
    "\n",
    "#if os.path.isdir(corpus.Channel_Dir):\n",
    "#    shutil.rmtree(corpus.Channel_Dir)\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'basic':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'medical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'radical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 4, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    # ANNO\n",
    "    'annoR':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    'annoE':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "# TODO: pretty print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T_C_b2_m2_r2_c4e_s3e_P_R_E'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.channels_folderName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token',\n",
       " 'char',\n",
       " 'basic2',\n",
       " 'medical2',\n",
       " 'radical2',\n",
       " 'subcomp4e',\n",
       " 'stroke3e',\n",
       " 'pos',\n",
       " 'annoR',\n",
       " 'annoE']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in BasicObject.GRAIN_UNI[BasicObject.TokenNum_Dir]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.GRAIN_UNI[corpus.TokenNum_Dir]['stroke3e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus.GRAIN_UNI[corpus.TokenNum_Dir]['medical2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['</pad>', '</start>', '</end>', 'O', 'company_name-B', 'company_name-I', 'location-B', 'location-I', 'org_name-B', 'org_name-I', 'person_name-B', 'person_name-I', 'product_name-B', 'product_name-I', 'time-B', 'time-I'], {'</pad>': 0, '</start>': 1, '</end>': 2, 'O': 3, 'company_name-B': 4, 'company_name-I': 5, 'location-B': 6, 'location-I': 7, 'org_name-B': 8, 'org_name-I': 9, 'person_name-B': 10, 'person_name-I': 11, 'product_name-B': 12, 'product_name-I': 13, 'time-B': 14, 'time-I': 15})\n"
     ]
    }
   ],
   "source": [
    "print(corpus.GRAIN_UNI[corpus.TokenNum_Dir]['annoE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tk 前日 >\n",
      "([14], 1)\n",
      "['time-B']\n"
     ]
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "tk = Token(2212)\n",
    "\n",
    "print(tk)\n",
    "print(tk.getGrainTensor('annoE'))\n",
    "print(tk.getChannelGrain('annoE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('株洲', ['product_name-B']),\n",
       " ('网', ['product_name-I']),\n",
       " ('讯', ['O']),\n",
       " ('(', ['O']),\n",
       " ('株洲', ['company_name-B']),\n",
       " ('晚报', ['company_name-I']),\n",
       " ('记者', ['O']),\n",
       " ('刘玺', ['person_name-B']),\n",
       " ('通讯员', ['O']),\n",
       " ('周捷', ['person_name-B']),\n",
       " (')', ['O']),\n",
       " ('前日', ['time-B']),\n",
       " (',', ['O']),\n",
       " ('在', ['O']),\n",
       " ('天元区', ['org_name-B']),\n",
       " ('劳动', ['org_name-I']),\n",
       " ('监察', ['org_name-I']),\n",
       " ('大队', ['org_name-I']),\n",
       " ('的', ['O']),\n",
       " ('监督', ['O']),\n",
       " ('下', ['O']),\n",
       " (',', ['O']),\n",
       " ('广洋', ['O']),\n",
       " ('工地', ['O']),\n",
       " ('项目部', ['O']),\n",
       " ('负责人', ['O']),\n",
       " ('支付', ['O']),\n",
       " ('了', ['O']),\n",
       " ('29', ['O']),\n",
       " ('万元', ['O']),\n",
       " ('现金', ['O']),\n",
       " (',', ['O']),\n",
       " ('结清', ['O']),\n",
       " ('了', ['O']),\n",
       " ('一年', ['time-B']),\n",
       " ('多', ['time-I']),\n",
       " ('前', ['time-I']),\n",
       " ('欠下', ['O']),\n",
       " ('的', ['O']),\n",
       " ('工资', ['O']),\n",
       " ('款', ['O']),\n",
       " ('。', ['O'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = tk.Sentence\n",
    "\n",
    "list(zip([tk.token for tk in st.Tokens], st.getChannelGrain('annoE')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('株洲', ['product_name-B']),\n",
       " ('网', ['product_name-E']),\n",
       " ('讯', ['O']),\n",
       " ('(', ['O']),\n",
       " ('株洲', ['company_name-B']),\n",
       " ('晚报', ['company_name-E']),\n",
       " ('记者', ['O']),\n",
       " ('刘玺', ['person_name-S']),\n",
       " ('通讯员', ['O']),\n",
       " ('周捷', ['person_name-S']),\n",
       " (')', ['O']),\n",
       " ('前日', ['time-S']),\n",
       " (',', ['O']),\n",
       " ('在', ['O']),\n",
       " ('天元区', ['org_name-B']),\n",
       " ('劳动', ['org_name-I']),\n",
       " ('监察', ['org_name-I']),\n",
       " ('大队', ['org_name-E']),\n",
       " ('的', ['O']),\n",
       " ('监督', ['O']),\n",
       " ('下', ['O']),\n",
       " (',', ['O']),\n",
       " ('广洋', ['O']),\n",
       " ('工地', ['O']),\n",
       " ('项目部', ['O']),\n",
       " ('负责人', ['O']),\n",
       " ('支付', ['O']),\n",
       " ('了', ['O']),\n",
       " ('29', ['O']),\n",
       " ('万元', ['O']),\n",
       " ('现金', ['O']),\n",
       " (',', ['O']),\n",
       " ('结清', ['O']),\n",
       " ('了', ['O']),\n",
       " ('一年', ['time-B']),\n",
       " ('多', ['time-I']),\n",
       " ('前', ['time-E']),\n",
       " ('欠下', ['O']),\n",
       " ('的', ['O']),\n",
       " ('工资', ['O']),\n",
       " ('款', ['O']),\n",
       " ('。', ['O'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = tk.Sentence\n",
    "\n",
    "list(zip([tk.token for tk in st.Tokens], st.getChannelGrain('annoE', tagScheme='BIOES')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use TokenNum_Dir to get Different Grain_Index for the Same Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/char/Token3870/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/char/Token3870/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/char/Token3870/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/char/Token3870/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/char/Token3870/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 534211\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/char/Token3870/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3870\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['国']\n",
      "([20], 1)\n",
      "([20], 1)\n",
      "data/boson/char/Token3870\n"
     ]
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tk = Token(293)\n",
    "# tk = Token(token = '民')\n",
    "print(tk)\n",
    "\n",
    "channel_dict = {'channel': 'pos',}\n",
    "\n",
    "\n",
    "\n",
    "channel_dict = {'channel': 'subcomp', \n",
    "               'Max_Ngram':   3,\n",
    "               'end_grain':   False}\n",
    "\n",
    "channel_dict = {'channel': 'token',}\n",
    "\n",
    "\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = True, **channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = False, **channel_dict))\n",
    "print(tk.TokenNum_Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 44\n",
      "TEXT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 227876\n",
      "SENT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1213151\n",
      "TOKEN\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36760202\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/LuohuCorpus/char/Token3546/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3546\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['国']\n",
      "([1326], 1)\n",
      "([1326], 1)\n",
      "\n",
      " Use Other TokenNum_Dir\n",
      "\n",
      "([20], 1)\n",
      "([20], 1)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "# ########### Wiki ###########\n",
    "# CORPUSPath = 'corpus/WikiTotal/'\n",
    "# corpusFileIden = '.txt'\n",
    "\n",
    "# textType   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'sep- '\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# anno = False\n",
    "# annoKW = {}\n",
    "# MaxTextIdx = 100\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "#                  Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "#                  anno, annoKW, MaxTextIdx)\n",
    "\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "BOB = 'data/LuohuCorpus/char/Token3546/Pyramid'\n",
    "LGU = 'data/LuohuCorpus/char/Token3546/GrainUnique'\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "tk = Token(99410)\n",
    "print(tk)\n",
    "other_TN_Dir = 'data/boson/char/Token3870'\n",
    "# other_TN_Dir = 'data/boson/word/Token34139'\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "\n",
    "print(tk.getGrainTensor(dontUseLookUp=True, **channel_dict))\n",
    "print(tk.getGrainTensor(**channel_dict))\n",
    "print('\\n', 'Use Other TokenNum_Dir\\n')\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, dontUseLookUp=True,  **channel_dict))\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, **channel_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/char/Token3870/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/char/Token3870/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/char/Token3870/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/char/Token3870/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/char/Token3870/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 534211\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/char/Token3870/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3870\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['c120', 'c173', 'c6', 'c120-c173', 'c173-c6', 'c120-c173-c6']\n",
      "([37, 40, 8, 38, 41, 39], 6)\n",
      "Get LookUp Table for Channel: subcomp3\n",
      "([37, 40, 8, 38, 41, 39], 6)\n",
      "data/boson/char/Token3870\n"
     ]
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tk = Token(293)\n",
    "# tk = Token(token = '民')\n",
    "print(tk)\n",
    "\n",
    "channel_dict = {'channel': 'pos',}\n",
    "\n",
    "\n",
    "\n",
    "channel_dict = {'channel': 'subcomp', \n",
    "               'Max_Ngram':   3,\n",
    "               'end_grain':   False}\n",
    "\n",
    "\n",
    "\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = True, **channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = False, **channel_dict))\n",
    "print(tk.TokenNum_Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 44\n",
      "TEXT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 227876\n",
      "SENT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1213151\n",
      "TOKEN\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36760202\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/LuohuCorpus/char/Token3546/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3546\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['c120', 'c173', 'c6', 'c120-c173', 'c173-c6', 'c120-c173-c6']\n",
      "([472, 281, 68, 3091, 3093, 3092], 6)\n",
      "Get LookUp Table for Channel: subcomp3\n",
      "([472, 281, 68, 3091, 3093, 3092], 6)\n",
      "\n",
      " Use Other TokenNum_Dir\n",
      "\n",
      "([37, 40, 8, 38, 41, 39], 6)\n",
      "([37, 40, 8, 38, 41, 39], 6)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "# ########### Wiki ###########\n",
    "# CORPUSPath = 'corpus/WikiTotal/'\n",
    "# corpusFileIden = '.txt'\n",
    "\n",
    "# textType   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'sep- '\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# anno = False\n",
    "# annoKW = {}\n",
    "# MaxTextIdx = 100\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "#                  Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "#                  anno, annoKW, MaxTextIdx)\n",
    "\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "BOB = 'data/LuohuCorpus/char/Token3546/Pyramid'\n",
    "LGU = 'data/LuohuCorpus/char/Token3546/GrainUnique'\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "tk = Token(99410)\n",
    "print(tk)\n",
    "other_TN_Dir = 'data/boson/char/Token3870'\n",
    "# other_TN_Dir = 'data/boson/word/Token34139'\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "\n",
    "print(tk.getGrainTensor(dontUseLookUp=True, **channel_dict))\n",
    "print(tk.getGrainTensor(**channel_dict))\n",
    "print('\\n', 'Use Other TokenNum_Dir\\n')\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, dontUseLookUp=True,  **channel_dict))\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, **channel_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/boson/char/Token3870/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/boson/char/Token3870/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/boson/char/Token3870/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tread from pickle file : data/boson/char/Token3870/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tread from pickle file : data/boson/char/Token3870/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 534211\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/boson/char/Token3870/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3870\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['n-B']\n",
      "([46], 1)\n",
      "([46], 1)\n",
      "data/boson/char/Token3870\n"
     ]
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tk = Token(293)\n",
    "# tk = Token(token = '民')\n",
    "print(tk)\n",
    "\n",
    "channel_dict = {'channel': 'pos',}\n",
    "\n",
    "\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = True, **channel_dict))\n",
    "print(tk.getGrainTensor(dontUseLookUp = False, **channel_dict))\n",
    "print(tk.TokenNum_Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 44\n",
      "TEXT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 227876\n",
      "SENT\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1213151\n",
      "TOKEN\tread from pickle file : data/LuohuCorpus/char/Token3546/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36760202\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/LuohuCorpus/char/Token3546/GrainUnique/token.p\n",
      "token\tthe length of it is   : 3546\n",
      "**************************************** \n",
      "\n",
      "<tk 国 >\n",
      "['n-B']\n",
      "([46], 1)\n",
      "([46], 1)\n",
      "\n",
      " Use Other TokenNum_Dir\n",
      "\n",
      "([46], 1)\n",
      "([46], 1)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "# ########### Wiki ###########\n",
    "# CORPUSPath = 'corpus/WikiTotal/'\n",
    "# corpusFileIden = '.txt'\n",
    "\n",
    "# textType   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'sep- '\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# anno = False\n",
    "# annoKW = {}\n",
    "# MaxTextIdx = 100\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "#                  Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "#                  anno, annoKW, MaxTextIdx)\n",
    "\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "BOB = 'data/LuohuCorpus/char/Token3546/Pyramid'\n",
    "LGU = 'data/LuohuCorpus/char/Token3546/GrainUnique'\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "tk = Token(99410)\n",
    "print(tk)\n",
    "other_TN_Dir = 'data/boson/char/Token3870'\n",
    "# other_TN_Dir = 'data/boson/word/Token34139'\n",
    "print(tk.getChannelGrain(**channel_dict))\n",
    "\n",
    "print(tk.getGrainTensor(dontUseLookUp=True, **channel_dict))\n",
    "print(tk.getGrainTensor(**channel_dict))\n",
    "print('\\n', 'Use Other TokenNum_Dir\\n')\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, dontUseLookUp=True,  **channel_dict))\n",
    "print(tk.getGrainTensor(TokenNum_Dir= other_TN_Dir, **channel_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Get GrainTensor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'basic':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'medical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'radical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 5, 'end_grain': True},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    # ANNO\n",
    "    'annoR':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    'annoE':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.CHANNEL_SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from nlptext.token import Token\n",
    "def test_speed(total_token_num):\n",
    "    tokenIdx = list(range(total_token_num))\n",
    "    total_start = datetime.now()\n",
    "    for ch, cs in CH.items():\n",
    "        print(ch)\n",
    "        s_ch = datetime.now()\n",
    "        for i in tokenIdx:\n",
    "            tk =Token(i)\n",
    "            tk.getGrainTensor(ch, **cs, dontUseLookUp=False)\n",
    "        e_ch = datetime.now()\n",
    "        print(e_ch - s_ch)\n",
    "\n",
    "    total_end = datetime.now()\n",
    "    time = total_end- total_start\n",
    "    print('Total time:', time)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def batch_channel_info(tk, ch, **cs):\n",
    "    return tk.getGrainTensor(ch, **cs)\n",
    "\n",
    "channel_extractor_func = partial(batch_channel_info, ch = ch, cs = cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_num = 10000\n",
    "tokenBatch = [Token(i) for i in range(total_token_num)]\n",
    "tk = tokenBatch[0]\n",
    "CH = BasicObject.CHANNEL_SETTINGS\n",
    "from nlptext.utils.grain import batch_info_generator\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "def batch_channel_info(tk, ch, **cs):\n",
    "    return tk.getGrainTensor(ch, **cs)\n",
    "\n",
    "ch = 'subcomp'\n",
    "cs =CH[ch]\n",
    "print(cs)\n",
    "channel_extractor_func = partial(batch_channel_info, ch = ch, **cs)\n",
    "\n",
    "# channel_extractor_func = batch_info_generator(ch, **cs)\n",
    "\n",
    "channel_extractor_func(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# def channel_extractor_func(x):\n",
    "#     return 2*x\n",
    "# tokenBatch = [1,2,3,4,5,6,7]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "result = pool.map(channel_extractor_func, tokenBatch)\n",
    "pool.close()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_info_generator(ch, **cs):\n",
    "    # tk = Token(i)\n",
    "    def batch_channel_info(tk):\n",
    "        return tk.getGrainTensor(ch, **cs)\n",
    "    return batch_channel_info\n",
    "\n",
    "\n",
    "ch = 'stroke'\n",
    "cs =CH[ch]\n",
    "tk = tokenBatch[5]\n",
    "print(tk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = {'token': {'Max_Ngram': 1},\n",
    " 'char': {'Max_Ngram': 1, 'end_grain': False},\n",
    " 'basic': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'medical': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'radical': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'subcomp': {'Max_Ngram': 3, 'end_grain': True},\n",
    " 'stroke': {'Max_Ngram': 5, 'end_grain': True},\n",
    "  'pos': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'},\n",
    " 'annoR': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'},\n",
    "  'annoE': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'}\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_num = 10000\n",
    "test_speed(total_token_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Get GrainTensor Test for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'basic':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'medical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'radical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 5, 'end_grain': True},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    # ANNO\n",
    "    'annoR':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    'annoE':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "}\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.array(range(102, 112)))\n",
    "print(BasicObject.TOKEN['ORIGTokenIndex'][0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "st = Sentence(100)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel_dict = {'channel': 'subcomp', \n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False}\n",
    "\n",
    "\n",
    "# channel_dict = {'channel': 'pos',\n",
    "#                'tagScheme': 'BIO'}\n",
    "\n",
    "\n",
    "st.getChannelGrain( ** channel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(dontUseLookUp = True, ** channel_dict)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(dontUseLookUp = False, ** channel_dict)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = st.sentence\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st2 = Sentence(sentence=string)\n",
    "info, leng_st, leng_tk, max_gr = st2.getGrainTensor(dontUseLookUp = True, ** channel_dict)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiTotal/'\n",
    "corpusFileIden = '.txt'\n",
    "\n",
    "textType   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "MaxTextIdx = 100\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "channel_dict = {'channel': 'subcomp', \n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False}\n",
    "\n",
    "print(string)\n",
    "st = Sentence(sentence=string)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "channel_dict = {'channel': 'subcomp', \n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False}\n",
    "\n",
    "\n",
    "# channel_dict = {'channel': 'pos',\n",
    "#                'tagScheme': 'BIO'}\n",
    "\n",
    "\n",
    "\n",
    "st.getChannelGrain( ** channel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(dontUseLookUp = True, ** channel_dict)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(dontUseLookUp = False, ** channel_dict)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(TokenNum_Dir= 'data/boson/char/Token3870', dontUseLookUp=True,  **channel_dict)\n",
    "info[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(TokenNum_Dir= 'data/boson/char/Token3870', **channel_dict)\n",
    "info[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'basic':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'medical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'radical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 5, 'end_grain': True},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    # ANNO\n",
    "    'annoR':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    'annoE':   {'use': True,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "}\n",
    "\n",
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid/'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from nlptext.sentence import Sentence\n",
    "def test_speed(total_token_num, CH):\n",
    "    tokenIdx = list(range(total_token_num))\n",
    "    total_start = datetime.now()\n",
    "    for ch, cs in CH.items():\n",
    "        print(ch)\n",
    "        s_ch = datetime.now()\n",
    "        for i in tokenIdx:\n",
    "            st =Sentence(i)\n",
    "            st.getGrainTensor(ch, **cs, dontUseLookUp=False)\n",
    "        e_ch = datetime.now()\n",
    "        print(e_ch - s_ch)\n",
    "\n",
    "    total_end = datetime.now()\n",
    "    time = total_end- total_start\n",
    "    print('Total time:', time)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = {'token': {'Max_Ngram': 1},\n",
    " 'char': {'Max_Ngram': 1, 'end_grain': False},\n",
    " 'basic': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'medical': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'radical': {'Max_Ngram': 2, 'end_grain': False},\n",
    " 'subcomp': {'Max_Ngram': 3, 'end_grain': True},\n",
    " 'stroke': {'Max_Ngram': 5, 'end_grain': True},\n",
    "  'pos': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'},\n",
    " 'annoR': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'},\n",
    "  'annoE': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIO'}\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_token_num = 5000\n",
    "\n",
    "test_speed(total_token_num, CH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luohu Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### LUOHUCORPUS ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "corpusFileIden = '.p'\n",
    "textType   = 'element'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "MaxTextIdx = False\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel,\n",
    "                 anno, annoKW, MaxTextIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "# ########### Wiki All ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "MaxTextIdx = False\n",
    "MaxTokenUnique = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType,\n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel,\n",
    "                 anno, annoKW, MaxTextIdx, MaxTokenUnique)\n",
    "\n",
    "\n",
    "\n",
    "# BOB = 'data/WikiTotal/word/Token447170/Pyramid'\n",
    "# LGU = 'data/WikiTotal/word/Token447170/GrainUnique'\n",
    "# BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "print('BasicObject is Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pay attention, pos is still here\n",
    "[i for i in BasicObject.TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(BasicObject.TOKEN['ORIGTokenIndex'] == 3)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.bincount(BasicObject.TOKEN['ORIGTokenIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "960282/257789077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data[data >= 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data[:447166+4])/257789077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, right = [], []\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_batch_idx(tgt_start_idx, token_per_batch, maxlen = 100, skip_window=5, \n",
    "                       left2right='tgt2ctx', size2size = '1-1', tgtInctx=False, **kwargs):\n",
    "    \n",
    "\n",
    "    # batch_size = token_per_batch * [(2*skip_window + I{tagInctx})if 1to1 else 1] \n",
    "    next_tgt_start_idx = tgt_start_idx + token_per_batch\n",
    "    maxIdx = maxlen - skip_window\n",
    "    if next_tgt_start_idx > maxIdx:\n",
    "        next_tgt_start_idx  = next_tgt_start_idx -  maxIdx + skip_window\n",
    "        tgt_idx_list = list(range(tgt_start_idx, maxlen - skip_window)) + list(range(skip_window, next_tgt_start_idx))\n",
    "    else:\n",
    "        tgt_idx_list = list(range(tgt_start_idx, next_tgt_start_idx))\n",
    "    # print(tgt_idx_list, len(tgt_idx_list))\n",
    "    # return next_tgt_start_idx\n",
    "    # do as tgt-ctx\n",
    "    left, right = [], []\n",
    "    if size2size == '1-1':\n",
    "        if tgtInctx == True:\n",
    "            for tgt_idx in tgt_idx_list:\n",
    "                # print(tgt_idx)\n",
    "                context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "                context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "                right_new = context_ahead + context_after + [tgt_idx]\n",
    "                # print(right)\n",
    "                right.extend(right_new)\n",
    "                left.extend([tgt_idx]*len(right_new))\n",
    "        else:\n",
    "            for tgt_idx in tgt_idx_list:\n",
    "                # print(tgt_idx)\n",
    "                context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "                context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "                right_new = context_ahead + context_after\n",
    "                right.extend(right_new)\n",
    "                left.extend([tgt_idx]*len(right_new))\n",
    "    if size2size in ['1-All', 'All-1']:\n",
    "        if tgtInctx == True:\n",
    "            for tgt_idx in tgt_idx_list:\n",
    "                # print(tgt_idx)\n",
    "                context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "                context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "                right_new = context_ahead + context_after + [tgt_idx]\n",
    "                right.append(right_new)\n",
    "                left.append(tgt_idx)\n",
    "        else:\n",
    "            for tgt_idx in tgt_idx_list:\n",
    "                # print(tgt_idx)\n",
    "                context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "                context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "                right_new = context_ahead + context_after \n",
    "                right.append(right_new)\n",
    "                left.append(tgt_idx)\n",
    "\n",
    "    if left2right == 'ctx-tgt':\n",
    "        right, left = left, right\n",
    "\n",
    "    left, right = np.array(left), np.array(right)\n",
    "    if len(left.shape) == 1:\n",
    "        left = left.reshape(len(left), 1)\n",
    "    if len(right.shape) == 1:\n",
    "        right = right.reshape(len(right), 1) \n",
    "\n",
    "    return left, right, next_tgt_start_idx\n",
    "\n",
    "\n",
    "maxlen = 1000\n",
    "\n",
    "\n",
    "\n",
    "left2right = 'tgt-ctx'   \n",
    "size2size  = '1-All'\n",
    "tgtInctx   = True\n",
    "\n",
    "tgt_token_num_per_batch = 4\n",
    "\n",
    "skip_window = 2\n",
    "tgt_start_idx = 107\n",
    "\n",
    "left, right, tgt_start_idx = generate_batch_idx(tgt_start_idx, tgt_token_num_per_batch, maxlen = maxlen, skip_window=skip_window, left2right=left2right, size2size = size2size, tgtInctx=tgtInctx)\n",
    "# print(left)\n",
    "# print(right)\n",
    "batch = list(zip(left, right))\n",
    "\n",
    "print(left2right, size2size, 'useTgt', tgtInctx, '| Tgt Number in a Batch:', tgt_token_num_per_batch)\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# batch_size = 10000\n",
    "tgt_start_idx = skip_window # initial tgt_start_idx\n",
    "batch_token_num = 10\n",
    "\n",
    "\n",
    "left2right = 'ctx-tgt' # 'ctx-tgt'\n",
    "size2size  = 'All-1'     # 'All-1' (for ctx-tgt only) and '1-All' (for tgt-ctx only)\n",
    "tgtInctx   = True\n",
    "\n",
    "tgt_idx_list = list(range(tgt_start_idx, tgt_start_idx + batch_token_num))\n",
    "\n",
    "\n",
    "# do as tgt-ctx\n",
    "left, right = [], []\n",
    "if size2size == '1-1':\n",
    "    if tgtInctx == True:\n",
    "        for tgt_idx in tgt_idx_list:\n",
    "            # print(tgt_idx)\n",
    "            context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "            context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "            right_new = context_ahead + context_after + [tgt_idx]\n",
    "            # print(right)\n",
    "            right.extend(right_new)\n",
    "            left.extend([tgt_idx]*len(right_new))\n",
    "    else:\n",
    "        for tgt_idx in tgt_idx_list:\n",
    "            # print(tgt_idx)\n",
    "            context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "            context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "            right_new = context_ahead + context_after\n",
    "            right.extend(right_new)\n",
    "            left.extend([tgt_idx]*len(right_new))\n",
    "if size2size in ['1-All', 'All-1']:\n",
    "    if tgtInctx == True:\n",
    "        for tgt_idx in tgt_idx_list:\n",
    "            # print(tgt_idx)\n",
    "            context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "            context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "            right_new = context_ahead + context_after + [tgt_idx]\n",
    "            right.append(right_new)\n",
    "            left.append(tgt_idx)\n",
    "    else:\n",
    "        for tgt_idx in tgt_idx_list:\n",
    "            # print(tgt_idx)\n",
    "            context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "            context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "            right_new = context_ahead + context_after \n",
    "            right.append(right_new)\n",
    "            left.append(tgt_idx)\n",
    "            \n",
    "if left2right == 'ctx-tgt':\n",
    "    right, left = left, right\n",
    "    \n",
    "right, left = np.array(right), np.array(left )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "b, a = a, b\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tgt_idx = 5\n",
    "\n",
    "context_ahead = list(range(tgt_idx - skip_window, tgt_idx))\n",
    "print(context_ahead)\n",
    "context_after = list(range(tgt_idx+1, tgt_idx + skip_window+1))\n",
    "print(context_after)\n",
    "\n",
    "# mode = tgt2ctx\n",
    "# 1to1\n",
    "\n",
    "right = np.array(context_ahead + context_after)\n",
    "left = np.array([tgt_idx] *len(right))\n",
    "print(left)\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'channel':ch, **cs} for ch, cs in CH.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "a[:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
