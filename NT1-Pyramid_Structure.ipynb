{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant\n",
    "\n",
    "## 1. `base.py::BasicObject.INIT`\n",
    "\n",
    "## 2. `utils/pyramid.py`\n",
    "\n",
    "## 3. `utils/vocab.py`\n",
    "\n",
    "\n",
    "# Less Relevant \n",
    "\n",
    "## 4. `base.py::BasicObject.OBJECT_TO_PICKLE`\n",
    "\n",
    "## 5. `base.py::BasicObject.INIT_FROM_PICKLE`\n",
    "\n",
    "## 6. `folder.py`, `text.py`, `sentence.py`, `token.py` (partial)\n",
    "\n",
    "## 7. `utils/infrastructure.py::Part CODE & Input and Output `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "# ########### Wiki All ###########\n",
    "CORPUSPath = 'corpus/WikiTotal/'\n",
    "GroupIden = '.txt'\n",
    "TextType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, GroupIden, TextType,\n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel,\n",
    "                 anno, annoKW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "'File'\n",
    "corpus/WikiTotal/WikiTotal7k_v2.txt\n",
    "100000 -- 3 190 2019-06-27 14:40:58.170793\n",
    "200000 -- 1 94 2019-06-27 14:41:21.156057\n",
    "300000 -- 1 139 2019-06-27 14:41:42.477667\n",
    "400000 -- 2 299 2019-06-27 14:42:03.825076\n",
    "500000 -- 2 184 2019-06-27 14:42:24.798024\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/WikiEng/wiki_en.txt\n",
      "100000 -- 1 4442 2019-06-27 16:14:21.655432\n",
      "200000 -- 1 18068 2019-06-27 16:20:37.503412\n",
      "300000 -- 1 453 2019-06-27 16:25:31.245183\n",
      "400000 -- 1 842 2019-06-27 16:29:37.455739\n",
      "500000 -- 1 2541 2019-06-27 16:33:20.352198\n",
      "600000 -- 1 5285 2019-06-27 16:36:45.087139\n",
      "700000 -- 1 6559 2019-06-27 16:39:49.359356\n",
      "800000 -- 1 4660 2019-06-27 16:42:47.864291\n",
      "900000 -- 1 4273 2019-06-27 16:45:37.061889\n",
      "1000000 -- 1 893 2019-06-27 16:48:20.266300\n",
      "1100000 -- 1 548 2019-06-27 16:51:00.526036\n",
      "1200000 -- 1 5721 2019-06-27 16:53:26.978164\n",
      "1300000 -- 1 913 2019-06-27 16:55:54.312335\n",
      "1400000 -- 1 870 2019-06-27 16:58:44.156900\n",
      "1500000 -- 1 971 2019-06-27 17:02:19.521715\n",
      "1600000 -- 1 499 2019-06-27 17:05:35.338467\n",
      "1700000 -- 1 564 2019-06-27 17:08:38.328902\n",
      "1800000 -- 1 751 2019-06-27 17:11:39.533079\n",
      "1900000 -- 1 1057 2019-06-27 17:14:38.481220\n",
      "2000000 -- 1 2679 2019-06-27 17:17:34.452599\n",
      "2100000 -- 1 514 2019-06-27 17:20:41.928869\n",
      "2200000 -- 1 666 2019-06-27 17:23:40.534399\n",
      "2300000 -- 1 4124 2019-06-27 17:26:24.996834\n",
      "2400000 -- 1 1747 2019-06-27 17:29:17.729493\n",
      "2500000 -- 1 513 2019-06-27 17:32:08.491034\n",
      "2600000 -- 1 1622 2019-06-27 17:35:38.259851\n",
      "2700000 -- 1 1492 2019-06-27 17:39:07.305473\n",
      "2800000 -- 1 391 2019-06-27 17:42:21.309896\n",
      "2900000 -- 1 1256 2019-06-27 17:45:50.562741\n",
      "3000000 -- 1 487 2019-06-27 17:49:28.115683\n",
      "3100000 -- 1 2152 2019-06-27 17:52:55.269243\n",
      "3200000 -- 1 523 2019-06-27 17:55:47.257303\n",
      "3300000 -- 1 920 2019-06-27 17:58:26.418914\n",
      "3400000 -- 1 606 2019-06-27 18:01:14.595926\n",
      "3500000 -- 1 3078 2019-06-27 18:04:00.740838\n",
      "3600000 -- 1 455 2019-06-27 18:06:46.670992\n",
      "3700000 -- 1 464 2019-06-27 18:09:22.282308\n",
      "3800000 -- 1 3934 2019-06-27 18:11:56.918033\n",
      "3900000 -- 1 8563 2019-06-27 18:14:30.676302\n",
      "4000000 -- 1 728 2019-06-27 18:16:59.480797\n",
      "4100000 -- 1 2123 2019-06-27 18:19:26.122881\n",
      "4200000 -- 1 2031 2019-06-27 18:21:52.635799\n",
      "4300000 -- 1 963 2019-06-27 18:24:25.512550\n",
      "4400000 -- 1 379 2019-06-27 18:26:53.582774\n",
      "4500000 -- 1 2787 2019-06-27 18:29:16.505739\n",
      "4600000 -- 1 842 2019-06-27 18:31:45.137842\n",
      "Total Num of All    Tokens 2618652299\n",
      "Total Num of Unique Tokens 9531927\n",
      "CORPUS\tit is Dumped into file: data/WikiEng/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/WikiEng/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/WikiEng/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4672758\n",
      "SENT\tit is Dumped into file: data/WikiEng/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4672758\n",
      "TOKEN\tit is Dumped into file: data/WikiEng/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2618652299\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/WikiEng/word/Vocab/token.voc\n",
      "token\tthe length of it is   : 9531927\n",
      "\t\tWrite to: data/WikiEng/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "# ########### Wiki All ###########\n",
    "CORPUSPath = 'corpus/WikiEng/'\n",
    "GroupIden = '.txt'\n",
    "TextType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, GroupIden, TextType,\n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel,\n",
    "                 anno, annoKW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'sep- ',\n",
       " 'EndIDXTokens': array([       12,        22,        53, ..., 257789044, 257789070,\n",
       "        257789077], dtype=uint32),\n",
       " 'EndIDXTokens_File': array([        51,         93,        266, ..., 1433929701, 1433929815,\n",
       "        1433929853], dtype=uint32),\n",
       " 'length': 11199643}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Brief Overview for Pyramid\n",
    "\n",
    "\n",
    "Show the pyramid structures: Corpus, Folder, Text, Sentence, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1.病史：患者为63岁女性，慢性病程，急性加重。', '既往有“高脂血症”病史。', '2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
       " ['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。', '双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
       " ['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
       "  '心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
       "  '腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
       "  '肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
       "  '双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "{'1': 0, '.': 1, '病': 2, '史': 3, '：': 4, '患': 5, '者': 6, '为': 7, '6': 8, '3': 9, '岁': 10, '女': 11, '性': 12, '，': 13, '慢': 14, '程': 15, '急': 16, '加': 17, '重': 18, '。': 19, '既': 20, '往': 21, '有': 22, '“': 23, '高': 24, '脂': 25, '血': 26, '症': 27, '”': 28, '2': 29, '因': 30, '反': 31, '复': 32, '脐': 33, '周': 34, '疼': 35, '痛': 36, '年': 37, '余': 38, '再': 39, '发': 40, '并': 41, '入': 42, '院': 43, '体': 44, '查': 45, '压': 46, '8': 47, '/': 48, 'm': 49, 'H': 50, 'g': 51, '神': 52, '志': 53, '清': 54, '楚': 55, '浅': 56, '表': 57, '淋': 58, '巴': 59, '结': 60, '无': 61, '肿': 62, '大': 63, '口': 64, '唇': 65, '苍': 66, '白': 67, '双': 68, '侧': 69, '扁': 70, '桃': 71, '、': 72, '充': 73, '咽': 74, '颈': 75, '静': 76, '脉': 77, '怒': 78, '张': 79, '肺': 80, '呼': 81, '吸': 82, '音': 83, '晰': 84, '未': 85, '闻': 86, '及': 87, '干': 88, '湿': 89, '啰': 90, '胸': 91, '膜': 92, '摩': 93, '擦': 94, '心': 95, '率': 96, '次': 97, '分': 98, '律': 99, '齐': 100, '各': 101, '瓣': 102, '区': 103, '理': 104, '杂': 105, '腹': 106, '部': 107, '平': 108, '坦': 109, '见': 110, '胃': 111, '肠': 112, '型': 113, '蠕': 114, '动': 115, '波': 116, '壁': 117, '柔': 118, '软': 119, '跳': 120, '扪': 121, '包': 122, '块': 123, '肝': 124, '脾': 125, '肋': 126, '下': 127, 'M': 128, 'u': 129, 'r': 130, 'p': 131, 'h': 132, 'y': 133, '征': 134, '（': 135, '-': 136, '）': 137, '肾': 138, '叩': 139, '击': 140, '移': 141, '浊': 142, '鸣': 143, '4': 144, '肢': 145, '浮': 146, '四': 147, '肌': 148, '力': 149, '正': 150, '常': 151, '生': 152, '射': 153, '存': 154, '在': 155, '引': 156, '出': 157}\n",
      "['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '程', '急', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '2', '因', '反', '复', '脐', '周', '疼', '痛', '年', '余', '再', '发', '并', '入', '院', '体', '查', '压', '8', '/', 'm', 'H', 'g', '神', '志', '清', '楚', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '口', '唇', '苍', '白', '双', '侧', '扁', '桃', '、', '充', '咽', '颈', '静', '脉', '怒', '张', '肺', '呼', '吸', '音', '晰', '未', '闻', '及', '干', '湿', '啰', '胸', '膜', '摩', '擦', '心', '率', '次', '分', '律', '齐', '各', '瓣', '区', '理', '杂', '腹', '部', '平', '坦', '见', '胃', '肠', '型', '蠕', '动', '波', '壁', '柔', '软', '跳', '扪', '包', '块', '肝', '脾', '肋', '下', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肾', '叩', '击', '移', '浊', '鸣', '4', '肢', '浮', '四', '肌', '力', '正', '常', '生', '射', '存', '在', '引', '出']\n",
      "[3, 3, 5, 2, 2, 1, 1, 1, 3, 3, 1, 1, 6, 21, 1, 1, 1, 2, 2, 12, 1, 1, 1, 2, 1, 1, 4, 1, 2, 4, 1, 4, 1, 2, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 9, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 6, 1, 6, 2, 5, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SENT = []\n",
    "\n",
    "processed_data_path = 'processed_data.txt'\n",
    "\n",
    "\n",
    "DTU = {}\n",
    "LTU = []\n",
    "index2freq = []\n",
    "# data = np.zeros(5000, dtype= np.uint32)\n",
    "token_num_in_corpus = 0\n",
    "\n",
    "if os.path.isfile(processed_data_path):\n",
    "    os.remove(processed_data_path)\n",
    "        \n",
    "for text in corpus:\n",
    "    for strSent in text:\n",
    "        strTokens = [i for i in strSent]\n",
    "        for token in strTokens:\n",
    "            if token not in DTU:\n",
    "                # deal with new words\n",
    "                token_idx  = len(DTU)\n",
    "                DTU[token] = token_idx\n",
    "                index2freq.append(1)\n",
    "                LTU.append(token)\n",
    "            else:\n",
    "                # deal with old words\n",
    "                token_idx = DTU[token]\n",
    "                index2freq[token_idx] += 1\n",
    "            # data.append(token_idx)\n",
    "            # data[token_num_in_corpus] = token_idx\n",
    "            token_num_in_corpus = token_num_in_corpus + 1\n",
    "            \n",
    "        with open(processed_data_path, 'a') as f:\n",
    "            line_sentence = ' '.join(strTokens) + '\\n'\n",
    "            f.write(line_sentence)\n",
    "            SENT.append(utf8len(line_sentence))\n",
    "            \n",
    "            \n",
    "            \n",
    "data = data[: token_num_in_corpus]\n",
    "print(len(data))\n",
    "# print(data)\n",
    "print(DTU)\n",
    "print(LTU)\n",
    "print(index2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  88,  136,  228,  356,  452,  548,  638,  840,  934, 1066])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentIdx = np.cumsum(SENT)\n",
    "sentIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "sent_idx = 2 # the three line\n",
    "sent_pos_start = sentIdx[sent_idx - 1]\n",
    "print(sent_pos_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_with_position(processed_data_path,start_position):\n",
    "    with open(processed_data_path, 'r') as f:\n",
    "        f.seek(start_position)\n",
    "        line = f.readline() # there is no 's' in f.readline()\n",
    "        # last_pos = f.tell()\n",
    "        # f.close()\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 12,  9,  6,  6,  6,  5,  5,  4,  4,  4,  4,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index2freq = np.sort(index2freq)[::-1]\n",
    "new_index2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,  19,  61,  83,  12,  85,  87,   2,  29,  31,  36,  26,   0,\n",
       "        62,   9,   1,  34,  68,  48, 104,   8, 115,  63, 112, 106, 103,\n",
       "        54,  98,  44,  46,  92,  49,  28,  72,  73,  86,  79,  97,  33,\n",
       "       135, 149, 136, 121, 145,  17,  18, 127, 148, 137,  23,   4, 153,\n",
       "         3, 124,  14,  55,   6,   7,  53,  59,   5,  52,  57,  51,  10,\n",
       "        58,  11,  60,  56,  27,  50,  47,  30,  25,  24,  32,  65,  35,\n",
       "        22,  37,  38,  21,  20,  39,  40,  41,  42,  43,  16,  45,  15,\n",
       "        64, 157,  66, 114, 117, 118, 119, 120, 122, 123, 125, 126, 128,\n",
       "       129, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144,\n",
       "       146, 147, 150, 151, 152, 154, 155, 116, 113,  67, 111,  69,  70,\n",
       "        71,  74,  75,  76,  77, 156,  80,  81,  82,  84,  88,  89,  90,\n",
       "        91,  93,  94,  95,  96,  99, 100, 101, 102, 105, 107, 108, 109,\n",
       "       110,  78])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newidx2oldidx = np.argsort(index2freq)[::-1]\n",
    "newidx2oldidx# [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  15,   7,  52,  50,  60,  56,  57,  20,  14,  64,  66,   4,\n",
       "         0,  54,  90,  88,  44,  45,   1,  82,  81,  78,  49,  74,  73,\n",
       "        11,  69,  32,   8,  72,   9,  75,  38,  16,  77,  10,  79,  80,\n",
       "        83,  84,  85,  86,  87,  28,  89,  29,  71,  18,  31,  70,  63,\n",
       "        61,  58,  26,  55,  68,  62,  65,  59,  67,   2,  13,  22,  91,\n",
       "        76,  93, 126,  17, 128, 129, 130,  33,  34, 131, 132, 133, 134,\n",
       "       157,  36, 136, 137, 138,   3, 139,   5,  35,   6, 140, 141, 142,\n",
       "       143,  30, 144, 145, 146, 147,  37,  27, 148, 149, 150, 151,  25,\n",
       "        19, 152,  24, 153, 154, 155, 156, 127,  23, 125,  94,  21, 124,\n",
       "        95,  96,  97,  98,  42,  99, 100,  53, 101, 102,  46, 103, 104,\n",
       "       105, 106, 107, 108, 109,  39,  41,  48, 110, 111, 112, 113, 114,\n",
       "       115, 116,  43, 117, 118,  47,  40, 119, 120, 121,  51, 122, 123,\n",
       "       135,  92])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldidx2newidx = np.zeros(len(newidx2oldidx), dtype= int)\n",
    "\n",
    "for new_idx, old_idx in enumerate(newidx2oldidx):\n",
    "    oldidx2newidx[old_idx] = new_idx\n",
    "    \n",
    "oldidx2newidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['，', '。', '无', '音', '性', '未', '及', '病', '2', '反', '痛', '血', '1', '肿', '3', '.', '周', '双', '/', '理', '6', '动', '大', '肠', '腹', '区', '清', '分', '体', '压', '膜', 'm', '”', '、', '充', '闻', '张', '次', '脐', '（', '力', '-', '扪', '肢', '加', '重', '下', '肌', '）', '“', '：', '射', '史', '肝', '慢', '楚', '者', '为', '志', '巴', '患', '神', '表', 'g', '岁', '淋', '女', '结', '浅', '症', 'H', '8', '因', '脂', '高', '复', '唇', '疼', '有', '年', '余', '往', '既', '再', '发', '并', '入', '院', '急', '查', '程', '口', '出', '苍', '蠕', '壁', '柔', '软', '跳', '包', '块', '脾', '肋', 'M', 'u', 'r', 'p', 'h', 'y', '征', '肾', '叩', '击', '移', '浊', '鸣', '4', '浮', '四', '正', '常', '生', '存', '在', '波', '型', '白', '胃', '侧', '扁', '桃', '咽', '颈', '静', '脉', '引', '肺', '呼', '吸', '晰', '干', '湿', '啰', '胸', '摩', '擦', '心', '率', '律', '齐', '各', '瓣', '杂', '部', '平', '坦', '见', '怒']\n",
      "{'，': 0, '。': 1, '无': 2, '音': 3, '性': 4, '未': 5, '及': 6, '病': 7, '2': 8, '反': 9, '痛': 10, '血': 11, '1': 12, '肿': 13, '3': 14, '.': 15, '周': 16, '双': 17, '/': 18, '理': 19, '6': 20, '动': 21, '大': 22, '肠': 23, '腹': 24, '区': 25, '清': 26, '分': 27, '体': 28, '压': 29, '膜': 30, 'm': 31, '”': 32, '、': 33, '充': 34, '闻': 35, '张': 36, '次': 37, '脐': 38, '（': 39, '力': 40, '-': 41, '扪': 42, '肢': 43, '加': 44, '重': 45, '下': 46, '肌': 47, '）': 48, '“': 49, '：': 50, '射': 51, '史': 52, '肝': 53, '慢': 54, '楚': 55, '者': 56, '为': 57, '志': 58, '巴': 59, '患': 60, '神': 61, '表': 62, 'g': 63, '岁': 64, '淋': 65, '女': 66, '结': 67, '浅': 68, '症': 69, 'H': 70, '8': 71, '因': 72, '脂': 73, '高': 74, '复': 75, '唇': 76, '疼': 77, '有': 78, '年': 79, '余': 80, '往': 81, '既': 82, '再': 83, '发': 84, '并': 85, '入': 86, '院': 87, '急': 88, '查': 89, '程': 90, '口': 91, '出': 92, '苍': 93, '蠕': 94, '壁': 95, '柔': 96, '软': 97, '跳': 98, '包': 99, '块': 100, '脾': 101, '肋': 102, 'M': 103, 'u': 104, 'r': 105, 'p': 106, 'h': 107, 'y': 108, '征': 109, '肾': 110, '叩': 111, '击': 112, '移': 113, '浊': 114, '鸣': 115, '4': 116, '浮': 117, '四': 118, '正': 119, '常': 120, '生': 121, '存': 122, '在': 123, '波': 124, '型': 125, '白': 126, '胃': 127, '侧': 128, '扁': 129, '桃': 130, '咽': 131, '颈': 132, '静': 133, '脉': 134, '引': 135, '肺': 136, '呼': 137, '吸': 138, '晰': 139, '干': 140, '湿': 141, '啰': 142, '胸': 143, '摩': 144, '擦': 145, '心': 146, '率': 147, '律': 148, '齐': 149, '各': 150, '瓣': 151, '杂': 152, '部': 153, '平': 154, '坦': 155, '见': 156, '怒': 157}\n"
     ]
    }
   ],
   "source": [
    "new_LTU = []\n",
    "for new_idx in range(len(LTU)):\n",
    "    new_LTU.append(LTU[newidx2oldidx[new_idx]])\n",
    "    \n",
    "print(new_LTU)\n",
    "new_DTU = {}\n",
    "for new_idx, token in enumerate(new_LTU):\n",
    "    new_DTU[token] = new_idx\n",
    "    \n",
    "print(new_DTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  15,   7,  52,  50,  60,  56,  57,  20,  14,  64,  66,   4,\n",
       "         0,  54,   4,   7,  90,   0,  88,   4,  44,  45,   1,  82,  81,\n",
       "        78,  49,  74,  73,  11,  69,  32,   7,  52,   1,   8,  15,  72,\n",
       "        49,   9,  75,  38,  16,  77,  10,   8,  79,  80,   0,  83,  84,\n",
       "        85,  44,  45,  12,  16,  32,  86,  87,   1,  14,  15,  28,  89,\n",
       "        50,  11,  29,  12,   8,  71,  18,  20,  14,  31,  31,  70,  63,\n",
       "         0,  61,  58,  26,  55,   0,  68,  62,  65,  59,  67,   2,  13,\n",
       "        22,   0,  91,  76,   2,  93, 126,   1,  17, 128, 129, 130,  28,\n",
       "         2,  13,  22,  33,  34,  11,   1, 131,   2,  34,  11,   1, 132,\n",
       "       133, 134,   2, 157,  36,   1,  17, 136, 137, 138,   3,  26, 139,\n",
       "         0,   5,  35,   6, 140, 141,   4, 142,   3,   0,   2, 143,  30,\n",
       "       144, 145,   3,   1, 146, 147,  20,   8,  37,  18,  27,   0, 148,\n",
       "       149,   0, 150, 151,  30,  25,   5,  35,   6,   7,  19,   4, 152,\n",
       "         3,   1,  24, 153, 154, 155,   0,   5, 156, 127,  23, 125,   6,\n",
       "        94,  21, 124,   0,  24,  95,  96,  97,   0,  38,  16,  29,  10,\n",
       "         0,   2,   9,  98,  10,   0,   5,  42,   6,  99, 100,   0,  53,\n",
       "       101, 102,  46,   5,  42,   6,   0, 103, 104, 105, 106, 107, 108,\n",
       "       109,  39,  41,  48,  53, 110,  25,   2, 111, 112,  10,   0, 113,\n",
       "        21,   4, 114,   3,  39,  41,  48,   0,  23, 115,   3, 116,  37,\n",
       "        18,  27,   1,  17,  46,  43,   2, 117,  13,   1, 118,  43,  47,\n",
       "        40,  33,  47,  36,  40, 119, 120,   0, 121,  19,   9,  51, 122,\n",
       "       123,   0,   7,  19,   9,  51,   5, 135,  92,   1], dtype=uint32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, oldidx in enumerate(data):\n",
    "    data[idx] = oldidx2newidx[oldidx]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  Level Dictionary\n",
      "{'NUMSents': [3, 2, 5], 'EndIDXSents': [3, 63, 128]}\n",
      "\n",
      "Sent  Level Dictionary\n",
      "{'NUMTokens': [24, 12, 25, 38, 24, 24, 24, 54, 25, 33], 'EndIDXTokens': [24, 36, 61, 99, 123, 147, 171, 225, 250, 283]}\n",
      "\n",
      "Token Level Dictionary\n",
      "{'DATAToken': ['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '性', '病', '程', '，', '急', '性', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '病', '史', '。', '2', '.', '因', '“', '反', '复', '脐', '周', '疼', '痛', '2', '年', '余', '，', '再', '发', '并', '加', '重', '1', '周', '”', '入', '院', '。', '3', '.', '体', '查', '：', '血', '压', '1', '2', '8', '/', '6', '3', 'm', 'm', 'H', 'g', '，', '神', '志', '清', '楚', '，', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '，', '口', '唇', '无', '苍', '白', '。', '双', '侧', '扁', '桃', '体', '无', '肿', '大', '、', '充', '血', '。', '咽', '无', '充', '血', '。', '颈', '静', '脉', '无', '怒', '张', '。', '双', '肺', '呼', '吸', '音', '清', '晰', '，', '未', '闻', '及', '干', '湿', '性', '啰', '音', '，', '无', '胸', '膜', '摩', '擦', '音', '。', '心', '率', '6', '2', '次', '/', '分', '，', '律', '齐', '，', '各', '瓣', '膜', '区', '未', '闻', '及', '病', '理', '性', '杂', '音', '。', '腹', '部', '平', '坦', '，', '未', '见', '胃', '肠', '型', '及', '蠕', '动', '波', '，', '腹', '壁', '柔', '软', '，', '脐', '周', '压', '痛', '，', '无', '反', '跳', '痛', '，', '未', '扪', '及', '包', '块', '，', '肝', '脾', '肋', '下', '未', '扪', '及', '，', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肝', '肾', '区', '无', '叩', '击', '痛', '，', '移', '动', '性', '浊', '音', '（', '-', '）', '，', '肠', '鸣', '音', '4', '次', '/', '分', '。', '双', '下', '肢', '无', '浮', '肿', '。', '四', '肢', '肌', '力', '、', '肌', '张', '力', '正', '常', '，', '生', '理', '反', '射', '存', '在', '，', '病', '理', '反', '射', '未', '引', '出', '。']}\n"
     ]
    }
   ],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', '病', '史', '：', '患', '者', '为', '6', '3', '岁', '女', '性', '，', '慢', '性', '病', '程', '，', '急', '性', '加', '重', '。', '既', '往', '有', '“', '高', '脂', '血', '症', '”', '病', '史', '。', '2', '.', '因', '“', '反', '复', '脐', '周', '疼', '痛', '2', '年', '余', '，', '再', '发', '并', '加', '重', '1', '周', '”', '入', '院', '。', '3', '.', '体', '查', '：', '血', '压', '1', '2', '8', '/', '6', '3', 'm', 'm', 'H', 'g', '，', '神', '志', '清', '楚', '，', '浅', '表', '淋', '巴', '结', '无', '肿', '大', '，', '口', '唇', '无', '苍', '白', '。', '双', '侧', '扁', '桃', '体', '无', '肿', '大', '、', '充', '血', '。', '咽', '无', '充', '血', '。', '颈', '静', '脉', '无', '怒', '张', '。', '双', '肺', '呼', '吸', '音', '清', '晰', '，', '未', '闻', '及', '干', '湿', '性', '啰', '音', '，', '无', '胸', '膜', '摩', '擦', '音', '。', '心', '率', '6', '2', '次', '/', '分', '，', '律', '齐', '，', '各', '瓣', '膜', '区', '未', '闻', '及', '病', '理', '性', '杂', '音', '。', '腹', '部', '平', '坦', '，', '未', '见', '胃', '肠', '型', '及', '蠕', '动', '波', '，', '腹', '壁', '柔', '软', '，', '脐', '周', '压', '痛', '，', '无', '反', '跳', '痛', '，', '未', '扪', '及', '包', '块', '，', '肝', '脾', '肋', '下', '未', '扪', '及', '，', 'M', 'u', 'r', 'p', 'h', 'y', '征', '（', '-', '）', '肝', '肾', '区', '无', '叩', '击', '痛', '，', '移', '动', '性', '浊', '音', '（', '-', '）', '，', '肠', '鸣', '音', '4', '次', '/', '分', '。', '双', '下', '肢', '无', '浮', '肿', '。', '四', '肢', '肌', '力', '、', '肌', '张', '力', '正', '常', '，', '生', '理', '反', '射', '存', '在', '，', '病', '理', '反', '射', '未', '引', '出', '。']\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "for newidx in data:\n",
    "    token_list.append(new_LTU[newidx])\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n",
      "1.病史：患者为63岁女性，慢性病程，急性加重。\n"
     ]
    }
   ],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Corpus to Folders and Folder to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "    \n",
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Folder to Texts\n",
    "\n",
    "Each text is a tuple for strText, SSET, orig_file_name, anno_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "FolderTextsReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.Entity'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/medpos/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/boson/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = '.txt')[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'line'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "     \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strText is then be spilted into sentences.\n",
    "\n",
    "If SSET exists, we need to match SSET and the splited sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##################################################################################################TEXT-SENT\n",
    "def reCutText2Sent(text, useSep = False):\n",
    "    \n",
    "    ###################### Remove some weird chars #######################\n",
    "    text = re.sub('\\xa0', '', text)\n",
    "    \n",
    "    ############# The Issue of Spaces\n",
    "    ###################### Convert the Spaces between two English Letters to 'ⴷ' #################\n",
    "    # Take care of Spaces\n",
    "    text = re.sub(r'(?<=[A-Za-z])\\s+(?=[|A-Za-z])', 'ⴷ',  text)\n",
    "    \n",
    "    ###################### Convert the S+ spaces to '〰' #################\n",
    "    text = re.sub(' {2}', '〰', text ).strip()\n",
    "    if useSep == ' ':\n",
    "        # if using space to sep the words\n",
    "        text = text.replace('\\t','').replace('〰', ' ')\n",
    "    elif useSep == '\\t':\n",
    "        # if using tab to sep the words, removing all spaces\n",
    "        text = text.replace(' ','').replace('〰', '')\n",
    "    else:\n",
    "        # if there is no sep char for Chinese, remove single space, and then convert space+ to single space\n",
    "        text = text.replace('\\t','').replace(' ', '',).replace('〰', ' ')\n",
    "        \n",
    "    # convert the spaces between English letters to single spaces\n",
    "    text = text.replace('ⴷ', ' ')\n",
    "    \n",
    "    # Other Things\n",
    "    text = re.sub('([。！!;；])([^”])', r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])',    r\"\\1\\n\\2\",text)\n",
    "    \n",
    "    # The \\n within \" \" is not considered\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [sent.strip() for sent in text]\n",
    "    # text = [sent.replace(' ', '').replace('\\\\n', '') for sent in text]\n",
    "    return [sent for sent in text if len(sent)>=2]\n",
    "\n",
    "\n",
    "text = '浙江在线杭州4月25日讯(记者施宇翔 通讯员 方英)毒贩很“时髦”,用微信交易毒品。没料想警方也很“潮”,将计就计,一举将其擒获。记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。钱某不寻常的特征,引起了警方注意。禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。'\n",
    "print(text)\n",
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole', **kwargs):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath. 2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.        \n",
    "    '''\n",
    "    if os.path.isfile(text):\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    elif method == 're':\n",
    "        return reCutText2Sent(text, **kwargs)\n",
    "    else:\n",
    "        return method(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deal with Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "print(strAnnoText)\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "notZeroIndex = 1 \n",
    "\n",
    "sset = SSETText[0]\n",
    "\n",
    "strAnno = sset[0]\n",
    "s       = int(sset[1]) - notZeroIndex\n",
    "tag     = sset[3] \n",
    "CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "CIT[-1][2] = tag + '-E'\n",
    "CIT[0][2]  = tag + '-B'\n",
    "    \n",
    "if len(CIT) == 1:\n",
    "    CIT[2] = tag + '-S'   \n",
    "print(CIT)\n",
    "\n",
    "\n",
    "CITAnnoText = []\n",
    "for sset in SSETText:\n",
    "    strAnno = sset[0]\n",
    "    s       = int(sset[1]) - notZeroIndex\n",
    "    tag     = sset[3] \n",
    "    CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "    CIT[-1][2] = tag + '-E'\n",
    "    CIT[0][2]  = tag + '-B'\n",
    "\n",
    "    if len(CIT) == 1:\n",
    "        CIT[0][2] = tag + '-S' \n",
    "        \n",
    "    CITAnnoText.extend(CIT)\n",
    "    \n",
    "    \n",
    "CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "\n",
    "for citAnno in CITAnnoText:\n",
    "    c, idx, t = citAnno\n",
    "    assert CITText[idx][0] == c\n",
    "    CITText[idx] = citAnno\n",
    "    \n",
    "CITText[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITSents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。']\n",
    "\n",
    "CITText = [['结', 0, 'O'],\n",
    " ['肠', 1, 'O'],\n",
    " ['多', 2, '疾病-B'],\n",
    " ['发', 3, '疾病-I'],\n",
    " ['息', 4, '疾病-I'],\n",
    " ['肉', 5, '疾病-E'],\n",
    " ['。', 6, 'O'],\n",
    " ['\\n', 7, 'O'],\n",
    " ['患', 8, 'O'],\n",
    " ['中', 9, 'O'],\n",
    " ['老', 10, 'O'],\n",
    " ['年', 11, 'O'],\n",
    " ['男', 12, 'O'],\n",
    " ['性', 13, 'O'],\n",
    " [',', 14, 'O'],\n",
    " ['慢', 15, '修饰-B'],\n",
    " ['性', 16, '修饰-E'],\n",
    " ['病', 17, 'O'],\n",
    " ['程', 18, 'O'],\n",
    " ['。', 19, 'O'],\n",
    " [' ', 20, 'O'],]\n",
    "\n",
    "\n",
    "\n",
    "lenLastSent = 0\n",
    "collapse    = 0\n",
    " \n",
    "CITSents = []\n",
    "for strSent in strSents:\n",
    "    CITSent = []\n",
    "    for sentTokenIdx, c in enumerate(strSent):\n",
    "        # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "        cT, _, tT = CITText[txtTokenIdx]\n",
    "        while c != cT:\n",
    "            collapse = collapse + 1\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            \n",
    "        CITSent.append([c,sentTokenIdx, tT])\n",
    "    lenLastSent = lenLastSent + len(strSent)\n",
    "    CITSents.append(CITSent)\n",
    "CITSents    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.textLineReader` with `anno == 'embed'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "string = 'This is an annotated entity {{2018-12-22:date}}, try to extract it out!'\n",
    "\n",
    "ST = [(block, 'O') if idx%2==0 else (block.split(':')[0], block.split(':')[-1]) \n",
    "    for idx, block in enumerate(string.replace(\"}}\", '{{').split('{{'))]\n",
    "\n",
    "pprint(ST)\n",
    "# SSET, Str, S(char), E(char), Tag.\n",
    "\n",
    "txtCharIdx = 0\n",
    "SSET = []\n",
    "strText = ''\n",
    "for st in ST:\n",
    "    string, tag = st\n",
    "    sset = [string, txtCharIdx, txtCharIdx + len(string), tag]\n",
    "    SSET.append(sset)\n",
    "    txtCharIdx = sset[2]\n",
    "    strText = strText + string\n",
    "    \n",
    "pprint(SSET)\n",
    "pprint(strText) \n",
    "\n",
    "# Only Way to Check a SSET\n",
    "for sset in SSET:\n",
    "    assert sset[0] == strText[sset[1]: sset[2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build Token String to Index Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.infrastructure import UNK_ID, specialTokens, specialTokensDict, strQ2B, fileReader\n",
    "\n",
    "##################################################################################################TOKEN_LTU\n",
    "def buildTokens(tokenList, MaxTokenUnique = None):\n",
    "    \"\"\"\n",
    "        Process raw inputs into a dataset.\n",
    "        words: a list of the whole corpus\n",
    "    \"\"\"\n",
    "    #########################################################################COUNT\n",
    "    total_len_token = len(tokenList)\n",
    "    print('The Total Number of Tokens:', total_len_token)\n",
    "    print('Counting the number unique Tokens...          \\t', datetime.now())\n",
    "    if MaxTokenUnique:\n",
    "        count = collections.Counter(tokenList).most_common(MaxTokenUnique)\n",
    "    else:\n",
    "        count = collections.Counter(tokenList).most_common()\n",
    "    print('\\t\\tDone!')\n",
    "    #########################################################################COUNT\n",
    "\n",
    "    print('Generating Dictionary of Token Unique...\\t', datetime.now())\n",
    "    DTU = specialTokensDict.copy()\n",
    "    DTU_freq = {sp_tk: 0 for sp_tk in specialTokens}\n",
    "    for token, freq in count:\n",
    "        if token is not specialTokens:\n",
    "            DTU[token] = len(DTU)\n",
    "            DTU_freq[token] = freq\n",
    "        else:\n",
    "            DTU_freq[token] = DTU_freq[token] + 1\n",
    "\n",
    "\n",
    "    print('\\t\\tThe length of DTU is:', len(DTU), '\\t', datetime.now())\n",
    "    print('Generating the ORIGTokenIndex...       \\t', datetime.now())\n",
    "    data = np.zeros(len(tokenList), dtype = np.uint32)\n",
    "    # data = []\n",
    "    for idx, token in enumerate(tokenList):\n",
    "        voc_id = DTU.get(token, UNK_ID)\n",
    "        data[idx] = voc_id\n",
    "        if voc_id == UNK_ID:\n",
    "            DTU_freq[UNK] = DTU_freq[UNK] + 1\n",
    "\n",
    "        # data.append(DTU.get(token,UNK_ID))\n",
    "        if idx % 5000000 == 0:\n",
    "            print('\\t\\tThe idx of token is:', idx, '\\t', datetime.now())\n",
    "    print('\\t\\tDone!')\n",
    "    LTU = list(DTU.keys())\n",
    "\n",
    "    if MaxTokenUnique:\n",
    "        print('Only Keep First', MaxTokenUnique, 'Tokens.')\n",
    "        print('The coverage rate is:', np.bincount(data)[UNK_ID]/total_len_token)\n",
    "    # data = np.array(data)\n",
    "    return data, LTU, DTU, DTU_freq\n",
    "##################################################################################################TOKEN_LTU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. From Corpus to Folders and From Folder to Texts\n",
    "\n",
    "\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "Folders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "\n",
    "pprint(Folders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "\n",
    "for folderPath in Folders:\n",
    "    print(folderPath)\n",
    "    fileNames = Folders[folderPath]\n",
    "    \n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        print(textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Text to Sentences and Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "# STAGE 2\n",
    "from nlptext.utils.pyramid import reCutText2Sent\n",
    "from nlptext.utils.pyramid import segText2Sents, segSent2Tokens# (text, method = 'whole')\n",
    "from nlptext.utils.pyramid import getCITSents\n",
    "from nlptext.utils.pyramid import getCITText\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'corpus/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed' # TODO\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "################   Things to Save   ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['corpusFileIden'] = corpusFileIden # None if Dir else\n",
    "CORPUS['CORPUSType']     = 'File' if corpusFileIden else 'Dir'\n",
    "CORPUS['textType'] = textType\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['folderPaths'] = [] \n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "        \n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['Text2SentMethod'] = Text2SentMethod\n",
    "if textType == 'file':\n",
    "    TEXT['ORIGFileName'] = []\n",
    "if anno:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "    \n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "SENT['Sent2TokenMethod'] = Sent2TokenMethod\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "TOKEN['TOKENLevel'] = TOKENLevel\n",
    "if anno:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "ANNO = {}\n",
    "ANNO['anno'] = anno\n",
    "ANNO['annoKW'] = annoKW\n",
    "\n",
    "    \n",
    "    \n",
    "########################################################\n",
    "##################     CHAINES      ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "###--> CHAIN: from Corpus to Folders <--###\n",
    "\n",
    "CorpusFolders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "assert CORPUS['CORPUSType'] == CORPUSType\n",
    "pprint(CorpusFolders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "for folderIdx, folderPath in enumerate(CorpusFolders):\n",
    "    print(folderPath)\n",
    "    fileNames = CorpusFolders[folderPath]\n",
    "    \n",
    "    ###--> CHAIN: from Folder to Texts <--###\n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        \n",
    "        print('\\n', textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        \n",
    "        ###--> CHAIN: from strText to strSents <--###\n",
    "        strSents = segText2Sents(strText, method = Text2SentMethod) # fixed\n",
    "        \n",
    "        for strSent in strSents:\n",
    "            #- print(strSent)\n",
    "            ###--> CHAIN: from strSent to strTokens <--###\n",
    "            strTokens = segSent2Tokens(strSent, method = Sent2TokenMethod)\n",
    "            \n",
    "            ###--> CHAIN's End: Token itself <--###\n",
    "            #- print(strTokens)\n",
    "            TOKEN['ORIGToken'].extend(strTokens)\n",
    "            \n",
    "            lenSent = len(strTokens)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "            \n",
    "        \n",
    "        lenText = len(strSents)\n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "            \n",
    "        if origTextName:\n",
    "            TEXT['ORIGFileName'].append(origTextName)\n",
    "            \n",
    "            \n",
    "        ########################################\n",
    "        if anno:\n",
    "            # TOKEN['ANNOToken'] = []\n",
    "            # assert SSETText   != [] # May occur Errors\n",
    "            for sset in SSETText:\n",
    "                assert sset[0] == strText[sset[1]: sset[2]]\n",
    "            if SSETText == []:\n",
    "                print('\\nThe SSET of this Text is Empty!!!')\n",
    "                print(strText, '\\n') # to check what happen\n",
    "                    \n",
    "            ############### PART One: Get CITText ###########\n",
    "            #\n",
    "            # CITText  = foo1(strText, SSETText)\n",
    "            # \n",
    "            CITText = getCITText(strText, SSETText)\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            ############### PART TWO: Get CITSents ###########\n",
    "            #\n",
    "            # CITSents = foo2(strSents, CITText)\n",
    "            #\n",
    "            CITSents = getCITSents(strSents, CITText)\n",
    "           \n",
    "\n",
    "            ############### PART THREE: Get TOKEN['ANNOToken'] ###########\n",
    "            #\n",
    "            # TOKEN['ANNOToken'] = foo3(CITSents, strSents)\n",
    "            #\n",
    "            for sentIdx, CITSent in enumerate(CITSents):\n",
    "                \n",
    "                # Corporate into TOKEN['ANNOToken']\n",
    "                # pay attention here, CIT is char-based, but TOKEN may be word-based.\n",
    "                # strTokens = segSent2Tokens(strSent, method=Sent2TokenMethod)\n",
    "            \n",
    "                if TOKENLevel == 'char':\n",
    "                    TOKEN['ANNOToken'].extend([CITToken[2] for CITToken in CITSent])\n",
    "                    #- pprint(sentIdx)\n",
    "                    #- pprint(CITSent)\n",
    "                else:\n",
    "                    # TODO\n",
    "                    pass \n",
    "            # save the file\n",
    "            \n",
    "            if annoTextName:\n",
    "                TEXT['ANNOFileName'].append(origTextName)\n",
    "            \n",
    "        \n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "    \n",
    "    # Back to Folder\n",
    "    lenFolder = textIdx\n",
    "    FOLDER['folderPaths'].append(folderPath)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder) # to remove\n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "        \n",
    "# End here\n",
    "lenCorpus = folderIdx\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text\n",
    "loc_idx = 9  # how to interpret loc_idx: support there are n texts in the whole corpus, get the loc_idx th text\n",
    "txt = Text(loc_idx)\n",
    "txt.Tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
