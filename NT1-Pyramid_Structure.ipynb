{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Brief View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deal with the Folder Corpus `(deprecate)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate Text File and Its Paths `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Important One\n",
    "def geneTextFilePaths(corpusPath, orig_iden = '.txt', anno_iden = None):\n",
    "    FolderNames = [i for i in np.sort(os.listdir(corpusPath)) if i[0] != '.']\n",
    "    # print(FolderNames)\n",
    "    FolderDict = {}\n",
    "    \n",
    "    for foldername in FolderNames:\n",
    "        path = corpusPath + foldername\n",
    "        # TODO: check the path by os\n",
    "        OrigFileList = [i for i in os.listdir(path) if orig_iden in i ]\n",
    "        if anno_iden:\n",
    "            AnnoFileList = [i.replace(orig_iden, anno_iden)  for i in OrigFileList]\n",
    "            AnnoFileList = [i if os.path.isfile(path + '/' + i) else '' for i in AnnoFileList]\n",
    "        else:\n",
    "            AnnoFileList = [''] * len(OrigFileList)\n",
    "            \n",
    "        FolderDict[foldername] = OrigFileList, AnnoFileList\n",
    "    return FolderDict\n",
    "\n",
    "corpusPath = 'dataset/ner/'\n",
    "anno_iden = '.Entity'\n",
    "FolderDict = geneTextFilePaths(corpusPath,  orig_iden = '.txt', anno_iden = anno_iden)\n",
    "print(FolderDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusPath = 'dataset/medpos/'\n",
    "anno_iden = '.UMLSTag'\n",
    "CorpusFolderDict = geneTextFilePaths(corpusPath, orig_iden = '.txt', anno_iden = anno_iden)\n",
    "print(CorpusFolderDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Read Text from a File Path `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    rstring = ''\n",
    "    for uchar in ustring:\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 12288:\n",
    "            inside_code = 32\n",
    "        elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "            inside_code -= 65248\n",
    "        # 2: unichr; 3: chr\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "\n",
    "def fileReader(fullfilepath):\n",
    "    with open(fullfilepath, 'r', encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "    return strQ2B(text)\n",
    "    \n",
    "\n",
    "\n",
    "filename = 'patient5212.txt'\n",
    "fullfilepath = 'dataset/medpos/'+ 'batch2/'+ filename\n",
    "\n",
    "\n",
    "text = fileReader(fullfilepath)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Segment Text to Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reCutText2Sent(text):\n",
    "    text = re.sub( ' +', ' ', text ).strip()\n",
    "    text = re.sub('([。！？\\?])([^”])',r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])'    ,r\"\\1\\n\\2\",text)\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "text = '1、急性咽炎    2、I型糖尿病\\n1、患儿男,11岁3月,因“发现血糖高半年余,发热、间断头晕头痛6小时”入院。查体:双侧扁桃体Ⅰ度肿大,有充血,咽部中度充血,血常规提示白细胞增高,以中性粒细胞为主,可诊断。\\n2、患儿男,11岁3月,因“发现血糖高半年余,发热、间断头晕头痛6小时”入院。2014-08-22我院胰岛素释放试验:73.46-495.3-309.8-432.4-293.5pmol/L;糖尿病分型三项:胰岛素细胞抗体阳性;糖化血红蛋白 6.3%;2014-10-04指尖血糖11.6mmol/L。可诊断。'\n",
    "\n",
    "'''\n",
    "text = re.sub( ' +', ' ', text ).strip()\n",
    "text = re.sub('([。！？\\?])([^”])',r\"\\1\\n\\2\",text) \n",
    "text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "text = re.sub('(\\…{2})([^”])'    ,r\"\\1\\n\\2\",text)\n",
    "text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') for i, x in enumerate(text.split('\"'))] )\n",
    "text = re.sub( '\\n+', '\\n', text ).strip()\n",
    "text\n",
    "'''\n",
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Old Method (Stupid) `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # depecated seg sentence\n",
    "# def filterSeps(sentSep, quota):\n",
    "#     if len(quota) == 0:\n",
    "#         return sentSep\n",
    "\n",
    "#     elif len(quota) % 2:\n",
    "#         return sentSep\n",
    "        \n",
    "#     a = int(len(quota)/2)\n",
    "#     quotas = [[quota[2*i], quota[2*i+1] ] for i in range(a)]\n",
    "#     # quotas\n",
    "#     newSeps = []\n",
    "#     for sep in sentSep:\n",
    "#         flag = 0\n",
    "#         for a, b in quotas:\n",
    "#             if a < sep and sep < b:\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 0:\n",
    "#             newSeps.append(sep)\n",
    "#     return newSeps\n",
    "\n",
    "# def segmentText2Sent(text):\n",
    "#     #0# Clean the Whole Text\n",
    "#     textAheadSpace = 0\n",
    "#     while text[textAheadSpace]    in [' ', '\\n']:\n",
    "#         textAheadSpace = textAheadSpace + 1\n",
    "    \n",
    "#     textBehindSpace = len(text)\n",
    "#     while text[textBehindSpace-1] in [' ', '\\n']:\n",
    "#         # print(text[: textBehindSpace])\n",
    "#         textBehindSpace = textBehindSpace - 1\n",
    "        \n",
    "#     # print(textAheadSpace, textBehindSpace)\n",
    "#     origtext = text\n",
    "#     text = text[textAheadSpace:textBehindSpace]\n",
    "#     # print(text)\n",
    "\n",
    "#     #1# For Spliting Sentence Based on '\\n' and '。'\n",
    "    \n",
    "#     sents = text.splitlines(True)\n",
    "#     L = []\n",
    "#     for sent in sents:\n",
    "#         periodIndex = [-1] + [i for i in range(len(sent)) if sent[i] == '。']\n",
    "#         #print(periodIndex)\n",
    "#         quota = [i for i in range(len(sent)) if sent[i] in '“”']\n",
    "        \n",
    "#         #print(quota)\n",
    "#         periodIndex = filterSeps(periodIndex, quota)\n",
    "#         #print(periodIndex)\n",
    "#         #print('---')\n",
    "#         l = [sent[periodIndex[ind]+1:periodIndex[ind+1]+1] \n",
    "#              for ind in range(len(periodIndex)-1)]\n",
    "#         L.extend( l+ [sent[periodIndex[-1]+1:]])\n",
    "#     # pprint(L)\n",
    "    \n",
    "#     newL = []\n",
    "    \n",
    "#     for ind in range(len(L)):\n",
    "#         currentL = ''.join(list(set(L[ind])))\n",
    "#         if len(L[ind]) >= 4 and currentL not in [' ', '', '\\n', '\\n ', ' \\n'] :\n",
    "#             newL.append(L[ind])\n",
    "#         else:\n",
    "#             newL[-1] = newL[-1] + L[ind]\n",
    "#     #1# Spliting End\n",
    "#     # pprint(newL)\n",
    "    \n",
    "    \n",
    "#     #2# For Calculating Cum Len\n",
    "#     cumLens = [0] + list(np.cumsum([len(i) for i in newL]))\n",
    "#     #2# Calculating End\n",
    "    \n",
    "#     #3# For Spliting Head and Tail Space and Adding Start and End Index\n",
    "#     newLStartEnd = []\n",
    "#     for ind_s in range(len(newL)):\n",
    "#         sent = newL[ind_s]\n",
    "        \n",
    "        \n",
    "#         ## For SentAheadSpace\n",
    "#         sentAheadSpace = 0\n",
    "#         while sent[:sentAheadSpace + 1] == ' ' * (sentAheadSpace+1):\n",
    "#             sentAheadSpace = sentAheadSpace + 1\n",
    "#         ## SentAheadSpace End\n",
    "            \n",
    "#         ## For sentBehindSpace\n",
    "#         sentBehindSpace = len(sent)\n",
    "        \n",
    "#         try:\n",
    "#             while sent[sentBehindSpace-1] in [' ', '\\n']:\n",
    "#                 # print(sent[: sentBehindSpace])\n",
    "#                 sentBehindSpace = sentBehindSpace - 1\n",
    "#         ## sentBehindSpace End\n",
    "#         except:\n",
    "#             print(L)\n",
    "#             print(newL)\n",
    "#             exit(0)\n",
    "\n",
    "#         sent_start = cumLens[ind_s] + sentAheadSpace  + textAheadSpace\n",
    "#         sent_end   = cumLens[ind_s] + sentBehindSpace + textAheadSpace\n",
    "#         new_sent   = sent[sentAheadSpace:sentBehindSpace]\n",
    "#         # print(new_sent)\n",
    "#         assert new_sent == origtext[sent_start: sent_end]\n",
    "\n",
    "#         # newLStartEnd.append([new_sent , sent_start, sent_end])\n",
    "#         newLStartEnd.append(new_sent)\n",
    "    \n",
    "#     return newLStartEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Cut Big Text to Sent by Each Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineCutText2Sent(fullfilepath):\n",
    "    with open(fullfilepath, 'r', encoding = 'utf-8') as f:\n",
    "        for sent in f:\n",
    "            yield strQ2B(sent).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corporated Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole'):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath\n",
    "        2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,\n",
    "                    then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.\n",
    "                    \n",
    "    '''\n",
    "    # return method(text)\n",
    "    \n",
    "    if os.path.isfile(text):\n",
    "        # filepath\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "        \n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    \n",
    "    else:\n",
    "        return method(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = fullfilepath\n",
    "print(text)\n",
    "a = segText2Sents(text, method = 'line')\n",
    "[i for i in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Segment Sentence to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corporated Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Initializing a Folder Type Corpus `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IDXOrient = 1\n",
    "SET_ANNO  = True\n",
    "TAG_SCHEME = \"BIO\"\n",
    "CORPUSPath = 'dataset/ner/'\n",
    "ORIGIden = '.txt'\n",
    "ANNOIden = '.Entity'\n",
    "Text2SentMethod = 'whole' # reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "        \n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['ORIGIden']   = ORIGIden\n",
    "CORPUS['SET_ANNO']   = SET_ANNO\n",
    "CORPUS['ANNOIden']   = ANNOIden if SET_ANNO else None\n",
    "CORPUS['IDXOrient'] = IDXOrient if SET_ANNO else None\n",
    "CORPUS['TAG_SCHEME'] = TAG_SCHEME if SET_ANNO else None\n",
    "\n",
    "\n",
    "CORPUS['NUMFolders']    = []\n",
    "CORPUS['EndIDXFolders'] = []\n",
    "CORPUS['EndIDXFolders'] = []\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['FolderName'] = []\n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "\n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['ORIGFileName'] = []\n",
    "if SET_ANNO:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "\n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "\n",
    "if SET_ANNO:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "\n",
    "corpus = geneTextFilePaths(CORPUSPath, \n",
    "                           orig_iden = ORIGIden, \n",
    "                           anno_iden = ANNOIden)\n",
    "# corpus: a dictionary\n",
    "# print(corpus)\n",
    "\n",
    "lenCorpus = len(corpus)\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]\n",
    "\n",
    "\n",
    "for folderIdx, folder in enumerate(corpus):\n",
    "    foldername = folder\n",
    "    FOLDER['FolderName'].append(folder)\n",
    "    # folder: string - folder name\n",
    "    folder, AnnoFilePath = corpus[folder]\n",
    "    # folder: a list of orig files\n",
    "    \n",
    "    lenFolder = len(folder)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder)\n",
    "    \n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "            \n",
    "            \n",
    "    for textIdx, text in enumerate(folder):\n",
    "        \n",
    "        # text: text file name\n",
    "        TEXT['ORIGFileName'].append(text)\n",
    "        \n",
    "        text = CORPUSPath + foldername + '/' + text\n",
    "        # text: full file path of the this orig file text\n",
    "        \n",
    "        text = segText2Sents(text, method = Text2SentMethod) ### KEY\n",
    "        # text: a list of sentence-level string\n",
    "        lenText = len(text)\n",
    "        \n",
    "        \n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        \n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "        \n",
    "        \n",
    "        for sentIdx, sent in enumerate(text):\n",
    "            # sent: a sent-level string\n",
    "            sent = segSent2Tokens(sent, method=Sent2TokenMethod)\n",
    "            # sent: list of token-level strings\n",
    "            \n",
    "            lenSent = len(sent)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "\n",
    "            # Do not need to iterrate them.\n",
    "            TOKEN['ORIGToken'].extend(sent)\n",
    "            \n",
    "            \n",
    "        # TEXT LEVEL ANNOTATION\n",
    "        if SET_ANNO:\n",
    "            \n",
    "            S_sentIdx = TEXT['EndIDXSents' ][textIdx-1]     if textIdx!= 0 else 0\n",
    "            S_tokenIdx= SENT['EndIDXTokens'][S_sentIdx - 1] if S_sentIdx != 0 else 0\n",
    "            \n",
    "            E_sentIdx = TEXT['EndIDXSents' ][textIdx]\n",
    "            E_tokenIdx= SENT['EndIDXTokens'][E_sentIdx - 1] # Pay attention here\n",
    "            numTokenInText = E_tokenIdx - S_tokenIdx\n",
    "            \n",
    "            ORIGTokenInText = TOKEN['ORIGToken'][S_tokenIdx:E_tokenIdx]\n",
    "            \n",
    "            print('\\n--------------- textIdx is', textIdx, numTokenInText)\n",
    "            print(S_tokenIdx, E_tokenIdx)\n",
    "            ANNOTokenInText = ['O'] * numTokenInText\n",
    "            annofilepath = AnnoFilePath[textIdx]\n",
    "            TEXT['ANNOFileName'].append(annofilepath)\n",
    "            annofilepath = CORPUSPath + foldername + '/' + annofilepath\n",
    "            print(annofilepath)\n",
    "            if os.path.isfile(annofilepath):\n",
    "                annotext = fileReader(annofilepath)\n",
    "                SSET = [sset.split('\\t') for sset in annotext.split('\\n') if '\\t' in sset] \n",
    "                # print(SSET)\n",
    "                for sset in SSET:\n",
    "                    string, start, end, tag = sset[0], int(sset[1]), int(sset[2]), sset[3] # start id ind \n",
    "                    if IDXOrient == 1:\n",
    "                        start = start - 1\n",
    "                    # print(string) \n",
    "                    # print(''.join(ORIGTokenInText[start:end]))\n",
    "                    assert string == ''.join(ORIGTokenInText[start:end])\n",
    "                    \n",
    "                    taglist = [tag + '-I' ] * (end - start)\n",
    "                    if TAG_SCHEME == 'BIO':\n",
    "                        taglist[0] = tag + '-B'\n",
    "                        \n",
    "                    elif TAG_SCHEME == 'BIOE':\n",
    "                        taglist[-1] = tag + '-E'\n",
    "                        taglist[0] = tag + '-B'\n",
    "                        \n",
    "                    # print(taglist)\n",
    "                        \n",
    "                    ANNOTokenInText[start: end] = taglist\n",
    "                    \n",
    "                print(ANNOTokenInText)\n",
    "                    \n",
    "            TOKEN['ANNOToken'].extend(ANNOTokenInText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CORPUS)\n",
    "print(FOLDER)\n",
    "print(TEXT)\n",
    "print(SENT)\n",
    "print(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bisect import bisect\n",
    "tokenIdx = 3\n",
    "sentIdx = bisect(SENT_DICT['EndIDXTokens'] , tokenIdx)\n",
    "\n",
    "print('sentIdx is', sentIdx)\n",
    "\n",
    "#######################\n",
    "\n",
    "s = SENT['EndIDXTokens'][sentIdx-1] if sentIdx != 0 else 0\n",
    "e = SENT['EndIDXTokens'][sentIdx]\n",
    "#######################\n",
    "\n",
    "print('start and end are', s,e)\n",
    "token = TOKEN['ORIGToken'][tokenIdx]\n",
    "print(token)\n",
    "idx = tokenIdx - s\n",
    "print('idx in the sent', idx)\n",
    "print(TOKEN['ORIGToken'][s:e][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TOKEN['ANNOToken']))\n",
    "print(len(TOKEN['ORIGToken']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Build Token String to Index Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "PAD   = '</pad>'\n",
    "UNK   = '</unk>'\n",
    "START = '</start>'\n",
    "END   = '</end>'\n",
    "\n",
    "init_dict = {PAD: 0, START: 1, END: 2, UNK : 3, }\n",
    "\n",
    "def buildTokens(tokens, init_dict = init_dict):\n",
    "    \"\"\"\n",
    "        Process raw inputs into a dataset.\n",
    "        words: a list of the whole corpus\n",
    "    \"\"\"\n",
    "    count = collections.Counter(tokens).most_common()\n",
    "    specailTokens = list(init_dict.keys())\n",
    "    dictionary = init_dict\n",
    "    \n",
    "    for token, _ in count:\n",
    "        if token is not specailTokens:\n",
    "            dictionary[token] = len(dictionary) \n",
    "    data = []\n",
    "    for token in tokens:\n",
    "        index = dictionary.get(token, 1)\n",
    "        data.append(index)\n",
    "    return data, dictionary\n",
    "\n",
    "TOKEN['ORIGTokenIndex'], DictToken =  buildTokens(TOKEN['ORIGToken'])\n",
    "print(DictToken)\n",
    "ListToken = list(DictToken.keys())\n",
    "print(ListToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. New Pyramid Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCorpusFolders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getCorpusFolders(CORPUSPath):\n",
    "    corpusFile = [i for i in os.listdir(CORPUSPath) if '.' in i]\n",
    "    if len(corpusFile) == 1:\n",
    "        return {os.path.join(CORPUSPath, corpusFile[0]): ''}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                 'Dir'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITText`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "print(strAnnoText)\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "notZeroIndex = 1 \n",
    "\n",
    "sset = SSETText[0]\n",
    "\n",
    "strAnno = sset[0]\n",
    "s       = int(sset[1]) - notZeroIndex\n",
    "tag     = sset[3] \n",
    "CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "CIT[-1][2] = tag + '-E'\n",
    "CIT[0][2]  = tag + '-B'\n",
    "    \n",
    "if len(CIT) == 1:\n",
    "    CIT[2] = tag + '-S'   \n",
    "print(CIT)\n",
    "\n",
    "\n",
    "CITAnnoText = []\n",
    "for sset in SSETText:\n",
    "    strAnno = sset[0]\n",
    "    s       = int(sset[1]) - notZeroIndex\n",
    "    tag     = sset[3] \n",
    "    CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "    CIT[-1][2] = tag + '-E'\n",
    "    CIT[0][2]  = tag + '-B'\n",
    "\n",
    "    if len(CIT) == 1:\n",
    "        CIT[0][2] = tag + '-S' \n",
    "        \n",
    "    CITAnnoText.extend(CIT)\n",
    "    \n",
    "    \n",
    "CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "\n",
    "for citAnno in CITAnnoText:\n",
    "    c, idx, t = citAnno\n",
    "    assert CITText[idx][0] == c\n",
    "    CITText[idx] = citAnno\n",
    "    \n",
    "CITText[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITSents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。']\n",
    "\n",
    "CITText = [['结', 0, 'O'],\n",
    " ['肠', 1, 'O'],\n",
    " ['多', 2, '疾病-B'],\n",
    " ['发', 3, '疾病-I'],\n",
    " ['息', 4, '疾病-I'],\n",
    " ['肉', 5, '疾病-E'],\n",
    " ['。', 6, 'O'],\n",
    " ['\\n', 7, 'O'],\n",
    " ['患', 8, 'O'],\n",
    " ['中', 9, 'O'],\n",
    " ['老', 10, 'O'],\n",
    " ['年', 11, 'O'],\n",
    " ['男', 12, 'O'],\n",
    " ['性', 13, 'O'],\n",
    " [',', 14, 'O'],\n",
    " ['慢', 15, '修饰-B'],\n",
    " ['性', 16, '修饰-E'],\n",
    " ['病', 17, 'O'],\n",
    " ['程', 18, 'O'],\n",
    " ['。', 19, 'O'],\n",
    " [' ', 20, 'O'],]\n",
    "\n",
    "\n",
    "\n",
    "lenLastSent = 0\n",
    "collapse    = 0\n",
    " \n",
    "CITSents = []\n",
    "for strSent in strSents:\n",
    "    CITSent = []\n",
    "    for sentTokenIdx, c in enumerate(strSent):\n",
    "        # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "        cT, _, tT = CITText[txtTokenIdx]\n",
    "        while c != cT:\n",
    "            collapse = collapse + 1\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            \n",
    "        CITSent.append([c,sentTokenIdx, tT])\n",
    "    lenLastSent = lenLastSent + len(strSent)\n",
    "    CITSents.append(CITSent)\n",
    "CITSents    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.textLineReader` with `anno == 'embed'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "string = 'This is an annotated entity {{2018-12-22:date}}, try to extract it out!'\n",
    "\n",
    "ST = [(block, 'O') if idx%2==0 else (block.split(':')[0], block.split(':')[-1]) \n",
    "    for idx, block in enumerate(string.replace(\"}}\", '{{').split('{{'))]\n",
    "\n",
    "pprint(ST)\n",
    "# SSET, Str, S(char), E(char), Tag.\n",
    "\n",
    "txtCharIdx = 0\n",
    "SSET = []\n",
    "strText = ''\n",
    "for st in ST:\n",
    "    string, tag = st\n",
    "    sset = [string, txtCharIdx, txtCharIdx + len(string), tag]\n",
    "    SSET.append(sset)\n",
    "    txtCharIdx = sset[2]\n",
    "    strText = strText + string\n",
    "    \n",
    "pprint(SSET)\n",
    "pprint(strText) \n",
    "\n",
    "# Only Way to Check a SSET\n",
    "for sset in SSET:\n",
    "    assert sset[0] == strText[sset[1]: sset[2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 From Corpus to Folders and From Folder to Texts\n",
    "\n",
    "\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "########### NER ###########\n",
    "\n",
    "CORPUSPath = 'dataset/ner/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.Entity'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 1,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "########### MedPOS ###########\n",
    "\n",
    "CORPUSPath = 'dataset/medpos/'\n",
    "textType   = 'file'\n",
    "corpusFileIden = None\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "########### Weibo Test ###########\n",
    "CORPUSPath = 'dataset/weibotest/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'sep'\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'dataset/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "\n",
    "textType   = 'line'\n",
    "\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'sep'\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'dataset/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'dataset/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "Folders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "\n",
    "pprint(Folders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "\n",
    "for folderPath in Folders:\n",
    "    print(folderPath)\n",
    "    fileNames = Folders[folderPath]\n",
    "    \n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        print(textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 Text to Sentences and Sentence to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 Saved Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "# STAGE 2\n",
    "from nlptext.utils.pyramid import reCutText2Sent\n",
    "from nlptext.utils.pyramid import segText2Sents, segSent2Tokens# (text, method = 'whole')\n",
    "\n",
    "\n",
    "########################################################\n",
    "################## Dataset Description #################\n",
    "########################################################\n",
    "\n",
    "\n",
    "########### NER ###########\n",
    "\n",
    "CORPUSPath = 'dataset/ner/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.Entity'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 1,\n",
    "}\n",
    "\n",
    "\n",
    "########### MedPOS ###########\n",
    "\n",
    "CORPUSPath = 'dataset/medpos/'\n",
    "textType   = 'file'\n",
    "corpusFileIden = None\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "########### Weibo Test ###########\n",
    "CORPUSPath = 'dataset/weibotest/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep-\\t'\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'dataset/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "\n",
    "textType   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'dataset/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed' # TODO\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'dataset/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "################   Things to Save   ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['corpusFileIden'] = corpusFileIden # None if Dir else\n",
    "CORPUS['CORPUSType']     = 'File' if corpusFileIden else 'Dir'\n",
    "CORPUS['textType'] = textType\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['folderPaths'] = [] \n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "        \n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['Text2SentMethod'] = Text2SentMethod\n",
    "if textType == 'file':\n",
    "    TEXT['ORIGFileName'] = []\n",
    "if anno:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "    \n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "SENT['Sent2TokenMethod'] = Sent2TokenMethod\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "TOKEN['TOKENLevel'] = TOKENLevel\n",
    "if anno:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "ANNO = {}\n",
    "ANNO['anno'] = anno\n",
    "ANNO['annoKW'] = annoKW\n",
    "\n",
    "    \n",
    "    \n",
    "########################################################\n",
    "##################     CHAINES      ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "###--> CHAIN: from Corpus to Folders <--###\n",
    "\n",
    "CorpusFolders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "assert CORPUS['CORPUSType'] == CORPUSType\n",
    "pprint(CorpusFolders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "for folderIdx, folderPath in enumerate(CorpusFolders):\n",
    "    print(folderPath)\n",
    "    fileNames = CorpusFolders[folderPath]\n",
    "    \n",
    "    ###--> CHAIN: from Folder to Texts <--###\n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        \n",
    "        print('\\n', textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        \n",
    "        ###--> CHAIN: from strText to strSents <--###\n",
    "        strSents = segText2Sents(strText, method = Text2SentMethod) # fixed\n",
    "        \n",
    "        for strSent in strSents:\n",
    "            #- print(strSent)\n",
    "            ###--> CHAIN: from strSent to strTokens <--###\n",
    "            strTokens = segSent2Tokens(strSent, method = Sent2TokenMethod)\n",
    "            \n",
    "            ###--> CHAIN's End: Token itself <--###\n",
    "            #- print(strTokens)\n",
    "            TOKEN['ORIGToken'].extend(strTokens)\n",
    "            \n",
    "            lenSent = len(strTokens)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "            \n",
    "        \n",
    "        lenText = len(strSents)\n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "            \n",
    "        if origTextName:\n",
    "            TEXT['ORIGFileName'].append(origTextName)\n",
    "            \n",
    "            \n",
    "        ########################################\n",
    "        if anno:\n",
    "            # TOKEN['ANNOToken'] = []\n",
    "            # assert SSETText   != [] # May occur Errors\n",
    "            for sset in SSETText:\n",
    "                assert sset[0] == strText[sset[1]: sset[2]]\n",
    "            if SSETText == []:\n",
    "                print('\\nThe SSET of this Text is Empty!!!')\n",
    "                print(strText, '\\n') # to check what happen\n",
    "                    \n",
    "            ############### PART One: Get CITText ###########\n",
    "            #\n",
    "            # CITText  = foo1(strText, SSETText)\n",
    "            # \n",
    "            \n",
    "            from nlptext.utils.pyramid import getCITText\n",
    "            CITText = getCITText(strText, SSETText)\n",
    "            #- print(CITText)\n",
    "            '''\n",
    "            CITAnnoText = []\n",
    "            for sset in SSETText:\n",
    "                # BIOES\n",
    "                strAnno, s, e, tag = sset\n",
    "                CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "                CIT[-1][2] = tag + '-E'\n",
    "                CIT[ 0][2] = tag + '-B'\n",
    "                if len(CIT) == 1:\n",
    "                    CIT[0][2] = tag + '-S' \n",
    "                CITAnnoText.extend(CIT)\n",
    "\n",
    "            # print(strAnnoText)\n",
    "            CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "            for citAnno in CITAnnoText:\n",
    "                c, idx, t = citAnno\n",
    "                assert CITText[idx][0] == c\n",
    "                CITText[idx] = citAnno\n",
    "            # CITText \n",
    "            # Here we get a CITText\n",
    "            #- pprint(CITText)\n",
    "            '''\n",
    "            \n",
    "            \n",
    "                \n",
    "            ############### PART TWO: Get CITSents ###########\n",
    "            #\n",
    "            # CITSents = foo2(strSents, CITText)\n",
    "            #\n",
    "            \n",
    "            from nlptext.utils.pyramid import getCITSents\n",
    "            CITSents = getCITSents(strSents, CITText)\n",
    "            '''\n",
    "            lenLastSent = 0\n",
    "            collapse    = 0 # don't need to move \n",
    "            CITSents = []\n",
    "            for strSent in strSents:\n",
    "                CITSent = []\n",
    "                for sentTokenIdx, c in enumerate(strSent):\n",
    "                    # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "                    txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                    cT, _, tT = CITText[txtTokenIdx]\n",
    "                    while c != cT:\n",
    "                        collapse = collapse + 1\n",
    "                        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                        cT, _, tT = CITText[txtTokenIdx]\n",
    "                    CITSent.append([c,sentTokenIdx, tT])\n",
    "                lenLastSent = lenLastSent + len(strSent)\n",
    "                CITSents.append(CITSent)\n",
    "            # CITSents\n",
    "            # Here we get CITSents \n",
    "            '''\n",
    "            \n",
    "\n",
    "            ############### PART THREE: Get TOKEN['ANNOToken'] ###########\n",
    "            #\n",
    "            # TOKEN['ANNOToken'] = foo3(CITSents, strSents)\n",
    "            #\n",
    "            for sentIdx, CITSent in enumerate(CITSents):\n",
    "                \n",
    "                # Corporate into TOKEN['ANNOToken']\n",
    "                # pay attention here, CIT is char-based, but TOKEN may be word-based.\n",
    "                # strTokens = segSent2Tokens(strSent, method=Sent2TokenMethod)\n",
    "            \n",
    "                if TOKENLevel == 'char':\n",
    "                    TOKEN['ANNOToken'].extend([CITToken[2] for CITToken in CITSent])\n",
    "                    #- pprint(sentIdx)\n",
    "                    #- pprint(CITSent)\n",
    "                else:\n",
    "                    # TODO\n",
    "                    pass \n",
    "            # save the file\n",
    "            \n",
    "            if annoTextName:\n",
    "                TEXT['ANNOFileName'].append(origTextName)\n",
    "            \n",
    "        \n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "    \n",
    "    # Back to Folder\n",
    "    lenFolder = textIdx\n",
    "    FOLDER['folderPaths'].append(folderPath)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder) # to remove\n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "        \n",
    "# End here\n",
    "lenCorpus = folderIdx\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'dataset/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "txt = Text(9)\n",
    "txt.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def readFile2GrainList(channel_name_path):\n",
    "#     ListGrainUnique = []\n",
    "#     with open(channel_name_path, 'r', encoding = 'utf-8') as f:\n",
    "#         for gr in f.readlines():\n",
    "#             gr = '\\n' if  '\\\\n' in gr[:-1] else gr[:-1]\n",
    "#             ListGrainUnique.append(gr)\n",
    "#     return ListGrainUnique\n",
    "\n",
    "\n",
    "# LTU = readFile2GrainList('token.tsv')\n",
    "# a = [i for i in LTU if i >= '\\u4e00' and i <= '\\u9fff'][:6000]\n",
    "# a_dict = dict(zip(a, range(len(a))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # def modify(line):\n",
    "# # #     L = []\n",
    "# # #     for char in line:\n",
    "        \n",
    "# #         if char >= '\\u4e00' and char <= '\\u9fff':\n",
    "# #             L.append(char)\n",
    "        \n",
    "# #         else:\n",
    "# #             inside_code = ord(char)\n",
    "# #             if inside_code == 12288:\n",
    "# #                 inside_code = 32\n",
    "# #             elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "# #                 inside_code -= 65248\n",
    "# #             char = chr(inside_code)\n",
    "            \n",
    "# #             if char in selected_non_cn_char:\n",
    "# #                 L.append(char)\n",
    "            \n",
    "# #     return ''.join(L)\n",
    "            \n",
    "\n",
    "# def modify(line):\n",
    "#     L = []\n",
    "#     strange = 0\n",
    "#     for char in line:\n",
    "#         if char >= '\\u4e00' and char <= '\\u9fff':\n",
    "#             if char not in a_dict:\n",
    "#                 # print(char)\n",
    "#                 char = '𐩧'\n",
    "#                 strange = strange + 1\n",
    "                \n",
    "#         L.append(char)\n",
    "#     return ''.join(L), strange\n",
    "                \n",
    "\n",
    "# from datetime import datetime\n",
    "# # BasicObject.BUILD_LIST_GRAIN_UNIQUE_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "# CORPUSPath = 'dataset/WikiTotal/'\n",
    "\n",
    "# total_strange = 0\n",
    "# with open('dataset/WikiTotal/WikiTotal2.txt', 'r') as f1:\n",
    "#     with open('dataset/WikiTotal/WikiTotal6k.txt', 'w') as f2:\n",
    "#         lastkey = ''\n",
    "#         i = 0\n",
    "#         count = 1\n",
    "#         for line in f1.readlines():\n",
    "#             # line = strQ2B(line)# .decode()\n",
    "\n",
    "#             line, strange = modify(line)\n",
    "#             total_strange = total_strange + strange\n",
    "#             # line = ''.join([i for i in line if i in pre_given_list])\n",
    "#             # key  = line.replace('\\n', '').split('\\t')[1]\n",
    "#             # line = line.replace('\\n', '\\t' + str(count) + '\\n') \n",
    "#             f2.write(line+'\\n')\n",
    "#             if i % 500000 == 0:\n",
    "#                 print(i, total_strange, datetime.now())\n",
    "#             i  = i + 1\n",
    "            \n",
    "# print('Total Strange:', total_strange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7k\n",
    "```\n",
    "0 0 2019-04-01 20:17:44.575479\n",
    "500000 6872 2019-04-01 20:18:00.339529\n",
    "1000000 13107 2019-04-01 20:18:13.829641\n",
    "1500000 18364 2019-04-01 20:18:26.433165\n",
    "2000000 24764 2019-04-01 20:18:38.503569\n",
    "2500000 31154 2019-04-01 20:18:50.281386\n",
    "3000000 34702 2019-04-01 20:18:58.808097\n",
    "3500000 39756 2019-04-01 20:19:09.941478\n",
    "4000000 43891 2019-04-01 20:19:21.889133\n",
    "4500000 49958 2019-04-01 20:19:33.441578\n",
    "Total Strange: 52514\n",
    "```\n",
    "\n",
    "* 6k\n",
    "```\n",
    "0 0 2019-04-01 20:22:29.925298\n",
    "500000 16943 2019-04-01 20:22:45.594240\n",
    "1000000 32007 2019-04-01 20:22:58.979631\n",
    "1500000 44940 2019-04-01 20:23:11.898943\n",
    "2000000 60453 2019-04-01 20:23:23.856685\n",
    "2500000 75492 2019-04-01 20:23:36.061031\n",
    "3000000 83949 2019-04-01 20:23:44.742128\n",
    "3500000 96150 2019-04-01 20:23:55.557540\n",
    "4000000 106365 2019-04-01 20:24:07.696283\n",
    "4500000 119614 2019-04-01 20:24:19.378625\n",
    "Total Strange: 125211\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path2Pyramid = 'data/boson/char/Token3870/Pyramid'\n",
    "Path2LGUnique = 'data/boson/char/Token3870/GrainUnique/'\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
