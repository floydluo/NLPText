{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Brief View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deal with the Folder Corpus `(deprecate)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate Text File and Its Paths `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# # Important One\n",
    "# def geneTextFilePaths(corpusPath, orig_iden = '.txt', anno_iden = None):\n",
    "#     FolderNames = [i for i in np.sort(os.listdir(corpusPath)) if i[0] != '.']\n",
    "#     # print(FolderNames)\n",
    "#     FolderDict = {}\n",
    "    \n",
    "#     for foldername in FolderNames:\n",
    "#         path = corpusPath + foldername\n",
    "#         # TODO: check the path by os\n",
    "#         OrigFileList = [i for i in os.listdir(path) if orig_iden in i ]\n",
    "#         if anno_iden:\n",
    "#             AnnoFileList = [i.replace(orig_iden, anno_iden)  for i in OrigFileList]\n",
    "#             AnnoFileList = [i if os.path.isfile(path + '/' + i) else '' for i in AnnoFileList]\n",
    "#         else:\n",
    "#             AnnoFileList = [''] * len(OrigFileList)\n",
    "            \n",
    "#         FolderDict[foldername] = OrigFileList, AnnoFileList\n",
    "#     return FolderDict\n",
    "\n",
    "# corpusPath = 'corpus/ner/'\n",
    "# anno_iden = '.Entity'\n",
    "# FolderDict = geneTextFilePaths(corpusPath,  orig_iden = '.txt', anno_iden = anno_iden)\n",
    "# print(FolderDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpusPath = 'corpus/medpos/'\n",
    "# anno_iden = '.UMLSTag'\n",
    "# CorpusFolderDict = geneTextFilePaths(corpusPath, orig_iden = '.txt', anno_iden = anno_iden)\n",
    "# print(CorpusFolderDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Read Text from a File Path `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strQ2B(ustring):\n",
    "#     rstring = ''\n",
    "#     for uchar in ustring:\n",
    "#         inside_code = ord(uchar)\n",
    "#         if inside_code == 12288:\n",
    "#             inside_code = 32\n",
    "#         elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "#             inside_code -= 65248\n",
    "#         # 2: unichr; 3: chr\n",
    "#         rstring += chr(inside_code)\n",
    "#     return rstring\n",
    "\n",
    "\n",
    "# def fileReader(fullfilepath):\n",
    "#     with open(fullfilepath, 'r', encoding = 'utf-8') as f:\n",
    "#         text = f.read()\n",
    "#     return strQ2B(text)\n",
    "    \n",
    "\n",
    "\n",
    "# filename = 'patient5212.txt'\n",
    "# fullfilepath = 'corpus/medpos/'+ 'batch2/'+ filename\n",
    "\n",
    "\n",
    "# text = fileReader(fullfilepath)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Segment Text to Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##################################################################################################TEXT-SENT\n",
    "def reCutText2Sent(text, useSep = False):\n",
    "    \n",
    "    \n",
    "    ###################### Remove some weird chars #######################\n",
    "    text = re.sub('\\xa0', '', text)\n",
    "    \n",
    "    ############# The Issue of Spaces\n",
    "    ###################### Convert the Spaces between two English Letters to 'ⴷ' #################\n",
    "    # Take care of Spaces\n",
    "    text = re.sub(r'(?<=[A-Za-z])\\s+(?=[|A-Za-z])', 'ⴷ',  text)\n",
    "    \n",
    "    ###################### Convert the S+ spaces to '〰' #################\n",
    "    text = re.sub(' {2}', '〰', text ).strip()\n",
    "    if useSep == ' ':\n",
    "        # if using space to sep the words\n",
    "        text = text.replace('\\t','').replace('〰', ' ')\n",
    "    elif useSep == '\\t':\n",
    "        # if using tab to sep the words, removing all spaces\n",
    "        text = text.replace(' ','').replace('〰', '')\n",
    "    else:\n",
    "        # if there is no sep char for Chinese, remove single space, and then convert space+ to single space\n",
    "        text = text.replace('\\t','').replace(' ', '',).replace('〰', ' ')\n",
    "        \n",
    "    # convert the spaces between English letters to single spaces\n",
    "    text = text.replace('ⴷ', ' ')\n",
    "    \n",
    "    # Other Things\n",
    "    text = re.sub('([。！!;；])([^”])', r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])',    r\"\\1\\n\\2\",text)\n",
    "    \n",
    "    # The \\n within \" \" is not considered\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [sent.strip() for sent in text]\n",
    "    # text = [sent.replace(' ', '').replace('\\\\n', '') for sent in text]\n",
    "    return [sent for sent in text if len(sent)>=2]\n",
    "\n",
    "\n",
    "text = 'HCI主要设计提高人们使用高科技产品的能力，并深入研究科技对个体和群体的影响，这个专业一般是计算机科学学院的研究生课程，但有些学校，像{{org_name:马里兰大学}}{{org_name:Maryland University}}已经开始开设本科课程。\\n\\n此专业关注社会与环境的相互影响，在不同学校里侧重稍有不同，比如，{{org_name:印第安纳大学}}开办的是环境科学（environmental science）和公众事务（public affairs）的双学位；而{{org_name:纽约州立大学环境科学与林业科学学院}}{{org_name:Environmental Science and Forestry}}提供环境科学领域内的各种专业课程及证书课程，包括理学学士（学士），硕士（MS）的，和硕士专业学位（M.P.S.）。\\n\\n'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Old Method (Stupid) `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # depecated seg sentence\n",
    "# def filterSeps(sentSep, quota):\n",
    "#     if len(quota) == 0:\n",
    "#         return sentSep\n",
    "\n",
    "#     elif len(quota) % 2:\n",
    "#         return sentSep\n",
    "        \n",
    "#     a = int(len(quota)/2)\n",
    "#     quotas = [[quota[2*i], quota[2*i+1] ] for i in range(a)]\n",
    "#     # quotas\n",
    "#     newSeps = []\n",
    "#     for sep in sentSep:\n",
    "#         flag = 0\n",
    "#         for a, b in quotas:\n",
    "#             if a < sep and sep < b:\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 0:\n",
    "#             newSeps.append(sep)\n",
    "#     return newSeps\n",
    "\n",
    "# def segmentText2Sent(text):\n",
    "#     #0# Clean the Whole Text\n",
    "#     textAheadSpace = 0\n",
    "#     while text[textAheadSpace]    in [' ', '\\n']:\n",
    "#         textAheadSpace = textAheadSpace + 1\n",
    "    \n",
    "#     textBehindSpace = len(text)\n",
    "#     while text[textBehindSpace-1] in [' ', '\\n']:\n",
    "#         # print(text[: textBehindSpace])\n",
    "#         textBehindSpace = textBehindSpace - 1\n",
    "        \n",
    "#     # print(textAheadSpace, textBehindSpace)\n",
    "#     origtext = text\n",
    "#     text = text[textAheadSpace:textBehindSpace]\n",
    "#     # print(text)\n",
    "\n",
    "#     #1# For Spliting Sentence Based on '\\n' and '。'\n",
    "    \n",
    "#     sents = text.splitlines(True)\n",
    "#     L = []\n",
    "#     for sent in sents:\n",
    "#         periodIndex = [-1] + [i for i in range(len(sent)) if sent[i] == '。']\n",
    "#         #print(periodIndex)\n",
    "#         quota = [i for i in range(len(sent)) if sent[i] in '“”']\n",
    "        \n",
    "#         #print(quota)\n",
    "#         periodIndex = filterSeps(periodIndex, quota)\n",
    "#         #print(periodIndex)\n",
    "#         #print('---')\n",
    "#         l = [sent[periodIndex[ind]+1:periodIndex[ind+1]+1] \n",
    "#              for ind in range(len(periodIndex)-1)]\n",
    "#         L.extend( l+ [sent[periodIndex[-1]+1:]])\n",
    "#     # pprint(L)\n",
    "    \n",
    "#     newL = []\n",
    "    \n",
    "#     for ind in range(len(L)):\n",
    "#         currentL = ''.join(list(set(L[ind])))\n",
    "#         if len(L[ind]) >= 4 and currentL not in [' ', '', '\\n', '\\n ', ' \\n'] :\n",
    "#             newL.append(L[ind])\n",
    "#         else:\n",
    "#             newL[-1] = newL[-1] + L[ind]\n",
    "#     #1# Spliting End\n",
    "#     # pprint(newL)\n",
    "    \n",
    "    \n",
    "#     #2# For Calculating Cum Len\n",
    "#     cumLens = [0] + list(np.cumsum([len(i) for i in newL]))\n",
    "#     #2# Calculating End\n",
    "    \n",
    "#     #3# For Spliting Head and Tail Space and Adding Start and End Index\n",
    "#     newLStartEnd = []\n",
    "#     for ind_s in range(len(newL)):\n",
    "#         sent = newL[ind_s]\n",
    "        \n",
    "        \n",
    "#         ## For SentAheadSpace\n",
    "#         sentAheadSpace = 0\n",
    "#         while sent[:sentAheadSpace + 1] == ' ' * (sentAheadSpace+1):\n",
    "#             sentAheadSpace = sentAheadSpace + 1\n",
    "#         ## SentAheadSpace End\n",
    "            \n",
    "#         ## For sentBehindSpace\n",
    "#         sentBehindSpace = len(sent)\n",
    "        \n",
    "#         try:\n",
    "#             while sent[sentBehindSpace-1] in [' ', '\\n']:\n",
    "#                 # print(sent[: sentBehindSpace])\n",
    "#                 sentBehindSpace = sentBehindSpace - 1\n",
    "#         ## sentBehindSpace End\n",
    "#         except:\n",
    "#             print(L)\n",
    "#             print(newL)\n",
    "#             exit(0)\n",
    "\n",
    "#         sent_start = cumLens[ind_s] + sentAheadSpace  + textAheadSpace\n",
    "#         sent_end   = cumLens[ind_s] + sentBehindSpace + textAheadSpace\n",
    "#         new_sent   = sent[sentAheadSpace:sentBehindSpace]\n",
    "#         # print(new_sent)\n",
    "#         assert new_sent == origtext[sent_start: sent_end]\n",
    "\n",
    "#         # newLStartEnd.append([new_sent , sent_start, sent_end])\n",
    "#         newLStartEnd.append(new_sent)\n",
    "    \n",
    "#     return newLStartEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corporated Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole', **kwargs):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath. 2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.        \n",
    "    '''\n",
    "    if os.path.isfile(text):\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    elif method == 're':\n",
    "        return reCutText2Sent(text, **kwargs)\n",
    "    else:\n",
    "        return method(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Segment Sentence to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corporated Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Initializing a Folder Type Corpus `(deprecate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IDXOrient = 1\n",
    "# SET_ANNO  = True\n",
    "# TAG_SCHEME = \"BIO\"\n",
    "# CORPUSPath = 'dataset/ner/'\n",
    "# ORIGIden = '.txt'\n",
    "# ANNOIden = '.Entity'\n",
    "# Text2SentMethod = 'whole' # reCutText2Sent\n",
    "# Sent2TokenMethod = 'iter'\n",
    "# TOKENLevel = 'char'\n",
    "\n",
    "\n",
    "# CORPUS = {}\n",
    "        \n",
    "# CORPUS['CORPUSPath'] = CORPUSPath\n",
    "# CORPUS['ORIGIden']   = ORIGIden\n",
    "# CORPUS['SET_ANNO']   = SET_ANNO\n",
    "# CORPUS['ANNOIden']   = ANNOIden if SET_ANNO else None\n",
    "# CORPUS['IDXOrient'] = IDXOrient if SET_ANNO else None\n",
    "# CORPUS['TAG_SCHEME'] = TAG_SCHEME if SET_ANNO else None\n",
    "\n",
    "\n",
    "# CORPUS['NUMFolders']    = []\n",
    "# CORPUS['EndIDXFolders'] = []\n",
    "# CORPUS['EndIDXFolders'] = []\n",
    "\n",
    "# FOLDER = {}\n",
    "# FOLDER['FolderName'] = []\n",
    "# FOLDER['NUMTexts'] = []\n",
    "# FOLDER['EndIDXTexts'] = []\n",
    "\n",
    "# TEXT = {}\n",
    "# TEXT['NUMSents'] = []\n",
    "# TEXT['EndIDXSents'] = []\n",
    "# TEXT['ORIGFileName'] = []\n",
    "# if SET_ANNO:\n",
    "#     TEXT['ANNOFileName'] = []\n",
    "\n",
    "# SENT = {}\n",
    "# SENT['NUMTokens'] = []\n",
    "# SENT['EndIDXTokens'] = []\n",
    "\n",
    "# TOKEN = {}\n",
    "# TOKEN['ORIGToken'] = []\n",
    "\n",
    "# if SET_ANNO:\n",
    "#     TOKEN['ANNOToken'] = []\n",
    "\n",
    "\n",
    "# corpus = geneTextFilePaths(CORPUSPath, \n",
    "#                            orig_iden = ORIGIden, \n",
    "#                            anno_iden = ANNOIden)\n",
    "# # corpus: a dictionary\n",
    "# # print(corpus)\n",
    "\n",
    "# lenCorpus = len(corpus)\n",
    "# CORPUS['NUMFolders'] = [lenCorpus]\n",
    "# CORPUS['EndIDXFolders'] = [lenCorpus]\n",
    "\n",
    "\n",
    "# for folderIdx, folder in enumerate(corpus):\n",
    "#     foldername = folder\n",
    "#     FOLDER['FolderName'].append(folder)\n",
    "#     # folder: string - folder name\n",
    "#     folder, AnnoFilePath = corpus[folder]\n",
    "#     # folder: a list of orig files\n",
    "    \n",
    "#     lenFolder = len(folder)\n",
    "    \n",
    "#     FOLDER['NUMTexts'].append(lenFolder)\n",
    "    \n",
    "#     try:\n",
    "#         FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "#     except:\n",
    "#         FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "            \n",
    "            \n",
    "#     for textIdx, text in enumerate(folder):\n",
    "        \n",
    "#         # text: text file name\n",
    "#         TEXT['ORIGFileName'].append(text)\n",
    "        \n",
    "#         text = CORPUSPath + foldername + '/' + text\n",
    "#         # text: full file path of the this orig file text\n",
    "        \n",
    "#         text = segText2Sents(text, method = Text2SentMethod) ### KEY\n",
    "#         # text: a list of sentence-level string\n",
    "#         lenText = len(text)\n",
    "        \n",
    "        \n",
    "#         TEXT['NUMSents'].append(lenText)\n",
    "        \n",
    "#         try:\n",
    "#             TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "#         except:\n",
    "#             TEXT['EndIDXSents'].append(lenText)\n",
    "        \n",
    "        \n",
    "#         for sentIdx, sent in enumerate(text):\n",
    "#             # sent: a sent-level string\n",
    "#             sent = segSent2Tokens(sent, method=Sent2TokenMethod)\n",
    "#             # sent: list of token-level strings\n",
    "            \n",
    "#             lenSent = len(sent)\n",
    "#             SENT['NUMTokens'].append(lenSent)\n",
    "#             try:\n",
    "#                 SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "#             except:\n",
    "#                 SENT['EndIDXTokens'].append(lenSent)\n",
    "\n",
    "#             # Do not need to iterrate them.\n",
    "#             TOKEN['ORIGToken'].extend(sent)\n",
    "            \n",
    "            \n",
    "#         # TEXT LEVEL ANNOTATION\n",
    "#         if SET_ANNO:\n",
    "            \n",
    "#             S_sentIdx = TEXT['EndIDXSents' ][textIdx-1]     if textIdx!= 0 else 0\n",
    "#             S_tokenIdx= SENT['EndIDXTokens'][S_sentIdx - 1] if S_sentIdx != 0 else 0\n",
    "            \n",
    "#             E_sentIdx = TEXT['EndIDXSents' ][textIdx]\n",
    "#             E_tokenIdx= SENT['EndIDXTokens'][E_sentIdx - 1] # Pay attention here\n",
    "#             numTokenInText = E_tokenIdx - S_tokenIdx\n",
    "            \n",
    "#             ORIGTokenInText = TOKEN['ORIGToken'][S_tokenIdx:E_tokenIdx]\n",
    "            \n",
    "#             print('\\n--------------- textIdx is', textIdx, numTokenInText)\n",
    "#             print(S_tokenIdx, E_tokenIdx)\n",
    "#             ANNOTokenInText = ['O'] * numTokenInText\n",
    "#             annofilepath = AnnoFilePath[textIdx]\n",
    "#             TEXT['ANNOFileName'].append(annofilepath)\n",
    "#             annofilepath = CORPUSPath + foldername + '/' + annofilepath\n",
    "#             print(annofilepath)\n",
    "#             if os.path.isfile(annofilepath):\n",
    "#                 annotext = fileReader(annofilepath)\n",
    "#                 SSET = [sset.split('\\t') for sset in annotext.split('\\n') if '\\t' in sset] \n",
    "#                 # print(SSET)\n",
    "#                 for sset in SSET:\n",
    "#                     string, start, end, tag = sset[0], int(sset[1]), int(sset[2]), sset[3] # start id ind \n",
    "#                     if IDXOrient == 1:\n",
    "#                         start = start - 1\n",
    "#                     # print(string) \n",
    "#                     # print(''.join(ORIGTokenInText[start:end]))\n",
    "#                     assert string == ''.join(ORIGTokenInText[start:end])\n",
    "                    \n",
    "#                     taglist = [tag + '-I' ] * (end - start)\n",
    "#                     if TAG_SCHEME == 'BIO':\n",
    "#                         taglist[0] = tag + '-B'\n",
    "                        \n",
    "#                     elif TAG_SCHEME == 'BIOE':\n",
    "#                         taglist[-1] = tag + '-E'\n",
    "#                         taglist[0] = tag + '-B'\n",
    "                        \n",
    "#                     # print(taglist)\n",
    "                        \n",
    "#                     ANNOTokenInText[start: end] = taglist\n",
    "                    \n",
    "#                 print(ANNOTokenInText)\n",
    "                    \n",
    "#             TOKEN['ANNOToken'].extend(ANNOTokenInText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(CORPUS)\n",
    "# print(FOLDER)\n",
    "# print(TEXT)\n",
    "# print(SENT)\n",
    "# print(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from bisect import bisect\n",
    "# tokenIdx = 3\n",
    "# sentIdx = bisect(SENT_DICT['EndIDXTokens'] , tokenIdx)\n",
    "\n",
    "# print('sentIdx is', sentIdx)\n",
    "\n",
    "# #######################\n",
    "\n",
    "# s = SENT['EndIDXTokens'][sentIdx-1] if sentIdx != 0 else 0\n",
    "# e = SENT['EndIDXTokens'][sentIdx]\n",
    "# #######################\n",
    "\n",
    "# print('start and end are', s,e)\n",
    "# token = TOKEN['ORIGToken'][tokenIdx]\n",
    "# print(token)\n",
    "# idx = tokenIdx - s\n",
    "# print('idx in the sent', idx)\n",
    "# print(TOKEN['ORIGToken'][s:e][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(TOKEN['ANNOToken']))\n",
    "# print(len(TOKEN['ORIGToken']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Build Token String to Index Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.infrastructure import UNK_ID, specialTokens, specialTokensDict, strQ2B, fileReader\n",
    "\n",
    "\n",
    "def buildTokens(tokenList, MaxTokenUnique = None):\n",
    "    \"\"\"\n",
    "        Process raw inputs into a dataset.\n",
    "        words: a list of the whole corpus\n",
    "    \"\"\"\n",
    "    #########################################################################COUNT\n",
    "    total_len_token = len(tokenList)\n",
    "    print('The Total Number of Tokens:', total_len_token)\n",
    "    print('Counting the number unique Tokens...          \\t', datetime.now())\n",
    "    if MaxTokenUnique:\n",
    "        count = collections.Counter(tokenList).most_common(MaxTokenUnique)\n",
    "    else:\n",
    "        count = collections.Counter(tokenList).most_common()\n",
    "    print('\\t\\tDone!')\n",
    "    #########################################################################COUNT\n",
    "\n",
    "    print('Generating Dictionary of Token Unique...\\t', datetime.now())\n",
    "    DTU = specialTokensDict.copy()\n",
    "    for token, _ in count:\n",
    "        if token is not specialTokens:\n",
    "            DTU[token] = len(DTU)\n",
    "\n",
    "\n",
    "    print('\\t\\tThe length of DTU is:', len(DTU), '\\t', datetime.now())\n",
    "    print('Generating the ORIGTokenIndex...       \\t', datetime.now())\n",
    "    data = np.zeros(len(tokenList), dtype=int)\n",
    "    # data = []\n",
    "    for idx, token in enumerate(tokenList):\n",
    "        data[idx] = DTU.get(token, UNK_ID)\n",
    "        # data.append(DTU.get(token,UNK_ID))\n",
    "        if idx % 5000000 == 0:\n",
    "            print('\\t\\tThe idx of token is:', idx, '\\t', datetime.now())\n",
    "    print('\\t\\tDone!')\n",
    "    LTU = list(DTU.keys())\n",
    "\n",
    "    if MaxTokenUnique:\n",
    "        print('Only Keep First', MaxTokenUnique, 'Tokens.')\n",
    "        print('The coverage rate is:', np.bincount(data)[UNK_ID]/total_len_token)\n",
    "    # data = np.array(data)\n",
    "    return data, LTU, DTU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. New Pyramid Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCorpusFolders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITText`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "print(strAnnoText)\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "notZeroIndex = 1 \n",
    "\n",
    "sset = SSETText[0]\n",
    "\n",
    "strAnno = sset[0]\n",
    "s       = int(sset[1]) - notZeroIndex\n",
    "tag     = sset[3] \n",
    "CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "CIT[-1][2] = tag + '-E'\n",
    "CIT[0][2]  = tag + '-B'\n",
    "    \n",
    "if len(CIT) == 1:\n",
    "    CIT[2] = tag + '-S'   \n",
    "print(CIT)\n",
    "\n",
    "\n",
    "CITAnnoText = []\n",
    "for sset in SSETText:\n",
    "    strAnno = sset[0]\n",
    "    s       = int(sset[1]) - notZeroIndex\n",
    "    tag     = sset[3] \n",
    "    CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "    CIT[-1][2] = tag + '-E'\n",
    "    CIT[0][2]  = tag + '-B'\n",
    "\n",
    "    if len(CIT) == 1:\n",
    "        CIT[0][2] = tag + '-S' \n",
    "        \n",
    "    CITAnnoText.extend(CIT)\n",
    "    \n",
    "    \n",
    "CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "\n",
    "for citAnno in CITAnnoText:\n",
    "    c, idx, t = citAnno\n",
    "    assert CITText[idx][0] == c\n",
    "    CITText[idx] = citAnno\n",
    "    \n",
    "CITText[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITSents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。']\n",
    "\n",
    "CITText = [['结', 0, 'O'],\n",
    " ['肠', 1, 'O'],\n",
    " ['多', 2, '疾病-B'],\n",
    " ['发', 3, '疾病-I'],\n",
    " ['息', 4, '疾病-I'],\n",
    " ['肉', 5, '疾病-E'],\n",
    " ['。', 6, 'O'],\n",
    " ['\\n', 7, 'O'],\n",
    " ['患', 8, 'O'],\n",
    " ['中', 9, 'O'],\n",
    " ['老', 10, 'O'],\n",
    " ['年', 11, 'O'],\n",
    " ['男', 12, 'O'],\n",
    " ['性', 13, 'O'],\n",
    " [',', 14, 'O'],\n",
    " ['慢', 15, '修饰-B'],\n",
    " ['性', 16, '修饰-E'],\n",
    " ['病', 17, 'O'],\n",
    " ['程', 18, 'O'],\n",
    " ['。', 19, 'O'],\n",
    " [' ', 20, 'O'],]\n",
    "\n",
    "\n",
    "\n",
    "lenLastSent = 0\n",
    "collapse    = 0\n",
    " \n",
    "CITSents = []\n",
    "for strSent in strSents:\n",
    "    CITSent = []\n",
    "    for sentTokenIdx, c in enumerate(strSent):\n",
    "        # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "        cT, _, tT = CITText[txtTokenIdx]\n",
    "        while c != cT:\n",
    "            collapse = collapse + 1\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            \n",
    "        CITSent.append([c,sentTokenIdx, tT])\n",
    "    lenLastSent = lenLastSent + len(strSent)\n",
    "    CITSents.append(CITSent)\n",
    "CITSents    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.textLineReader` with `anno == 'embed'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "string = 'This is an annotated entity {{2018-12-22:date}}, try to extract it out!'\n",
    "\n",
    "ST = [(block, 'O') if idx%2==0 else (block.split(':')[0], block.split(':')[-1]) \n",
    "    for idx, block in enumerate(string.replace(\"}}\", '{{').split('{{'))]\n",
    "\n",
    "pprint(ST)\n",
    "# SSET, Str, S(char), E(char), Tag.\n",
    "\n",
    "txtCharIdx = 0\n",
    "SSET = []\n",
    "strText = ''\n",
    "for st in ST:\n",
    "    string, tag = st\n",
    "    sset = [string, txtCharIdx, txtCharIdx + len(string), tag]\n",
    "    SSET.append(sset)\n",
    "    txtCharIdx = sset[2]\n",
    "    strText = strText + string\n",
    "    \n",
    "pprint(SSET)\n",
    "pprint(strText) \n",
    "\n",
    "# Only Way to Check a SSET\n",
    "for sset in SSET:\n",
    "    assert sset[0] == strText[sset[1]: sset[2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 From Corpus to Folders and From Folder to Texts\n",
    "\n",
    "\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "Folders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "\n",
    "pprint(Folders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "\n",
    "for folderPath in Folders:\n",
    "    print(folderPath)\n",
    "    fileNames = Folders[folderPath]\n",
    "    \n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        print(textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 Text to Sentences and Sentence to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 Saved Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "# STAGE 2\n",
    "from nlptext.utils.pyramid import reCutText2Sent\n",
    "from nlptext.utils.pyramid import segText2Sents, segSent2Tokens# (text, method = 'whole')\n",
    "\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'corpus/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed' # TODO\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "################   Things to Save   ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['corpusFileIden'] = corpusFileIden # None if Dir else\n",
    "CORPUS['CORPUSType']     = 'File' if corpusFileIden else 'Dir'\n",
    "CORPUS['textType'] = textType\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['folderPaths'] = [] \n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "        \n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['Text2SentMethod'] = Text2SentMethod\n",
    "if textType == 'file':\n",
    "    TEXT['ORIGFileName'] = []\n",
    "if anno:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "    \n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "SENT['Sent2TokenMethod'] = Sent2TokenMethod\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "TOKEN['TOKENLevel'] = TOKENLevel\n",
    "if anno:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "ANNO = {}\n",
    "ANNO['anno'] = anno\n",
    "ANNO['annoKW'] = annoKW\n",
    "\n",
    "    \n",
    "    \n",
    "########################################################\n",
    "##################     CHAINES      ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "###--> CHAIN: from Corpus to Folders <--###\n",
    "\n",
    "CorpusFolders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "assert CORPUS['CORPUSType'] == CORPUSType\n",
    "pprint(CorpusFolders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "for folderIdx, folderPath in enumerate(CorpusFolders):\n",
    "    print(folderPath)\n",
    "    fileNames = CorpusFolders[folderPath]\n",
    "    \n",
    "    ###--> CHAIN: from Folder to Texts <--###\n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        \n",
    "        print('\\n', textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        \n",
    "        ###--> CHAIN: from strText to strSents <--###\n",
    "        strSents = segText2Sents(strText, method = Text2SentMethod) # fixed\n",
    "        \n",
    "        for strSent in strSents:\n",
    "            #- print(strSent)\n",
    "            ###--> CHAIN: from strSent to strTokens <--###\n",
    "            strTokens = segSent2Tokens(strSent, method = Sent2TokenMethod)\n",
    "            \n",
    "            ###--> CHAIN's End: Token itself <--###\n",
    "            #- print(strTokens)\n",
    "            TOKEN['ORIGToken'].extend(strTokens)\n",
    "            \n",
    "            lenSent = len(strTokens)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "            \n",
    "        \n",
    "        lenText = len(strSents)\n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "            \n",
    "        if origTextName:\n",
    "            TEXT['ORIGFileName'].append(origTextName)\n",
    "            \n",
    "            \n",
    "        ########################################\n",
    "        if anno:\n",
    "            # TOKEN['ANNOToken'] = []\n",
    "            # assert SSETText   != [] # May occur Errors\n",
    "            for sset in SSETText:\n",
    "                assert sset[0] == strText[sset[1]: sset[2]]\n",
    "            if SSETText == []:\n",
    "                print('\\nThe SSET of this Text is Empty!!!')\n",
    "                print(strText, '\\n') # to check what happen\n",
    "                    \n",
    "            ############### PART One: Get CITText ###########\n",
    "            #\n",
    "            # CITText  = foo1(strText, SSETText)\n",
    "            # \n",
    "            \n",
    "            from nlptext.utils.pyramid import getCITText\n",
    "            CITText = getCITText(strText, SSETText)\n",
    "            #- print(CITText)\n",
    "            '''\n",
    "            CITAnnoText = []\n",
    "            for sset in SSETText:\n",
    "                # BIOES\n",
    "                strAnno, s, e, tag = sset\n",
    "                CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "                CIT[-1][2] = tag + '-E'\n",
    "                CIT[ 0][2] = tag + '-B'\n",
    "                if len(CIT) == 1:\n",
    "                    CIT[0][2] = tag + '-S' \n",
    "                CITAnnoText.extend(CIT)\n",
    "\n",
    "            # print(strAnnoText)\n",
    "            CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "            for citAnno in CITAnnoText:\n",
    "                c, idx, t = citAnno\n",
    "                assert CITText[idx][0] == c\n",
    "                CITText[idx] = citAnno\n",
    "            # CITText \n",
    "            # Here we get a CITText\n",
    "            #- pprint(CITText)\n",
    "            '''\n",
    "            \n",
    "            \n",
    "                \n",
    "            ############### PART TWO: Get CITSents ###########\n",
    "            #\n",
    "            # CITSents = foo2(strSents, CITText)\n",
    "            #\n",
    "            \n",
    "            from nlptext.utils.pyramid import getCITSents\n",
    "            CITSents = getCITSents(strSents, CITText)\n",
    "            '''\n",
    "            lenLastSent = 0\n",
    "            collapse    = 0 # don't need to move \n",
    "            CITSents = []\n",
    "            for strSent in strSents:\n",
    "                CITSent = []\n",
    "                for sentTokenIdx, c in enumerate(strSent):\n",
    "                    # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "                    txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                    cT, _, tT = CITText[txtTokenIdx]\n",
    "                    while c != cT:\n",
    "                        collapse = collapse + 1\n",
    "                        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                        cT, _, tT = CITText[txtTokenIdx]\n",
    "                    CITSent.append([c,sentTokenIdx, tT])\n",
    "                lenLastSent = lenLastSent + len(strSent)\n",
    "                CITSents.append(CITSent)\n",
    "            # CITSents\n",
    "            # Here we get CITSents \n",
    "            '''\n",
    "            \n",
    "\n",
    "            ############### PART THREE: Get TOKEN['ANNOToken'] ###########\n",
    "            #\n",
    "            # TOKEN['ANNOToken'] = foo3(CITSents, strSents)\n",
    "            #\n",
    "            for sentIdx, CITSent in enumerate(CITSents):\n",
    "                \n",
    "                # Corporate into TOKEN['ANNOToken']\n",
    "                # pay attention here, CIT is char-based, but TOKEN may be word-based.\n",
    "                # strTokens = segSent2Tokens(strSent, method=Sent2TokenMethod)\n",
    "            \n",
    "                if TOKENLevel == 'char':\n",
    "                    TOKEN['ANNOToken'].extend([CITToken[2] for CITToken in CITSent])\n",
    "                    #- pprint(sentIdx)\n",
    "                    #- pprint(CITSent)\n",
    "                else:\n",
    "                    # TODO\n",
    "                    pass \n",
    "            # save the file\n",
    "            \n",
    "            if annoTextName:\n",
    "                TEXT['ANNOFileName'].append(origTextName)\n",
    "            \n",
    "        \n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "    \n",
    "    # Back to Folder\n",
    "    lenFolder = textIdx\n",
    "    FOLDER['folderPaths'].append(folderPath)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder) # to remove\n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "        \n",
    "# End here\n",
    "lenCorpus = folderIdx\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "txt = Text(9)\n",
    "txt.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def modify(line):\n",
    "#     L = []\n",
    "#     for char in line:\n",
    "#         if char >= '\\u4e00' and char <= '\\u9fff' or char == ' ' :\n",
    "#             L.append(char)\n",
    "#     return ''.join(L), 0\n",
    "            \n",
    "\n",
    "# total_strange = 0\n",
    "# with open('dataset/WikiTotal/WikiTotal7k_v2.txt', 'r') as f1:\n",
    "#     with open('dataset/WikiTotal/WikiTotal_cn.txt', 'w') as f2:\n",
    "#         lastkey = ''\n",
    "#         i = 0\n",
    "#         count = 1\n",
    "#         for line in f1.readlines():\n",
    "#             # line = strQ2B(line)# .decode()\n",
    "\n",
    "#             line, strange = modify(line)\n",
    "#             total_strange = total_strange + strange\n",
    "#             f2.write(line+'\\n')\n",
    "#             if i % 500000 == 0:\n",
    "#                 print(i, total_strange, datetime.now())\n",
    "#             i  = i + 1\n",
    "            \n",
    "# print('Total Strange:', total_strange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # to_remove_char = [ '》', '《', '「', '」','{', '}','『', '』', '〉', '〈', '→', 'の','【', '】','β', 'α']\n",
    "# # # def modify(line):\n",
    "# # #     L = []\n",
    "# # #     for char in line:\n",
    "        \n",
    "# # #         if char >= '\\u4e00' and char <= '\\u9fff':\n",
    "# # #             L.append(char)\n",
    "        \n",
    "# # #         else:\n",
    "# # #             inside_code = ord(char)\n",
    "# # #             if inside_code == 12288:\n",
    "# # #                 inside_code = 32\n",
    "# # #             elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "# # #                 inside_code -= 65248\n",
    "# # #             char = chr(inside_code)\n",
    "            \n",
    "# # #             if char in selected_non_cn_char:\n",
    "# # #                 L.append(char)\n",
    "            \n",
    "# # #     return ''.join(L)\n",
    "            \n",
    "\n",
    "\n",
    "# # from datetime import datetime\n",
    "# # # BasicObject.BUILD_LIST_GRAIN_UNIQUE_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "# # CORPUSPath = 'dataset/WikiTotal/'\n",
    "\n",
    "# # total_strange = 0\n",
    "# # with open('dataset/WikiTotal/WikiTotal2.txt', 'r') as f1:\n",
    "# #     with open('dataset/WikiTotal/WikiTotal6k.txt', 'w') as f2:\n",
    "# #         lastkey = ''\n",
    "# #         i = 0\n",
    "# #         count = 1\n",
    "# #         for line in f1.readlines():\n",
    "# #             # line = strQ2B(line)# .decode()\n",
    "\n",
    "# #             line, strange = modify(line)\n",
    "# #             total_strange = total_strange + strange\n",
    "# #             # line = ''.join([i for i in line if i in pre_given_list])\n",
    "# #             # key  = line.replace('\\n', '').split('\\t')[1]\n",
    "# #             # line = line.replace('\\n', '\\t' + str(count) + '\\n') \n",
    "# #             f2.write(line+'\\n')\n",
    "# #             if i % 500000 == 0:\n",
    "# #                 print(i, total_strange, datetime.now())\n",
    "# #             i  = i + 1\n",
    "            \n",
    "# # print('Total Strange:', total_strange)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def modify(line):\n",
    "#     L = []\n",
    "#     strange = 0\n",
    "#     for char in line:\n",
    "#         # if char >= '\\u4e00' and char <= '\\u9fff':\n",
    "#         if char not in to_remove_char_dict:\n",
    "#             # print(char)\n",
    "#             # char = '𐩧'\n",
    "#             # strange = strange + 1\n",
    "#             L.append(char)\n",
    "#         elif char == '𐩧':\n",
    "#             L.append('*')\n",
    "#     return ''.join(L), strange\n",
    "                \n",
    "          \n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# # # BasicObject.BUILD_LIST_GRAIN_UNIQUE_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "# # CORPUSPath = 'dataset/WikiTotal/'\n",
    "\n",
    "# total_strange = 0\n",
    "# with open('dataset/WikiTotal/WikiTotal7k.txt', 'r') as f1:\n",
    "#     with open('dataset/WikiTotal/WikiTotal7k_v2.txt', 'w') as f2:\n",
    "#         lastkey = ''\n",
    "#         i = 0\n",
    "#         count = 1\n",
    "#         for line in f1.readlines():\n",
    "#             # line = strQ2B(line)# .decode()\n",
    "#             if line == '\\n':\n",
    "#                 continue\n",
    "#             line, strange = modify(line)\n",
    "#             total_strange = total_strange + strange\n",
    "#             # line = ''.join([i for i in line if i in pre_given_list])\n",
    "#             # key  = line.replace('\\n', '').split('\\t')[1]\n",
    "#             # line = line.replace('\\n', '\\t' + str(count) + '\\n') \n",
    "#             f2.write(line)\n",
    "#             if i % 500000 == 0:\n",
    "#                 print(i, total_strange, datetime.now())\n",
    "#             i  = i + 1\n",
    "            \n",
    "# print('Total Strange:', total_strange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7k\n",
    "```\n",
    "0 0 2019-04-01 20:17:44.575479\n",
    "500000 6872 2019-04-01 20:18:00.339529\n",
    "1000000 13107 2019-04-01 20:18:13.829641\n",
    "1500000 18364 2019-04-01 20:18:26.433165\n",
    "2000000 24764 2019-04-01 20:18:38.503569\n",
    "2500000 31154 2019-04-01 20:18:50.281386\n",
    "3000000 34702 2019-04-01 20:18:58.808097\n",
    "3500000 39756 2019-04-01 20:19:09.941478\n",
    "4000000 43891 2019-04-01 20:19:21.889133\n",
    "4500000 49958 2019-04-01 20:19:33.441578\n",
    "Total Strange: 52514\n",
    "```\n",
    "\n",
    "* 6k\n",
    "```\n",
    "0 0 2019-04-01 20:22:29.925298\n",
    "500000 16943 2019-04-01 20:22:45.594240\n",
    "1000000 32007 2019-04-01 20:22:58.979631\n",
    "1500000 44940 2019-04-01 20:23:11.898943\n",
    "2000000 60453 2019-04-01 20:23:23.856685\n",
    "2500000 75492 2019-04-01 20:23:36.061031\n",
    "3000000 83949 2019-04-01 20:23:44.742128\n",
    "3500000 96150 2019-04-01 20:23:55.557540\n",
    "4000000 106365 2019-04-01 20:24:07.696283\n",
    "4500000 119614 2019-04-01 20:24:19.378625\n",
    "Total Strange: 125211\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 * 125211/700000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path2Pyramid = 'channel/boson/char/Token3871/Pyramid'\n",
    "# Path2LGUnique = 'channel/boson/char/Token3871/LGUnique/'\n",
    "\n",
    "# from pprint import pprint\n",
    "# from nlptext.base import BasicObject\n",
    "# from nlptext.utils import reCutText2Sent\n",
    "\n",
    "# BasicObject.INIT_FROM_PICKLE(Path2Pyramid, Path2LGUnique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
