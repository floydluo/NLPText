{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Brief Overview for Pyramid\n",
    "\n",
    "\n",
    "Show the pyramid structures: Corpus, Folder, Text, Sentence, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [['1.病史：患者为63岁女性，慢性病程，急性加重。',\n",
    "'既往有“高脂血症”病史。',\n",
    "'2.因“反复脐周疼痛2年余，再发并加重1周”入院。'],\n",
    "['3.体查：血压128/63mmHg，神志清楚，浅表淋巴结无肿大，口唇无苍白。',\n",
    "'双侧扁桃体无肿大、充血。咽无充血。颈静脉无怒张。'],\n",
    "['双肺呼吸音清晰，未闻及干湿性啰音，无胸膜摩擦音。',\n",
    "'心率62次/分，律齐，各瓣膜区未闻及病理性杂音。',\n",
    "'腹部平坦，未见胃肠型及蠕动波，腹壁柔软，脐周压痛，无反跳痛，未扪及包块，肝脾肋下未扪及，Murphy征（-）',\n",
    "'肝肾区无叩击痛，移动性浊音（-），肠鸣音4次/分。',\n",
    "'双下肢无浮肿。四肢肌力、肌张力正常，生理反射存在，病理反射未引出。']]\n",
    "# print(len(corpus))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DICT = {}\n",
    "TEXT_DICT['NUMSents'] = []\n",
    "TEXT_DICT['EndIDXSents'] = []\n",
    "\n",
    "SENT_DICT = {}\n",
    "SENT_DICT['NUMTokens'] = []\n",
    "SENT_DICT['EndIDXTokens'] = []\n",
    "\n",
    "TOKEN_DICT = {}\n",
    "TOKEN_DICT['DATAToken'] = []\n",
    "\n",
    "for text in corpus:\n",
    "    # get text feature\n",
    "    \n",
    "    lenText = len(text)\n",
    "    TEXT_DICT['NUMSents'].append(lenText)\n",
    "    try:\n",
    "        TEXT_DICT['EndIDXSents'].append(SENT_DICT['EndIDXTokens'][-1] + lenText)\n",
    "    except:\n",
    "        TEXT_DICT['EndIDXSents'].append(lenText)\n",
    "    for sent in text:\n",
    "        lenSent = len(sent)\n",
    "        SENT_DICT['NUMTokens'].append(lenSent)\n",
    "        try:\n",
    "            SENT_DICT['EndIDXTokens'].append(SENT_DICT['EndIDXTokens'][-1] + lenSent)\n",
    "        except:\n",
    "            SENT_DICT['EndIDXTokens'].append(lenSent)\n",
    "        \n",
    "        \n",
    "        TOKEN_DICT['DATAToken'].extend([token for token in sent])\n",
    "               \n",
    "\n",
    "print('Text  Level Dictionary')\n",
    "print(TEXT_DICT)\n",
    "print()\n",
    "print('Sent  Level Dictionary')\n",
    "print(SENT_DICT)\n",
    "print()\n",
    "print('Token Level Dictionary')\n",
    "print(TOKEN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentId = 0\n",
    "StartIdx = SENT_DICT['EndIDXTokens'][sentId-1] if sentId != 0 else 0 # this is more faster\n",
    "EndIdx   = SENT_DICT['EndIDXTokens'][sentId]\n",
    "\n",
    "print(StartIdx, EndIdx)\n",
    "print(''.join(TOKEN_DICT['DATAToken'][StartIdx: EndIdx]))\n",
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Corpus to Folders and Folder to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "    \n",
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Folder to Texts\n",
    "\n",
    "Each text is a tuple for strText, SSET, orig_file_name, anno_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "FolderTextsReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/ner/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.Entity'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/medpos/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'file'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "    \n",
    "    \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUSPath = 'corpus/boson/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = '.txt')[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'line'\n",
    "\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "     \n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strText is then be spilted into sentences.\n",
    "\n",
    "If SSET exists, we need to match SSET and the splited sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##################################################################################################TEXT-SENT\n",
    "def reCutText2Sent(text, useSep = False):\n",
    "    \n",
    "    ###################### Remove some weird chars #######################\n",
    "    text = re.sub('\\xa0', '', text)\n",
    "    \n",
    "    ############# The Issue of Spaces\n",
    "    ###################### Convert the Spaces between two English Letters to 'ⴷ' #################\n",
    "    # Take care of Spaces\n",
    "    text = re.sub(r'(?<=[A-Za-z])\\s+(?=[|A-Za-z])', 'ⴷ',  text)\n",
    "    \n",
    "    ###################### Convert the S+ spaces to '〰' #################\n",
    "    text = re.sub(' {2}', '〰', text ).strip()\n",
    "    if useSep == ' ':\n",
    "        # if using space to sep the words\n",
    "        text = text.replace('\\t','').replace('〰', ' ')\n",
    "    elif useSep == '\\t':\n",
    "        # if using tab to sep the words, removing all spaces\n",
    "        text = text.replace(' ','').replace('〰', '')\n",
    "    else:\n",
    "        # if there is no sep char for Chinese, remove single space, and then convert space+ to single space\n",
    "        text = text.replace('\\t','').replace(' ', '',).replace('〰', ' ')\n",
    "        \n",
    "    # convert the spaces between English letters to single spaces\n",
    "    text = text.replace('ⴷ', ' ')\n",
    "    \n",
    "    # Other Things\n",
    "    text = re.sub('([。！!;；])([^”])', r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\.{6})([^”])',    r\"\\1\\n\\2\",text) \n",
    "    text = re.sub('(\\…{2})([^”])',    r\"\\1\\n\\2\",text)\n",
    "    \n",
    "    # The \\n within \" \" is not considered\n",
    "    text = '\"'.join( [ x if i % 2 == 0 else x.replace('\\n', '') \n",
    "                         for i, x in enumerate(text.split('\"'))] )\n",
    "    text = re.sub( '\\n+', '\\n', text ).strip() # replace '\\n+' to '\\n'\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [sent.strip() for sent in text]\n",
    "    # text = [sent.replace(' ', '').replace('\\\\n', '') for sent in text]\n",
    "    return [sent for sent in text if len(sent)>=2]\n",
    "\n",
    "\n",
    "text = '浙江在线杭州4月25日讯(记者施宇翔 通讯员 方英)毒贩很“时髦”,用微信交易毒品。没料想警方也很“潮”,将计就计,一举将其擒获。记者从杭州江干区公安分局了解到,经过一个多月的侦查工作,江干区禁毒专案组抓获吸贩毒人员5名,缴获“冰毒”400余克,毒资30000余元,扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓,经常变换住址。他有一辆车,经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓,并且常要活动到凌晨6、7点钟,白天则在家里呼呼大睡。钱某不寻常的特征,引起了警方注意。禁毒大队通过侦查,发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。'\n",
    "print(text)\n",
    "reCutText2Sent(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segText2Sents(text, method = 'whole', **kwargs):\n",
    "    \n",
    "    '''\n",
    "    text:\n",
    "        1. textfilepath. 2. text-level string\n",
    "    method: \n",
    "        1. 'whole': when text is a text-level string,then use this text-level string as sent-level string directly.\n",
    "                    and return text = [sent-level string].\n",
    "        2. `funct`: when method is a function, whose input is a text-level string,\n",
    "                    then return text = funct(text) = [..., sent-level string, ...]\n",
    "        3. 'line' : string. when text is filepath where each line is a sentence\n",
    "                    then return a generator text = generate(text), item is a sent-level string.        \n",
    "    '''\n",
    "    if os.path.isfile(text):\n",
    "        if method == 'line':\n",
    "            text = lineCutText2Sent(text)\n",
    "            return text\n",
    "        else:\n",
    "            text = fileReader(text)\n",
    "    if method == 'whole':\n",
    "        return [text]\n",
    "    elif method == 're':\n",
    "        return reCutText2Sent(text, **kwargs)\n",
    "    else:\n",
    "        return method(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segSent2Tokens(sent, method = 'iter'):\n",
    "    return [i for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deal with Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "print(strAnnoText)\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "notZeroIndex = 1 \n",
    "\n",
    "sset = SSETText[0]\n",
    "\n",
    "strAnno = sset[0]\n",
    "s       = int(sset[1]) - notZeroIndex\n",
    "tag     = sset[3] \n",
    "CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "CIT[-1][2] = tag + '-E'\n",
    "CIT[0][2]  = tag + '-B'\n",
    "    \n",
    "if len(CIT) == 1:\n",
    "    CIT[2] = tag + '-S'   \n",
    "print(CIT)\n",
    "\n",
    "\n",
    "CITAnnoText = []\n",
    "for sset in SSETText:\n",
    "    strAnno = sset[0]\n",
    "    s       = int(sset[1]) - notZeroIndex\n",
    "    tag     = sset[3] \n",
    "    CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "\n",
    "    CIT[-1][2] = tag + '-E'\n",
    "    CIT[0][2]  = tag + '-B'\n",
    "\n",
    "    if len(CIT) == 1:\n",
    "        CIT[0][2] = tag + '-S' \n",
    "        \n",
    "    CITAnnoText.extend(CIT)\n",
    "    \n",
    "    \n",
    "CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "\n",
    "for citAnno in CITAnnoText:\n",
    "    c, idx, t = citAnno\n",
    "    assert CITText[idx][0] == c\n",
    "    CITText[idx] = citAnno\n",
    "    \n",
    "CITText[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.getCITSents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。']\n",
    "\n",
    "CITText = [['结', 0, 'O'],\n",
    " ['肠', 1, 'O'],\n",
    " ['多', 2, '疾病-B'],\n",
    " ['发', 3, '疾病-I'],\n",
    " ['息', 4, '疾病-I'],\n",
    " ['肉', 5, '疾病-E'],\n",
    " ['。', 6, 'O'],\n",
    " ['\\n', 7, 'O'],\n",
    " ['患', 8, 'O'],\n",
    " ['中', 9, 'O'],\n",
    " ['老', 10, 'O'],\n",
    " ['年', 11, 'O'],\n",
    " ['男', 12, 'O'],\n",
    " ['性', 13, 'O'],\n",
    " [',', 14, 'O'],\n",
    " ['慢', 15, '修饰-B'],\n",
    " ['性', 16, '修饰-E'],\n",
    " ['病', 17, 'O'],\n",
    " ['程', 18, 'O'],\n",
    " ['。', 19, 'O'],\n",
    " [' ', 20, 'O'],]\n",
    "\n",
    "\n",
    "\n",
    "lenLastSent = 0\n",
    "collapse    = 0\n",
    " \n",
    "CITSents = []\n",
    "for strSent in strSents:\n",
    "    CITSent = []\n",
    "    for sentTokenIdx, c in enumerate(strSent):\n",
    "        # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "        txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "        cT, _, tT = CITText[txtTokenIdx]\n",
    "        while c != cT:\n",
    "            collapse = collapse + 1\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            \n",
    "        CITSent.append([c,sentTokenIdx, tT])\n",
    "    lenLastSent = lenLastSent + len(strSent)\n",
    "    CITSents.append(CITSent)\n",
    "CITSents    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`utils.textLineReader` with `anno == 'embed'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "string = 'This is an annotated entity {{2018-12-22:date}}, try to extract it out!'\n",
    "\n",
    "ST = [(block, 'O') if idx%2==0 else (block.split(':')[0], block.split(':')[-1]) \n",
    "    for idx, block in enumerate(string.replace(\"}}\", '{{').split('{{'))]\n",
    "\n",
    "pprint(ST)\n",
    "# SSET, Str, S(char), E(char), Tag.\n",
    "\n",
    "txtCharIdx = 0\n",
    "SSET = []\n",
    "strText = ''\n",
    "for st in ST:\n",
    "    string, tag = st\n",
    "    sset = [string, txtCharIdx, txtCharIdx + len(string), tag]\n",
    "    SSET.append(sset)\n",
    "    txtCharIdx = sset[2]\n",
    "    strText = strText + string\n",
    "    \n",
    "pprint(SSET)\n",
    "pprint(strText) \n",
    "\n",
    "# Only Way to Check a SSET\n",
    "for sset in SSET:\n",
    "    assert sset[0] == strText[sset[1]: sset[2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build Token String to Index Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.infrastructure import UNK_ID, specialTokens, specialTokensDict, strQ2B, fileReader\n",
    "\n",
    "##################################################################################################TOKEN_LTU\n",
    "def buildTokens(tokenList, MaxTokenUnique = None):\n",
    "    \"\"\"\n",
    "        Process raw inputs into a dataset.\n",
    "        words: a list of the whole corpus\n",
    "    \"\"\"\n",
    "    #########################################################################COUNT\n",
    "    total_len_token = len(tokenList)\n",
    "    print('The Total Number of Tokens:', total_len_token)\n",
    "    print('Counting the number unique Tokens...          \\t', datetime.now())\n",
    "    if MaxTokenUnique:\n",
    "        count = collections.Counter(tokenList).most_common(MaxTokenUnique)\n",
    "    else:\n",
    "        count = collections.Counter(tokenList).most_common()\n",
    "    print('\\t\\tDone!')\n",
    "    #########################################################################COUNT\n",
    "\n",
    "    print('Generating Dictionary of Token Unique...\\t', datetime.now())\n",
    "    DTU = specialTokensDict.copy()\n",
    "    DTU_freq = {sp_tk: 0 for sp_tk in specialTokens}\n",
    "    for token, freq in count:\n",
    "        if token is not specialTokens:\n",
    "            DTU[token] = len(DTU)\n",
    "            DTU_freq[token] = freq\n",
    "        else:\n",
    "            DTU_freq[token] = DTU_freq[token] + 1\n",
    "\n",
    "\n",
    "    print('\\t\\tThe length of DTU is:', len(DTU), '\\t', datetime.now())\n",
    "    print('Generating the ORIGTokenIndex...       \\t', datetime.now())\n",
    "    data = np.zeros(len(tokenList), dtype = np.uint32)\n",
    "    # data = []\n",
    "    for idx, token in enumerate(tokenList):\n",
    "        voc_id = DTU.get(token, UNK_ID)\n",
    "        data[idx] = voc_id\n",
    "        if voc_id == UNK_ID:\n",
    "            DTU_freq[UNK] = DTU_freq[UNK] + 1\n",
    "\n",
    "        # data.append(DTU.get(token,UNK_ID))\n",
    "        if idx % 5000000 == 0:\n",
    "            print('\\t\\tThe idx of token is:', idx, '\\t', datetime.now())\n",
    "    print('\\t\\tDone!')\n",
    "    LTU = list(DTU.keys())\n",
    "\n",
    "    if MaxTokenUnique:\n",
    "        print('Only Keep First', MaxTokenUnique, 'Tokens.')\n",
    "        print('The coverage rate is:', np.bincount(data)[UNK_ID]/total_len_token)\n",
    "    # data = np.array(data)\n",
    "    return data, LTU, DTU, DTU_freq\n",
    "##################################################################################################TOKEN_LTU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. From Corpus to Folders and From Folder to Texts\n",
    "\n",
    "\n",
    "There are three methods\n",
    "\n",
    "1. textFile\n",
    "\n",
    "2. textLine\n",
    "\n",
    "3. textBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "Folders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "\n",
    "pprint(Folders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "\n",
    "for folderPath in Folders:\n",
    "    print(folderPath)\n",
    "    fileNames = Folders[folderPath]\n",
    "    \n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        print(textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Text to Sentences and Sentence to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "from pprint import pprint\n",
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "# STAGE 2\n",
    "from nlptext.utils.pyramid import reCutText2Sent\n",
    "from nlptext.utils.pyramid import segText2Sents, segSent2Tokens# (text, method = 'whole')\n",
    "from nlptext.utils.pyramid import getCITSents\n",
    "from nlptext.utils.pyramid import getCITText\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'corpus/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed' # TODO\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "assert anno == False or '.' in anno or anno == 'embed'\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "MaxTextIdx = 10\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "################   Things to Save   ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "CORPUS = {}\n",
    "CORPUS['CORPUSPath'] = CORPUSPath\n",
    "CORPUS['corpusFileIden'] = corpusFileIden # None if Dir else\n",
    "CORPUS['CORPUSType']     = 'File' if corpusFileIden else 'Dir'\n",
    "CORPUS['textType'] = textType\n",
    "\n",
    "FOLDER = {}\n",
    "FOLDER['folderPaths'] = [] \n",
    "FOLDER['NUMTexts'] = []\n",
    "FOLDER['EndIDXTexts'] = []\n",
    "        \n",
    "TEXT = {}\n",
    "TEXT['NUMSents'] = []\n",
    "TEXT['EndIDXSents'] = []\n",
    "TEXT['Text2SentMethod'] = Text2SentMethod\n",
    "if textType == 'file':\n",
    "    TEXT['ORIGFileName'] = []\n",
    "if anno:\n",
    "    TEXT['ANNOFileName'] = []\n",
    "    \n",
    "SENT = {}\n",
    "SENT['NUMTokens'] = []\n",
    "SENT['EndIDXTokens'] = []\n",
    "SENT['Sent2TokenMethod'] = Sent2TokenMethod\n",
    "\n",
    "TOKEN = {}\n",
    "TOKEN['ORIGToken'] = []\n",
    "TOKEN['TOKENLevel'] = TOKENLevel\n",
    "if anno:\n",
    "    TOKEN['ANNOToken'] = []\n",
    "\n",
    "ANNO = {}\n",
    "ANNO['anno'] = anno\n",
    "ANNO['annoKW'] = annoKW\n",
    "\n",
    "    \n",
    "    \n",
    "########################################################\n",
    "##################     CHAINES      ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "###--> CHAIN: from Corpus to Folders <--###\n",
    "\n",
    "CorpusFolders, CORPUSType = CorpusFoldersReader(CORPUSPath, iden = corpusFileIden)\n",
    "assert CORPUS['CORPUSType'] == CORPUSType\n",
    "pprint(CorpusFolders) # all possible files in this directory\n",
    "pprint(CORPUSType)\n",
    "\n",
    "for folderIdx, folderPath in enumerate(CorpusFolders):\n",
    "    print(folderPath)\n",
    "    fileNames = CorpusFolders[folderPath]\n",
    "    \n",
    "    ###--> CHAIN: from Folder to Texts <--###\n",
    "    FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno, **annoKW)\n",
    "    \n",
    "    for textIdx, strText_SSET_O_A in enumerate(FolderTexts):\n",
    "        \n",
    "        # we need to add some constraits to filter the textStrs\n",
    "        strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "        \n",
    "        print('\\n', textIdx, '--', strText)\n",
    "        print(SSETText, '\\n')\n",
    "        \n",
    "        ###--> CHAIN: from strText to strSents <--###\n",
    "        strSents = segText2Sents(strText, method = Text2SentMethod) # fixed\n",
    "        \n",
    "        for strSent in strSents:\n",
    "            #- print(strSent)\n",
    "            ###--> CHAIN: from strSent to strTokens <--###\n",
    "            strTokens = segSent2Tokens(strSent, method = Sent2TokenMethod)\n",
    "            \n",
    "            ###--> CHAIN's End: Token itself <--###\n",
    "            #- print(strTokens)\n",
    "            TOKEN['ORIGToken'].extend(strTokens)\n",
    "            \n",
    "            lenSent = len(strTokens)\n",
    "            SENT['NUMTokens'].append(lenSent)\n",
    "            try:\n",
    "                SENT['EndIDXTokens'].append(SENT['EndIDXTokens'][-1] + lenSent)\n",
    "            except:\n",
    "                SENT['EndIDXTokens'].append(lenSent)\n",
    "            \n",
    "        \n",
    "        lenText = len(strSents)\n",
    "        TEXT['NUMSents'].append(lenText)\n",
    "        try:\n",
    "            TEXT['EndIDXSents'].append(TEXT['EndIDXSents'][-1] + lenText)\n",
    "        except:\n",
    "            TEXT['EndIDXSents'].append(lenText)\n",
    "            \n",
    "        if origTextName:\n",
    "            TEXT['ORIGFileName'].append(origTextName)\n",
    "            \n",
    "            \n",
    "        ########################################\n",
    "        if anno:\n",
    "            # TOKEN['ANNOToken'] = []\n",
    "            # assert SSETText   != [] # May occur Errors\n",
    "            for sset in SSETText:\n",
    "                assert sset[0] == strText[sset[1]: sset[2]]\n",
    "            if SSETText == []:\n",
    "                print('\\nThe SSET of this Text is Empty!!!')\n",
    "                print(strText, '\\n') # to check what happen\n",
    "                    \n",
    "            ############### PART One: Get CITText ###########\n",
    "            #\n",
    "            # CITText  = foo1(strText, SSETText)\n",
    "            # \n",
    "            CITText = getCITText(strText, SSETText)\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            ############### PART TWO: Get CITSents ###########\n",
    "            #\n",
    "            # CITSents = foo2(strSents, CITText)\n",
    "            #\n",
    "            CITSents = getCITSents(strSents, CITText)\n",
    "           \n",
    "\n",
    "            ############### PART THREE: Get TOKEN['ANNOToken'] ###########\n",
    "            #\n",
    "            # TOKEN['ANNOToken'] = foo3(CITSents, strSents)\n",
    "            #\n",
    "            for sentIdx, CITSent in enumerate(CITSents):\n",
    "                \n",
    "                # Corporate into TOKEN['ANNOToken']\n",
    "                # pay attention here, CIT is char-based, but TOKEN may be word-based.\n",
    "                # strTokens = segSent2Tokens(strSent, method=Sent2TokenMethod)\n",
    "            \n",
    "                if TOKENLevel == 'char':\n",
    "                    TOKEN['ANNOToken'].extend([CITToken[2] for CITToken in CITSent])\n",
    "                    #- pprint(sentIdx)\n",
    "                    #- pprint(CITSent)\n",
    "                else:\n",
    "                    # TODO\n",
    "                    pass \n",
    "            # save the file\n",
    "            \n",
    "            if annoTextName:\n",
    "                TEXT['ANNOFileName'].append(origTextName)\n",
    "            \n",
    "        \n",
    "        if textIdx == MaxTextIdx:\n",
    "            break\n",
    "    \n",
    "    # Back to Folder\n",
    "    lenFolder = textIdx\n",
    "    FOLDER['folderPaths'].append(folderPath)\n",
    "    \n",
    "    FOLDER['NUMTexts'].append(lenFolder) # to remove\n",
    "    try:\n",
    "        FOLDER['EndIDXTexts'].append(FOLDER['EndIDXTexts'][-1] + lenFolder)\n",
    "    except:\n",
    "        FOLDER['EndIDXTexts'].append(lenFolder)\n",
    "        \n",
    "# End here\n",
    "lenCorpus = folderIdx\n",
    "CORPUS['NUMFolders'] = [lenCorpus]\n",
    "CORPUS['EndIDXFolders'] = [lenCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "# DictToken = corpus.DictToken\n",
    "# print(DictToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.SENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "txtIdxes = list(set(list(np.random.randint(corpus.TEXT['length'], size = 10))))\n",
    "txtIdxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(corpus.Folders)\n",
    "# print(corpus.FOLDER)\n",
    "# print(corpus.Texts)\n",
    "# print(corpus.TEXT)\n",
    "# print(corpus.Sentences)\n",
    "from nlptext.text import Text\n",
    "\n",
    "sentIdx = 0\n",
    "for txtIdx in txtIdxes:\n",
    "    \n",
    "    txt = Text(txtIdx)\n",
    "    print('\\n', txt, '\\n')\n",
    "    for st in txt.Sentences:\n",
    "        print(sentIdx, '-->',st.sentence)\n",
    "        sentIdx = sentIdx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = corpus.Sentences[31]\n",
    "st.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text\n",
    "loc_idx = 9  # how to interpret loc_idx: support there are n texts in the whole corpus, get the loc_idx th text\n",
    "txt = Text(loc_idx)\n",
    "txt.Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
