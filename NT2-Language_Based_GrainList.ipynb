{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Initialized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.598 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 534211\n",
      "The Total Number of Tokens: 534211\n",
      "Counting the number unique Tokens...          \t 2019-06-08 17:06:30.443100\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-08 17:06:30.608856\n",
      "\t\tThe length of DTU is: 3870 \t 2019-06-08 17:06:30.609617\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-08 17:06:30.609681\n",
      "\t\tThe idx of token is: 0 \t 2019-06-08 17:06:30.609976\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 3870\n",
      "['</pad>', '</start>', '</end>', 'O', 'company_name-B', 'company_name-E', 'company_name-I', 'company_name-S', 'location-B', 'location-E', 'location-I', 'location-S', 'org_name-B', 'org_name-E', 'org_name-I', 'org_name-S', 'person_name-B', 'person_name-E', 'person_name-I', 'person_name-S', 'product_name-B', 'product_name-E', 'product_name-I', 'product_name-S', 'time-B', 'time-E', 'time-I', 'time-S']\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Token3870/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/boson/char/Token3870/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Token3870/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Token3870/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10281\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Token3870/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 534211\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/boson/char/Token3870/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 3870\n",
      "\t\tWrite to: data/boson/char/Token3870/GrainUnique/token.tsv\n",
      "annoE-es\tis Dumped into file: data/boson/char/Token3870/GrainUnique/annoE-es.voc\n",
      "annoE-es\tthe length of it is   : 28\n",
      "\t\tWrite to: data/boson/char/Token3870/GrainUnique/annoE-es.tsv\n",
      "pos-es\tis Dumped into file: data/boson/char/Token3870/GrainUnique/pos-es.voc\n",
      "pos-es\tthe length of it is   : 232\n",
      "\t\tWrite to: data/boson/char/Token3870/GrainUnique/pos-es.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### BOSON ###########\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 22, 22, ...,  3,  3,  3], dtype=uint32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN['ANNOTokenIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Special Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "#### 1. Special Token\n",
    "\n",
    "`PADDING` is 0;\n",
    "\n",
    "`START` is the start of a sentence;\n",
    "\n",
    "`END` is the end of a sentence;\n",
    "\n",
    "So, what is `START`'s channel grain?\n",
    "\n",
    "Itself? Or Unknown?\n",
    "\n",
    "These three tokens are present in every possible channels;\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "These is also an anther special token:\n",
    "\n",
    "`UNK`: the special tokens that is not present in the DictToken.\n",
    "\n",
    "The unk is present in token level channel.\n",
    "\n",
    "If the channel is context-dependent, then the corresponding grains for unk token are not unk grain.\n",
    "\n",
    "If the channel is context-independent, then the corresponding grains for unk token are unk grains.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Interpreting Dict: String to Index Dicts\n",
    "\n",
    "DictToken is TokenString to Index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD   = '</pad>'\n",
    "UNK   = '</unk>'\n",
    "START = '</start>'\n",
    "END   = '</end>'\n",
    "\n",
    "specialTokens     = [ PAD, UNK, START, END]\n",
    "specialTokensDict = {PAD: 0, START: 1, END: 2, UNK : 3, }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get Universe Grain List from a Token or a Sentence\n",
    "\n",
    "**Helper Function**\n",
    "\n",
    "\n",
    "\n",
    "Main module:\n",
    "\n",
    "    `utils/channel.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grainChar(char):\n",
    "    pass \n",
    "\n",
    "\n",
    "\n",
    "def getGrainNgrams(subword_infos, n):\n",
    "    # Here N is the Num for n_gram\n",
    "    #     subword_infos: [subcomp1, subcomp2, ...] or [stroke1, stroke2, ...]\n",
    "    #                 n: the targeted n gram\n",
    "    if n == 1:\n",
    "        return [i for i in subword_infos]\n",
    "    if n > len(subword_infos):\n",
    "        # How to deal this when the length is not so long\n",
    "        # Condition: where n is larger than the infos\n",
    "        \n",
    "        return [] # this or below?\n",
    "        # return [UNK] \n",
    "\n",
    "    l = [subword_infos[i:n+i] for i in range(len(subword_infos) - n + 1)]\n",
    "    l = ['-'.join(i) for i in l]\n",
    "    return l\n",
    "\n",
    "def grainToken(token, grainCharFunction, Ngram = 1,Max_Ngram = None, end_grain = True):\n",
    "    '''\n",
    "        token level only!\n",
    "        The input token is not in Special Tokens\n",
    "        The input token is a string!\n",
    "        TODO: handle the `ngram` problems here.\n",
    "        Content-Idenpendent Only\n",
    "    '''\n",
    "    if token not in specialTokens:\n",
    "        infos = sum([grainCharFunction(char, end_grain) for char in token], [])\n",
    "        if not Max_Ngram:\n",
    "            return getGrainNgrams(infos, Ngram)\n",
    "        else:\n",
    "            return sum([getGrainNgrams(infos, idx+1) for idx in range(Max_Ngram)], [])\n",
    "    else:\n",
    "        return getGrainNgrams([token], Ngram) # deal with the special tokens\n",
    "    \n",
    "    \n",
    "def grainSent_ctxInd(sent, grainCharFunction, Ngram = 1, Max_Ngram = None, end_grain = True):\n",
    "    '''\n",
    "        TODO: to update with context-dependent channel method\n",
    "    '''\n",
    "    return [grainToken(token, grainCharFunction, Ngram, Max_Ngram, end_grain) for token in sent]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 CASE `basic`\n",
    "\n",
    "\n",
    "### context-independent. A Token's Grain List (May More Than One Grain Element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def basicGrainChar(char, end_grain = False):\n",
    "    '''\n",
    "        Char level only, char is 字符 in the computational languages\n",
    "        The input char is a string!\n",
    "        This is only for context-independent channels\n",
    "    '''\n",
    "    punStr = string.punctuation + '＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､\\u3000、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。'\n",
    "    engReg = r'[A-Za-z]{1}'\n",
    "    if '%' in char or '%' in char:\n",
    "        info =  'PERC'\n",
    "    elif re.match(r'[0-9]{1}', char):\n",
    "        info = \"NUM\"\n",
    "    elif char in punStr:\n",
    "        info = \"PUNC\"\n",
    "    elif char >= '\\u4e00' and char <= '\\u9fff':\n",
    "        info = \"CHN\"\n",
    "    elif re.match(engReg, char):\n",
    "        info = 'ENG'\n",
    "    #elif word in string.whitespace:\n",
    "        #return 'SPA'\n",
    "    elif char == '@':\n",
    "        info = 'SPA'\n",
    "    else:\n",
    "        info = 'OTHER'\n",
    "    info = [info]\n",
    "    if end_grain:\n",
    "        info = info + ['b0']\n",
    "    return info\n",
    "    \n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "end_grain = False\n",
    "print(basicGrainChar(char))\n",
    "print(grainToken(token, basicGrainChar,                   end_grain=end_grain))\n",
    "print(grainSent_ctxInd(sent, basicGrainChar,              end_grain=end_grain))\n",
    "print(grainSent_ctxInd(sent, basicGrainChar, Ngram=2,     end_grain=end_grain)) \n",
    "print(grainSent_ctxInd(sent, basicGrainChar, Max_Ngram=2, end_grain=end_grain)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 CASE `radical`\n",
    "\n",
    "\n",
    "### context-independent. A Token's Grain List (May More Than One Grain Element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nlptext/sources/CharRadical.p', 'rb') as handle:\n",
    "    CharRadicalInfos = pickle.load(handle)\n",
    "\n",
    "def radicalGrainChar(char, end_grain = False):\n",
    "    '''\n",
    "        char level only!\n",
    "    '''\n",
    "    if char in CharRadicalInfos:\n",
    "        info = CharRadicalInfos[char]\n",
    "        if info:\n",
    "            info = [info] # here Radical Data is in str not list\n",
    "        else:\n",
    "            info = [char]\n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['r0']\n",
    "    return info\n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "end_grain = False\n",
    "print(radicalGrainChar(char), '\\n')\n",
    "\n",
    "print(grainToken(token,      radicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, radicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, radicalGrainChar, Ngram=2,     end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, radicalGrainChar, Max_Ngram=2, end_grain=end_grain), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 CASE `pos`\n",
    "\n",
    "\n",
    "### context-dependent. A Token's Grain List (May More Than One Grain Element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jieba import posseg\n",
    "\n",
    "def POSGrainSent(sent, tokenLevel = 'word', useStartEnd = True, tagScheme = 'BIO'):\n",
    "    '''\n",
    "        Here only for Atom is Char Based\n",
    "        This method should be enriched\n",
    "\n",
    "        sent: List of Token(String), with or without Start or End\n",
    "        \n",
    "    '''\n",
    "    # d = []\n",
    "    if useStartEnd:\n",
    "        sent = sent[1:-1]\n",
    "    segs = list(posseg.cut(''.join(sent)))\n",
    "    \n",
    "    GrainSent = []\n",
    "    \n",
    "    if tokenLevel == 'word':\n",
    "        for i in range(len(segs)):\n",
    "            pair  = segs[i]\n",
    "            label = pair.flag\n",
    "            GrainSent.append([label])\n",
    "            # print(pair.word)\n",
    "        if useStartEnd:\n",
    "            return [[START]] + GrainSent + [[END]]\n",
    "        return GrainSent\n",
    "    elif tokenLevel == 'char':\n",
    "        for i in range(len(segs)):\n",
    "            '''\n",
    "            pair  = segs[i]\n",
    "            start = sum(len(p.word) for p in segs[:i])\n",
    "            end   = sum(len(p.word) for p in segs[:i+1])\n",
    "            label = pair.flag\n",
    "            d.append([pair.word, start, end, label]) # still: SSET\n",
    "            '''\n",
    "            pair  = segs[i]\n",
    "            leng  = len(pair.word)\n",
    "            label = pair.flag\n",
    "            labels= [label + '-I' ]*leng\n",
    "            labels[0] = label + '-B'\n",
    "            if 'E' in tagScheme and leng >= 2:\n",
    "                labels[-1] = label + '-E'\n",
    "            if 'S' in tagScheme and leng == 1:\n",
    "                labels[0] = label + '-S'\n",
    "            GrainSent.extend([[i] for i in labels])\n",
    "        if useStartEnd:\n",
    "            return [[START]] + GrainSent + [[END]]\n",
    "        return GrainSent\n",
    "    \n",
    "print(POSGrainSent(sent, tokenLevel='word')                   , '\\n')\n",
    "print(POSGrainSent(sent, tokenLevel='char', tagScheme='BIEO') , '\\n')\n",
    "print(POSGrainSent(sent, tokenLevel='char', tagScheme='BIO')  , '\\n')\n",
    "print(POSGrainSent(sent, tokenLevel='char', tagScheme='BIEOS'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grainSent_ctxDep(sent, channelGrainSent, tokenLevel = 'word', tagScheme = 'BIO', useStartEnd = True):\n",
    "    return channelGrainSent(sent, tokenLevel=tokenLevel, tagScheme=tagScheme, useStartEnd = useStartEnd)\n",
    "\n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "pprint(sent)\n",
    "print(len(sent))\n",
    "grainSentFunction = POSGrainSent\n",
    "tokenLevel = 'word'\n",
    "tagScheme = 'BIEO'\n",
    "\n",
    "\n",
    "######## STARS #########\n",
    "posResult = grainSent_ctxDep(sent, grainSentFunction, tokenLevel =tokenLevel , tagScheme = tagScheme)\n",
    "\n",
    "\n",
    "pprint(posResult)\n",
    "print(len(posResult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other Context-Ind Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 CASE: `subcomp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nlptext/sources/CharSubComp.p', 'rb') as handle:\n",
    "    CharSubCompInfos = pickle.load(handle)\n",
    "\n",
    "def subcompGrainChar(char, end_grain = False):\n",
    "    '''\n",
    "        char level only!\n",
    "    '''\n",
    "    if char in CharSubCompInfos:\n",
    "        info = CharSubCompInfos[char]\n",
    "        if info:\n",
    "            info = info \n",
    "        else:\n",
    "            info = [char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['c0']\n",
    "    return info\n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "\n",
    "end_grain = True\n",
    "print(subcompGrainChar(char), '\\n')\n",
    "\n",
    "print(grainToken(token,      subcompGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, subcompGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, subcompGrainChar, Ngram=2,     end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, subcompGrainChar, Max_Ngram=2, end_grain=end_grain), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 CASE: `stroke`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('nlptext/sources/CharStroke.p', 'rb') as handle:\n",
    "    CharStrokeInfos = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "def strokeGrainChar(char, end_grain = False):\n",
    "    '''\n",
    "        char level only!\n",
    "    '''\n",
    "    if char in CharStrokeInfos:\n",
    "        info = CharStrokeInfos[char]\n",
    "        if info:\n",
    "            info = info \n",
    "        else:\n",
    "            info = [char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['r0']\n",
    "    return info\n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "end_grain = True\n",
    "print(strokeGrainChar(char), '\\n')\n",
    "\n",
    "print(grainToken(token,      strokeGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, strokeGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, strokeGrainChar, Ngram=2,     end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, strokeGrainChar, Max_Ngram=2, end_grain=end_grain), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 CASE: `medical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical Dictionary Tag\n",
    "def medicalGrainChar(word, end_grain = False):\n",
    "    # TODO\n",
    "    units = 'kBq kbq mg Mg UG Ug ug MG ml ML Ml GM iu IU u U g G l L cm CM mm s S T % % mol mml mmol MMOL HP hp mmHg umol ng'.split(' ')\n",
    "    chn_units = '毫升 毫克 单位 升 克 第 粒 颗粒 支 件 散 丸 瓶 袋 板 盒 合 包 贴 张 泡 国际单位 万 特充 个 分 次'.split(' ')\n",
    "    med_units = 'qd bid tid qid qh q2h q4h q6h qn qod biw hs am pm St DC prn sos ac pc gtt IM IV po iH'.split(' ')\n",
    "    all_units = units + chn_units + med_units\n",
    "\n",
    "    site_units = '上 下 左 右 间 片 部 内 外 前 侧 后'.split(' ')\n",
    "    sym_units = '大 小 增 减 多 少 升 降 高 低 宽 厚 粗 两 双 延 长 短 疼 痛 终 炎 咳'.split(' ')\n",
    "    part_units = '脑 心 肝 脾 肺 肾 胸 脏 口 腹 胆 眼 耳 鼻 颈 手 足 脚 指 壁 膜 管 窦 室 管 髋 头 骨 膝 肘 肢 腰 背 脊 腿 茎 囊 精 唇 咽'.split(' ')\n",
    "    break_units = '呈 示 见 伴 的 因'.split(' ')\n",
    "    more_units = '较 稍 约 频 偶 偏'.split(' ')\n",
    "    non_units = '无 不 非 未 否'.split(' ')\n",
    "    tr_units = '服 予 行'.split(' ')\n",
    "\n",
    "    if word in units:\n",
    "        info =  'UNIT'\n",
    "    elif word in chn_units:\n",
    "        info =  'CHN_UNIT'\n",
    "    elif word in med_units:\n",
    "        info =  'MED_UNIT'\n",
    "    elif word in site_units:\n",
    "        info =  'SITE_UNIT'\n",
    "    elif word in sym_units:\n",
    "        info =  'SYM_UNIT'\n",
    "    elif word in part_units:\n",
    "        info =  'PART_UNIT'\n",
    "    elif word in break_units:\n",
    "        info =  'BREAK_UNIT'\n",
    "    elif word in more_units:\n",
    "        info =  'more_UNIT'\n",
    "    elif word in non_units:\n",
    "        info =  'NON_UNIT'\n",
    "    elif word in tr_units:\n",
    "        info =  'TR_UNIT'\n",
    "    else:\n",
    "        info =  'OTHER'\n",
    "        \n",
    "    # return [info]\n",
    "    info = [info]\n",
    "    if end_grain:\n",
    "        info = info + ['m0']\n",
    "    return info\n",
    "    \n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "end_grain = False\n",
    "print(medicalGrainChar(char), '\\n')\n",
    "\n",
    "print(grainToken(token,      medicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar, Ngram=2,     end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar, Max_Ngram=2, end_grain=end_grain), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def charGrainChar(char, end_grain = False):\n",
    "    '''\n",
    "        char level only!\n",
    "    '''\n",
    "    info = [char]\n",
    "    if end_grain:\n",
    "        info = info + ['ch0']\n",
    "    return info\n",
    "\n",
    "\n",
    "char = '病'\n",
    "token = '病理性'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "\n",
    "end_grain = False\n",
    "print(medicalGrainChar(char), '\\n')\n",
    "\n",
    "print(grainToken(token,      medicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar,              end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar, Ngram=2,     end_grain=end_grain), '\\n')\n",
    "print(grainSent_ctxInd(sent, medicalGrainChar, Max_Ngram=2, end_grain=end_grain), '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Context-Dep Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Wrapped Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getChannelGrain4Token(token, channel, Ngram = 1, Max_Ngram = None,  end_grain = False):\n",
    "    '''\n",
    "        token level only!\n",
    "        The input token is not in Special Tokens\n",
    "        The input token is a string!\n",
    "        TODO: handle the `ngram` problems here.\n",
    "        Content-Idenpendent Only\n",
    "    '''\n",
    "\n",
    "    if channel == 'token':\n",
    "        return [token]\n",
    "\n",
    "    if channel == 'basic':\n",
    "        return grainToken(token, basicGrainChar,   Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    if channel == 'medical':\n",
    "        return grainToken(token, medicalGrainChar, Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    if channel == 'radical':\n",
    "        return grainToken(token, radicalGrainChar, Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    if channel == 'subcomp':\n",
    "        return grainToken(token, subcompGrainChar, Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    if channel == 'stroke':\n",
    "        return grainToken(token, strokeGrainChar,  Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    if channel == 'char':\n",
    "        return grainToken(token, charGrainChar,    Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    \n",
    "    else:\n",
    "        print('The Channel \"', channel, '\" is not available currently!')\n",
    "\n",
    "        \n",
    "channel = 'stroke'\n",
    "token = '四方'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Max_Ngram= 2, end_grain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_IND_CHANNELS = ['basic', 'medical', 'radical', 'token', 'char', 'subcomp', 'stroke']\n",
    "CONTEXT_DEP_CHANNELS = ['pos']\n",
    "\n",
    "def getChannelGrain4Sent(sent, channel, Ngram = 1, Max_Ngram = None, tokenLevel = 'char', tagScheme =  'BIO', useStartEnd = True, end_grain = False):\n",
    "    '''\n",
    "        token level only!\n",
    "        The input token is not in Special Tokens\n",
    "        The input token is a string!\n",
    "        TODO: handle the `ngram` problems here.\n",
    "        Content-Idenpendent Only\n",
    "    '''\n",
    "    if channel in CONTEXT_IND_CHANNELS:\n",
    "        return [getChannelGrain4Token(token, channel, \n",
    "                                      Ngram = Ngram, \n",
    "                                      Max_Ngram = Max_Ngram, \n",
    "                                      end_grain = False) for token in sent]\n",
    "        \n",
    "    elif channel in CONTEXT_DEP_CHANNELS:\n",
    "        if channel == 'pos':\n",
    "            return grainSent_ctxDep(sent, POSGrainSent, tokenLevel =tokenLevel , tagScheme = tagScheme, useStartEnd = useStartEnd)\n",
    "    \n",
    "\n",
    "channel = 'pos'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "print(''.join(sent))\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, tokenLevel = 'char')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'getChannelGrain4Token' from 'nlptext.utils.channel' (/home/floydluo/Desktop/nlptext/nlptext/utils/channel.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7aea15ed1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mspecialTokensDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNK\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlptext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetChannelGrain4Token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetChannelGrain4Sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'getChannelGrain4Token' from 'nlptext.utils.channel' (/home/floydluo/Desktop/nlptext/nlptext/utils/channel.py)"
     ]
    }
   ],
   "source": [
    "PAD   = '</pad>'\n",
    "START = '</start>'\n",
    "END   = '</end>'\n",
    "UNK   = '</unk>'\n",
    "\n",
    "specialTokens     = [ PAD, UNK, START, END]\n",
    "specialTokensDict = {PAD: 0, START: 1, END: 2, UNK : 3, }\n",
    "\n",
    "from nlptext.utils.channel import getChannelGrain4Token, getChannelGrain4Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'radical'\n",
    "token = '四方'\n",
    "\n",
    "getChannelGrain4Token(token, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'radical'\n",
    "sent = [START] + '心率 62 次 / 分 ， 律 齐 ， 各 瓣膜 区 未闻 及 病理性 杂音 。'.split(' ') + [END]\n",
    "getChannelGrain4Sent(sent, channel)                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test NLPText\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from nlptext.utils import reCutText2Sent\n",
    "\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'dataset/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n",
    "print([i for i in corpus.TOKEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Test Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = corpus.Tokens[35]\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'radical'\n",
    "tk.getChannelGrain(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'subcomp'\n",
    "tk.getChannelGrain(channel, end_grain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'subcomp'\n",
    "\n",
    "tk.getChannelGrain(channel, Ngram=2, end_grain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'stroke'\n",
    "print(tk.getChannelGrain(channel, Max_Ngram=1, end_grain=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Test Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = corpus.Sentences[2]\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'subcomp'\n",
    "\n",
    "st.getChannelGrain(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel = 'pos'\n",
    "tagScheme = 'BIOES' # 'BIOE'\n",
    "useStartEnd = True\n",
    "\n",
    "st.getChannelGrain(channel, tagScheme = tagScheme, useStartEnd = useStartEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'annoE'\n",
    "tagScheme = 'BIOE' # 'BIOE'\n",
    "useStartEnd = False\n",
    "\n",
    "annoE = st.getChannelGrain(channel, tagScheme = tagScheme, useStartEnd = useStartEnd)\n",
    "list(zip(st.sentence, annoE))\n",
    "# TODO, think more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -- END --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
