{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Initialized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n",
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text from Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.610 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现代 逻辑 被 分成 递归论 、 模型 论 和 证明 论 , 且 和 理论 电脑 科学 有著 密切 的 关连性 , 千禧年 大奖 难题 中 的 P / NP 问题 就是 理论 电脑 科学 中 的 著名 问题 。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "locidx = 25\n",
    "obj = Text(locidx)\n",
    "obj.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现代 逻辑 被 分成 递归论 、 模型 论 和 证明 论 , 且 和 理论 电脑 科学 有著 密切 的 关连性 , 千禧年 大奖 难题 中 的 P / NP 问题 就是 理论 电脑 科学 中 的 著名 问题 。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_stored_hyper('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'149 85 121 185 85 209 85 185 21 85 225 209 21 21 85 85 85 185 5 169 85 209 93 113 85 45 169 41 209 41 85 25 85 85 85 45 169 1 85 209'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_stored_hyper('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 85, 121, 185, 85, 209, 85, 185, 21, 85, 225, 209, 21, 21, 85, 85, 85, 185, 5, 169, 85, 209, 93, 113, 85, 45, 169, 41, 209, 41, 85, 25, 85, 85, 85, 45, 169, 1, 85, 209]\n"
     ]
    }
   ],
   "source": [
    "print(obj.get_stored_hypertagscheme('pos', 'BIOES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild GrainUnique for channel: pos-bio\n",
      "pos 1 False BIO\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bio.voc\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bio.tsv\n",
      "['t-B', 'n-B', 'p-B', 'v-B', 'n-B', 'x-B', 'n-B', 'v-B', 'c-B', 'n-B', 'zg-B', 'x-B', 'c-B', 'c-B', 'n-B', 'n-B', 'n-B', 'v-B', 'ad-B', 'uj-B', 'n-B', 'x-B', 'nr-B', 'nz-B', 'n-B', 'f-B', 'uj-B', 'eng-B', 'x-B', 'eng-B', 'n-B', 'd-B', 'n-B', 'n-B', 'n-B', 'f-B', 'uj-B', 'a-B', 'n-B', 'x-B']\n"
     ]
    }
   ],
   "source": [
    "print(obj.get_stored_hyperstring('pos', 'BIO'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Newly Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence from Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现代 逻辑 被 分成 递归论 、 模型 论 和 证明 论 , 且 和 理论 电脑 科学 有著 密切 的 关连性 , 千禧年 大奖 难题 中 的 P / NP 问题 就是 理论 电脑 科学 中 的 著名 问题 。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 25\n",
    "obj = Sentence(locidx)\n",
    "obj.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现代 逻辑 被 分成 递归论 、 模型 论 和 证明 论 , 且 和 理论 电脑 科学 有著 密切 的 关连性 , 千禧年 大奖 难题 中 的 P / NP 问题 就是 理论 电脑 科学 中 的 著名 问题 。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_stored_hyper('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 85, 121, 185, 85, 209, 85, 185, 21, 85, 225, 209, 21, 21, 85, 85, 85, 185, 5, 169, 85, 209, 93, 113, 85, 45, 169, 41, 209, 41, 85, 25, 85, 85, 85, 45, 169, 1, 85, 209]\n"
     ]
    }
   ],
   "source": [
    "print(obj.get_stored_hypertagscheme('pos', 'BIOES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t-B', 'n-B', 'p-B', 'v-B', 'n-B', 'x-B', 'n-B', 'v-B', 'c-B', 'n-B', 'zg-B', 'x-B', 'c-B', 'c-B', 'n-B', 'n-B', 'n-B', 'v-B', 'ad-B', 'uj-B', 'n-B', 'x-B', 'nr-B', 'nz-B', 'n-B', 'f-B', 'uj-B', 'eng-B', 'x-B', 'eng-B', 'n-B', 'd-B', 'n-B', 'n-B', 'n-B', 'f-B', 'uj-B', 'a-B', 'n-B', 'x-B']\n"
     ]
    }
   ],
   "source": [
    "print(obj.get_stored_hyperstring('pos', 'BIOES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t-B', 'n-B', 'p-B', 'v-B', 'n-B', 'x-B', 'n-B', 'v-B', 'c-B', 'n-B', 'zg-B', 'x-B', 'c-B', 'c-B', 'n-B', 'n-B', 'n-B', 'v-B', 'ad-B', 'uj-B', 'n-B', 'x-B', 'nr-B', 'nz-B', 'n-B', 'f-B', 'uj-B', 'eng-B', 'x-B', 'eng-B', 'n-B', 'd-B', 'n-B', 'n-B', 'n-B', 'f-B', 'uj-B', 'a-B', 'n-B', 'x-B']\n"
     ]
    }
   ],
   "source": [
    "print(obj.get_stored_hyperstring('pos', 'BIO'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_grain_str and get_grain_idx\n",
    "\n",
    "#### Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['现代'],\n",
       " ['逻辑'],\n",
       " ['被'],\n",
       " ['分成'],\n",
       " ['递归论'],\n",
       " ['、'],\n",
       " ['模型'],\n",
       " ['论'],\n",
       " ['和'],\n",
       " ['证明'],\n",
       " ['论'],\n",
       " [','],\n",
       " ['且'],\n",
       " ['和'],\n",
       " ['理论'],\n",
       " ['电脑'],\n",
       " ['科学'],\n",
       " ['有著'],\n",
       " ['密切'],\n",
       " ['的'],\n",
       " ['关连性'],\n",
       " [','],\n",
       " ['千禧年'],\n",
       " ['大奖'],\n",
       " ['难题'],\n",
       " ['中'],\n",
       " ['的'],\n",
       " ['P'],\n",
       " ['/'],\n",
       " ['NP'],\n",
       " ['问题'],\n",
       " ['就是'],\n",
       " ['理论'],\n",
       " ['电脑'],\n",
       " ['科学'],\n",
       " ['中'],\n",
       " ['的'],\n",
       " ['著名'],\n",
       " ['问题'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_grain_str('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[217],\n",
       " [103],\n",
       " [12],\n",
       " [795],\n",
       " [1743],\n",
       " [3],\n",
       " [486],\n",
       " [485],\n",
       " [4],\n",
       " [99],\n",
       " [485],\n",
       " [1],\n",
       " [50],\n",
       " [4],\n",
       " [31],\n",
       " [129],\n",
       " [28],\n",
       " [95],\n",
       " [833],\n",
       " [0],\n",
       " [1744],\n",
       " [1],\n",
       " [453],\n",
       " [452],\n",
       " [451],\n",
       " [10],\n",
       " [0],\n",
       " [815],\n",
       " [808],\n",
       " [794],\n",
       " [18],\n",
       " [123],\n",
       " [31],\n",
       " [129],\n",
       " [28],\n",
       " [10],\n",
       " [0],\n",
       " [173],\n",
       " [18],\n",
       " [2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info, leng_st, leng_tk, max_gr =  obj.get_grain_idx('token')\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild GrainUnique for channel: pos-bioe\n",
      "pos 1 False BIOE\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioe.voc\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioe.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['t-B'],\n",
       " ['n-B'],\n",
       " ['p-B'],\n",
       " ['v-B'],\n",
       " ['n-B'],\n",
       " ['x-B'],\n",
       " ['n-B'],\n",
       " ['v-B'],\n",
       " ['c-B'],\n",
       " ['n-B'],\n",
       " ['zg-B'],\n",
       " ['x-B'],\n",
       " ['c-B'],\n",
       " ['c-B'],\n",
       " ['n-B'],\n",
       " ['n-B'],\n",
       " ['n-B'],\n",
       " ['v-B'],\n",
       " ['ad-B'],\n",
       " ['uj-B'],\n",
       " ['n-B'],\n",
       " ['x-B'],\n",
       " ['nr-B'],\n",
       " ['nz-B'],\n",
       " ['n-B'],\n",
       " ['f-B'],\n",
       " ['uj-B'],\n",
       " ['eng-B'],\n",
       " ['x-B'],\n",
       " ['eng-B'],\n",
       " ['n-B'],\n",
       " ['d-B'],\n",
       " ['n-B'],\n",
       " ['n-B'],\n",
       " ['n-B'],\n",
       " ['f-B'],\n",
       " ['uj-B'],\n",
       " ['a-B'],\n",
       " ['n-B'],\n",
       " ['x-B']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_grain_str('pos', tagScheme = 'BIOE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[112],\n",
       " [64],\n",
       " [91],\n",
       " [139],\n",
       " [64],\n",
       " [157],\n",
       " [64],\n",
       " [139],\n",
       " [16],\n",
       " [64],\n",
       " [169],\n",
       " [157],\n",
       " [16],\n",
       " [16],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [139],\n",
       " [4],\n",
       " [127],\n",
       " [64],\n",
       " [157],\n",
       " [70],\n",
       " [85],\n",
       " [64],\n",
       " [34],\n",
       " [127],\n",
       " [31],\n",
       " [157],\n",
       " [31],\n",
       " [64],\n",
       " [19],\n",
       " [64],\n",
       " [64],\n",
       " [64],\n",
       " [34],\n",
       " [127],\n",
       " [1],\n",
       " [64],\n",
       " [157]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info, leng_st, leng_tk, max_gr =  obj.get_grain_idx('pos', tagScheme='BIOE')\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['现', '代', '现-代'],\n",
       " ['逻', '辑', '逻-辑'],\n",
       " ['被'],\n",
       " ['分', '成', '分-成'],\n",
       " ['递', '归', '论', '递-归', '归-论', '递-归-论'],\n",
       " ['、'],\n",
       " ['模', '型', '模-型'],\n",
       " ['论'],\n",
       " ['和'],\n",
       " ['证', '明', '证-明'],\n",
       " ['论'],\n",
       " [','],\n",
       " ['且'],\n",
       " ['和'],\n",
       " ['理', '论', '理-论'],\n",
       " ['电', '脑', '电-脑'],\n",
       " ['科', '学', '科-学'],\n",
       " ['有', '著', '有-著'],\n",
       " ['密', '切', '密-切'],\n",
       " ['的'],\n",
       " ['关', '连', '性', '关-连', '连-性', '关-连-性'],\n",
       " [','],\n",
       " ['千', '禧', '年', '千-禧', '禧-年', '千-禧-年'],\n",
       " ['大', '奖', '大-奖'],\n",
       " ['难', '题', '难-题'],\n",
       " ['中'],\n",
       " ['的'],\n",
       " ['P'],\n",
       " ['/'],\n",
       " ['N', 'P', 'N-P'],\n",
       " ['问', '题', '问-题'],\n",
       " ['就', '是', '就-是'],\n",
       " ['理', '论', '理-论'],\n",
       " ['电', '脑', '电-脑'],\n",
       " ['科', '学', '科-学'],\n",
       " ['中'],\n",
       " ['的'],\n",
       " ['著', '名', '著-名'],\n",
       " ['问', '题', '问-题'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "channel = 'char'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 2}\n",
    "\n",
    "\n",
    "obj.get_grain_str(channel, **channel_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[63, 39, 497, 0, 0, 0],\n",
       " [210, 213, 216, 0, 0, 0],\n",
       " [34, 0, 0, 0, 0, 0],\n",
       " [59, 115, 1304, 0, 0, 0],\n",
       " [624, 17, 0, 0, 0, 0],\n",
       " [11, 0, 0, 0, 0, 0],\n",
       " [785, 1080, 1081, 0, 0, 0],\n",
       " [17, 0, 0, 0, 0, 0],\n",
       " [9, 0, 0, 0, 0, 0],\n",
       " [125, 111, 295, 0, 0, 0],\n",
       " [17, 0, 0, 0, 0, 0],\n",
       " [2, 0, 0, 0, 0, 0],\n",
       " [127, 0, 0, 0, 0, 0],\n",
       " [9, 0, 0, 0, 0, 0],\n",
       " [12, 17, 85, 0, 0, 0],\n",
       " [414, 382, 396, 0, 0, 0],\n",
       " [38, 1, 61, 0, 0, 0],\n",
       " [15, 56, 299, 0, 0, 0],\n",
       " [722, 650, 1325, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [114, 641, 41, 0, 0, 0],\n",
       " [2, 0, 0, 0, 0, 0],\n",
       " [598, 1176, 164, 1209, 1204, 1201],\n",
       " [83, 286, 1174, 0, 0, 0],\n",
       " [476, 30, 1188, 0, 0, 0],\n",
       " [16, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [764, 0, 0, 0, 0, 0],\n",
       " [1368, 0, 0, 0, 0, 0],\n",
       " [1238, 764, 1242, 0, 0, 0],\n",
       " [37, 30, 54, 0, 0, 0],\n",
       " [176, 7, 349, 0, 0, 0],\n",
       " [12, 17, 85, 0, 0, 0],\n",
       " [414, 382, 396, 0, 0, 0],\n",
       " [38, 1, 61, 0, 0, 0],\n",
       " [16, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [56, 243, 492, 0, 0, 0],\n",
       " [37, 30, 54, 0, 0, 0],\n",
       " [3, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "channel = 'char'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 2}\n",
    "\n",
    "\n",
    "info, leng_st, leng_tk, max_gr  = obj.get_grain_idx(channel, **channel_setting)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Newly Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token from Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Newly Created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
