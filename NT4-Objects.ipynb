{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Init NLPText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medpos/batch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.629 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "corpus/medpos/batch1\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "Total Num of All    Tokens 6483\n",
      "The Total Number of Tokens: 6483\n",
      "Counting the number unique Tokens...          \t 2019-06-10 15:34:55.894899\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-10 15:34:55.896917\n",
      "\t\tThe length of DTU is: 718 \t 2019-06-10 15:34:55.897080\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-10 15:34:55.897196\n",
      "\t\tThe idx of token is: 0 \t 2019-06-10 15:34:55.897260\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 718\n",
      "['</pad>', '</start>', '</end>', 'O', 'notsure-B', 'notsure-E', 'notsure-I', 'notsure-S', '临床属性-B', '临床属性-E', '临床属性-I', '临床属性-S', '事件-B', '事件-E', '事件-I', '事件-S', '人群-B', '人群-E', '人群-I', '人群-S', '介词-B', '介词-E', '介词-I', '介词-S', '代词-B', '代词-E', '代词-I', '代词-S', '体征与症状-B', '体征与症状-E', '体征与症状-I', '体征与症状-S', '单位-B', '单位-E', '单位-I', '单位-S', '发现-B', '发现-E', '发现-I', '发现-S', '因果-B', '因果-E', '因果-I', '因果-S', '地名机构名-B', '地名机构名-E', '地名机构名-I', '地名机构名-S', '定性-B', '定性-E', '定性-I', '定性-S', '或许-B', '或许-E', '或许-I', '或许-S', '数字-B', '数字-E', '数字-I', '数字-S', '数学符号-B', '数学符号-E', '数学符号-I', '数学符号-S', '无-B', '无-E', '无-I', '无-S', '时间单位-B', '时间单位-E', '时间单位-I', '时间单位-S', '时间概念-B', '时间概念-E', '时间概念-I', '时间概念-S', '有-B', '有-E', '有-I', '有-S', '标点-B', '标点-E', '标点-I', '标点-S', '检查仪器-B', '检查仪器-E', '检查仪器-I', '检查仪器-S', '检查行为-B', '检查行为-E', '检查行为-I', '检查行为-S', '检查项目-B', '检查项目-E', '检查项目-I', '检查项目-S', '治疗行为-B', '治疗行为-E', '治疗行为-I', '治疗行为-S', '治疗项目-B', '治疗项目-E', '治疗项目-I', '治疗项目-S', '物体-B', '物体-E', '物体-I', '物体-S', '生物-B', '生物-E', '生物-I', '生物-S', '疾病-B', '疾病-E', '疾病-I', '疾病-S', '病理功能-B', '病理功能-E', '病理功能-I', '病理功能-S', '空间概念-B', '空间概念-E', '空间概念-I', '空间概念-S', '药物-B', '药物-E', '药物-I', '药物-S', '虚词-B', '虚词-E', '虚词-I', '虚词-S', '诊断行为-B', '诊断行为-E', '诊断行为-I', '诊断行为-S', '身体功能-B', '身体功能-E', '身体功能-I', '身体功能-S', '身体物质-B', '身体物质-E', '身体物质-I', '身体物质-S', '身体部位-B', '身体部位-E', '身体部位-I', '身体部位-S', '身体部位畸形-B', '身体部位畸形-E', '身体部位畸形-I', '身体部位畸形-S', '连词-B', '连词-E', '连词-I', '连词-S']\n",
      "CORPUS\tit is Dumped into file: data/medpos/char/Token718/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/medpos/char/Token718/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medpos/char/Token718/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medpos/char/Token718/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 206\n",
      "TOKEN\tit is Dumped into file: data/medpos/char/Token718/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6483\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/medpos/char/Token718/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 718\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/token.tsv\n",
      "annoE-es\tis Dumped into file: data/medpos/char/Token718/GrainUnique/annoE-es.voc\n",
      "annoE-es\tthe length of it is   : 156\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/annoE-es.tsv\n",
      "pos-es\tis Dumped into file: data/medpos/char/Token718/GrainUnique/pos-es.voc\n",
      "pos-es\tthe length of it is   : 232\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/pos-es.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from nlptext.utils import reCutText2Sent\n",
    "\n",
    "\n",
    "########### MedPOS ###########\n",
    "CORPUSPath = 'corpus/medpos/'\n",
    "textType   = 'file'\n",
    "corpusFileIden = None\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "MaxTextIdx = 100\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Corpus Channel Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus()\n",
    "print(corpus.TokenNum_Dir)\n",
    "\n",
    "\n",
    "#if os.path.isdir(corpus.Channel_Dir):\n",
    "#    shutil.rmtree(corpus.Channel_Dir)\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    \n",
    "    # CTX_IND\n",
    "    'token':  {'use':         True,\n",
    "               'Max_Ngram':   1},\n",
    "    \n",
    "    'char':   {'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False},\n",
    "    \n",
    "    'basic':  {'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False},\n",
    "        \n",
    "    'medical':{'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False},\n",
    "    \n",
    "    'radical':{'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False},\n",
    "    \n",
    "    'subcomp':{'use':         True,\n",
    "               'Max_Ngram':   3,\n",
    "               'end_grain':   False},\n",
    "\n",
    "    'stroke': {'use':         True,\n",
    "               'Max_Ngram':   5,\n",
    "               'end_grain':   False},\n",
    "    \n",
    "    # CTX_DEP\n",
    "    'pos':    {'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False,\n",
    "               'tagScheme':   'BIO',\n",
    "               'tagSet':      None},\n",
    "    \n",
    "    # ANNO\n",
    "    'annoR':  {'use':         True,\n",
    "               'Max_Ngram':   1,\n",
    "               'end_grain':   False,\n",
    "               'tagScheme':   'BIO',\n",
    "               'tagSet':      None},\n",
    "    \n",
    "    \n",
    "    'annoE': {'use':         True,\n",
    "              'Max_Ngram':   1,\n",
    "              'end_grain':   False,\n",
    "              'tagScheme':   'BIO',\n",
    "              'tagSet':      None},\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.BUILD_LIST_GRAIN_UNIQUE_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE)\n",
    "\n",
    "\n",
    "# TODO: pretty print the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. getTensor Test\n",
    "\n",
    "## 3.1 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = corpus.Tokens[12]\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Ngram = 1\n",
    "channel = 'token'\n",
    "tagScheme = 'BIO'\n",
    "end_grain = False\n",
    "\n",
    "ListGrainUnique = tk.getListGrainUnique(channel, Max_Ngram = Max_Ngram, end_grain = end_grain, tagScheme = tagScheme)\n",
    "\n",
    "info = [ListGrainUnique.index(i) for i in tk.getChannelGrain(channel, \n",
    "                                                             Max_Ngram = Max_Ngram, \n",
    "                                                             end_grain = end_grain,\n",
    "                                                             tagScheme = tagScheme)]\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Ngram = 5\n",
    "channel = 'stroke'\n",
    "tagScheme = 'BIO'\n",
    "end_grain = False\n",
    "\n",
    "ListGrainUnique = tk.getListGrainUnique(channel, Max_Ngram = Max_Ngram, end_grain = end_grain, tagScheme = tagScheme)\n",
    "\n",
    "info = [ListGrainUnique.index(i) for i in tk.getChannelGrain(channel, \n",
    "                                                             Max_Ngram = Max_Ngram, \n",
    "                                                             end_grain = end_grain,\n",
    "                                                             tagScheme = tagScheme)]\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stroke')\n",
    "info, leng = tk.getGrainTensor('stroke', Max_Ngram=5)\n",
    "print(info)\n",
    "print(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('pos')\n",
    "info, leng = tk.getGrainTensor('pos', Max_Ngram=1)\n",
    "print(info)\n",
    "print(leng)\n",
    "\n",
    "print()\n",
    "print('annoE')\n",
    "info, leng = tk.getGrainTensor('annoE', Max_Ngram=1)\n",
    "print(info)\n",
    "print(leng)\n",
    "\n",
    "\n",
    "print()\n",
    "print('subcomp')\n",
    "info, leng = tk.getGrainTensor('subcomp', Max_Ngram=3)\n",
    "print(info)\n",
    "print(leng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = tk.Sentence\n",
    "print(st)\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "channel = 'annoE'\n",
    "\n",
    "info, leng, leng_s, maxGr, maxTk = st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng)\n",
    "print(leng_s)\n",
    "print(maxGr)\n",
    "print(maxTk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel = 'pos'\n",
    "\n",
    "info, leng, leng_s, maxGr, maxTk= st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng)\n",
    "print(leng_s)\n",
    "print(maxGr)\n",
    "print(maxTk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'stroke'\n",
    "\n",
    "info, leng, leng_s, maxGr, maxTk = st.getGrainTensor(channel, Max_Ngram=5, useStartEnd=True)\n",
    "print(info)\n",
    "print(leng)\n",
    "print(leng_s)\n",
    "print(maxGr)\n",
    "print(maxTk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "st = Sentence(sentence = '1.咳嗽查因:咳嗽变异性哮喘?迁延性支气管炎?')\n",
    "print(st.sentence)\n",
    "print(st)\n",
    "\n",
    "\n",
    "channel = 'stroke'\n",
    "\n",
    "info, leng, leng_s, maxGr, maxTk  = st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng)\n",
    "print(leng_s)\n",
    "print(maxGr)\n",
    "print(maxTk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'stroke'\n",
    "info, leng, leng_s, maxGr, maxTk  = st.getGrainTensor(channel, Max_Ngram=2, useStartEnd=True)\n",
    "print(info)\n",
    "print(leng)\n",
    "print(leng_s)\n",
    "print(maxGr)\n",
    "print(maxTk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
