{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant\n",
    "\n",
    "## 1.  `sentence.py::Sentence.getGrainTensor`\n",
    "\n",
    "## 2.  `token.py::Token.getGrainTensor`\n",
    "\n",
    "\n",
    "# Less Relevant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Init NLPText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medpos/batch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.585 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "corpus/medpos/batch1\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "Total Num of All    Tokens 6483\n",
      "The Total Number of Tokens: 6483\n",
      "Counting the number unique Tokens...          \t 2019-06-17 22:28:09.446826\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-17 22:28:09.449235\n",
      "\t\tThe length of DTU is: 718 \t 2019-06-17 22:28:09.449402\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-17 22:28:09.449489\n",
      "\t\tThe idx of token is: 0 \t 2019-06-17 22:28:09.449615\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 718\n",
      "['</pad>', '</start>', '</end>', 'O', 'notsure-B', 'notsure-E', 'notsure-I', 'notsure-S', '临床属性-B', '临床属性-E', '临床属性-I', '临床属性-S', '事件-B', '事件-E', '事件-I', '事件-S', '人群-B', '人群-E', '人群-I', '人群-S', '介词-B', '介词-E', '介词-I', '介词-S', '代词-B', '代词-E', '代词-I', '代词-S', '体征与症状-B', '体征与症状-E', '体征与症状-I', '体征与症状-S', '单位-B', '单位-E', '单位-I', '单位-S', '发现-B', '发现-E', '发现-I', '发现-S', '因果-B', '因果-E', '因果-I', '因果-S', '地名机构名-B', '地名机构名-E', '地名机构名-I', '地名机构名-S', '定性-B', '定性-E', '定性-I', '定性-S', '或许-B', '或许-E', '或许-I', '或许-S', '数字-B', '数字-E', '数字-I', '数字-S', '数学符号-B', '数学符号-E', '数学符号-I', '数学符号-S', '无-B', '无-E', '无-I', '无-S', '时间单位-B', '时间单位-E', '时间单位-I', '时间单位-S', '时间概念-B', '时间概念-E', '时间概念-I', '时间概念-S', '有-B', '有-E', '有-I', '有-S', '标点-B', '标点-E', '标点-I', '标点-S', '检查仪器-B', '检查仪器-E', '检查仪器-I', '检查仪器-S', '检查行为-B', '检查行为-E', '检查行为-I', '检查行为-S', '检查项目-B', '检查项目-E', '检查项目-I', '检查项目-S', '治疗行为-B', '治疗行为-E', '治疗行为-I', '治疗行为-S', '治疗项目-B', '治疗项目-E', '治疗项目-I', '治疗项目-S', '物体-B', '物体-E', '物体-I', '物体-S', '生物-B', '生物-E', '生物-I', '生物-S', '疾病-B', '疾病-E', '疾病-I', '疾病-S', '病理功能-B', '病理功能-E', '病理功能-I', '病理功能-S', '空间概念-B', '空间概念-E', '空间概念-I', '空间概念-S', '药物-B', '药物-E', '药物-I', '药物-S', '虚词-B', '虚词-E', '虚词-I', '虚词-S', '诊断行为-B', '诊断行为-E', '诊断行为-I', '诊断行为-S', '身体功能-B', '身体功能-E', '身体功能-I', '身体功能-S', '身体物质-B', '身体物质-E', '身体物质-I', '身体物质-S', '身体部位-B', '身体部位-E', '身体部位-I', '身体部位-S', '身体部位畸形-B', '身体部位畸形-E', '身体部位畸形-I', '身体部位畸形-S', '连词-B', '连词-E', '连词-I', '连词-S']\n",
      "CORPUS\tit is Dumped into file: data/medpos/char/Token718/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/medpos/char/Token718/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medpos/char/Token718/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medpos/char/Token718/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 206\n",
      "TOKEN\tit is Dumped into file: data/medpos/char/Token718/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6483\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/medpos/char/Token718/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 718\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/token.tsv\n",
      "annoE-es\tis Dumped into file: data/medpos/char/Token718/GrainUnique/annoE-es.voc\n",
      "annoE-es\tthe length of it is   : 156\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/annoE-es.tsv\n",
      "pos-es\tis Dumped into file: data/medpos/char/Token718/GrainUnique/pos-es.voc\n",
      "pos-es\tthe length of it is   : 232\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/pos-es.tsv\n",
      "****************************************\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/char.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/char.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/char.lkp\n",
      "Deal with the Channel: basic\n",
      "Current Channel is        \t basic\n",
      "Current Channel Max_Ngram \t 2\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: basic2\n",
      "For channel: | basic | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/basic2.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/basic2.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/basic2.lkp\n",
      "Deal with the Channel: medical\n",
      "Current Channel is        \t medical\n",
      "Current Channel Max_Ngram \t 2\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: medical2\n",
      "For channel: | medical | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/medical2.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/medical2.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/medical2.lkp\n",
      "Deal with the Channel: radical\n",
      "Current Channel is        \t radical\n",
      "Current Channel Max_Ngram \t 2\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: radical2\n",
      "For channel: | radical | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/radical2.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/radical2.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/radical2.lkp\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: subcomp3e\n",
      "For channel: | subcomp | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3e.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3e.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3e.lkp\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 5\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke5e\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5e.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5e.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5e.lkp\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from nlptext.utils import reCutText2Sent\n",
    "\n",
    "\n",
    "########### MedPOS ###########\n",
    "CORPUSPath = 'corpus/medpos/'\n",
    "textType   = 'file'\n",
    "corpusFileIden = None\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.UMLSTag'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "MaxTextIdx = 100\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'basic':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'medical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'radical': {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 5, 'end_grain': True},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': False,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    # ANNO\n",
    "    'annoR':   {'use': False,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "    'annoE':   {'use': False,'Max_Ngram': 1, 'end_grain': False, 'tagScheme':   'BIO',},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE=CHANNEL_SETTINGS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. getTensor Test\n",
    "\n",
    "## 3.1 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tk 高 >"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "tk = Token(12)\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_Ngram = 1\n",
    "channel = 'token'\n",
    "tagScheme = 'BIO'\n",
    "end_grain = False\n",
    "\n",
    "GrainUnique = tk.getGrainUnique(channel, Max_Ngram = Max_Ngram, end_grain = end_grain, tagScheme = tagScheme)\n",
    "\n",
    "info = [GrainUnique[1][i] for i in tk.getChannelGrain(channel, \n",
    "                                                             Max_Ngram = Max_Ngram, \n",
    "                                                             end_grain = end_grain,\n",
    "                                                             tagScheme = tagScheme)]\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke5\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke5.lkp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 9,\n",
       " 22,\n",
       " 17,\n",
       " 9,\n",
       " 22,\n",
       " 17,\n",
       " 22,\n",
       " 17,\n",
       " 9,\n",
       " 71,\n",
       " 20,\n",
       " 46,\n",
       " 79,\n",
       " 20,\n",
       " 46,\n",
       " 54,\n",
       " 46,\n",
       " 79,\n",
       " 295,\n",
       " 59,\n",
       " 66,\n",
       " 137,\n",
       " 59,\n",
       " 47,\n",
       " 140,\n",
       " 66,\n",
       " 296,\n",
       " 60,\n",
       " 126,\n",
       " 138,\n",
       " 121,\n",
       " 293,\n",
       " 141,\n",
       " 297,\n",
       " 291,\n",
       " 127,\n",
       " 139,\n",
       " 292,\n",
       " 294]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_Ngram = 5\n",
    "channel = 'stroke'\n",
    "tagScheme = 'BIO'\n",
    "end_grain = False\n",
    "\n",
    "\n",
    "GrainUnique = tk.getGrainUnique(channel, Max_Ngram = Max_Ngram, end_grain = end_grain, tagScheme = tagScheme)\n",
    "\n",
    "info = [GrainUnique[1][i] for i in tk.getChannelGrain(channel,  Max_Ngram = Max_Ngram, \n",
    "                                                      end_grain = end_grain, tagScheme = tagScheme)]\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroke\n",
      "[32, 9, 22, 17, 9, 22, 17, 22, 17, 9, 71, 20, 46, 79, 20, 46, 54, 46, 79, 295, 59, 66, 137, 59, 47, 140, 66, 296, 60, 126, 138, 121, 293, 141, 297, 291, 127, 139, 292, 294]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print('stroke')\n",
    "info, leng = tk.getGrainTensor('stroke', Max_Ngram=5)\n",
    "print(info)\n",
    "print(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pos\n",
      "\t\tBuild GrainUnique for channel: pos\n",
      "pos 1 False BIO\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/pos.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/pos.tsv\n",
      "[4]\n",
      "1\n",
      "\n",
      "annoE\n",
      "\t\tBuild GrainUnique for channel: annoE\n",
      "annoE 1 False BIO\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/annoE.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/annoE.tsv\n",
      "[26]\n",
      "1\n",
      "\n",
      "subcomp\n",
      "Get LookUp Table for Channel: subcomp3\n",
      "\tNo LookUp Table is found for channel:   subcomp3 Turn to the orignal way... (tk.getGrainTensor)\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: subcomp3\n",
      "For channel: | subcomp | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/subcomp3.lkp\n",
      "[163, 35, 54, 35, 164, 160, 162, 165, 161]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('pos')\n",
    "info, leng = tk.getGrainTensor('pos', Max_Ngram=1)\n",
    "print(info)\n",
    "print(leng)\n",
    "\n",
    "print()\n",
    "print('annoE')\n",
    "info, leng = tk.getGrainTensor('annoE', Max_Ngram=1)\n",
    "print(info)\n",
    "print(leng)\n",
    "\n",
    "print()\n",
    "print('subcomp')\n",
    "info, leng = tk.getGrainTensor('subcomp', Max_Ngram=3)\n",
    "print(info)\n",
    "print(leng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<st 0 (tokenNum: 14) >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'急性脑梗死,高血压病3级高危'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = tk.Sentence\n",
    "print(st)\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26], [26], [74], [60], [61], [42], [26], [72], [6], [58], [30], [18], [26], [26]]\n",
      "14\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "channel = 'annoE'\n",
    "\n",
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng_st)\n",
    "print(leng_tk)\n",
    "print(max_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46], [47], [46], [47], [47], [108], [46], [47], [47], [47], [40], [66], [4], [46]]\n",
      "14\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "channel = 'pos'\n",
    "\n",
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng_st)\n",
    "print(leng_tk)\n",
    "print(max_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [[15, 17, 17, 9, 9, 32, 17, 32, 32, 16, 422, 79, 10, 62, 81, 84, 37, 485, 457, 80, 410, 413, 82, 85, 486, 458, 420, 411, 483, 83, 487, 488, 421, 482, 484], [32, 32, 22, 15, 9, 9, 22, 9, 37, 33, 24, 28, 10, 20, 23, 38, 34, 25, 29, 18, 21, 39, 35, 26, 30, 19, 40, 36, 27, 31], [15, 17, 9, 9, 32, 9, 15, 32, 17, 22, 16, 79, 10, 62, 71, 13, 97, 81, 54, 229, 80, 410, 796, 114, 104, 128, 134, 230, 420, 793, 797, 115, 248, 129, 416, 800, 794, 798, 799, 795], [9, 22, 15, 32, 9, 22, 17, 9, 9, 15, 32, 20, 24, 97, 71, 20, 46, 79, 10, 13, 97, 86, 92, 98, 295, 59, 66, 80, 11, 104, 87, 93, 646, 296, 60, 67, 193, 370, 88, 644, 917, 297, 61, 228, 918], [9, 15, 17, 32, 15, 17, 13, 16, 84, 75, 16, 14, 157, 815, 600, 227, 1161, 816, 1207, 1162], [4], [32, 9, 22, 17, 9, 22, 17, 22, 17, 9, 71, 20, 46, 79, 20, 46, 54, 46, 79, 295, 59, 66, 137, 59, 47, 140, 66, 296, 60, 126, 138, 121, 293, 141, 297, 291, 127, 139, 292, 294], [15, 22, 17, 22, 22, 9, 50, 46, 54, 44, 23, 51, 47, 55, 45, 52, 48, 56, 53, 49], [9, 15, 9, 22, 9, 32, 13, 28, 20, 23, 62, 178, 68, 21, 181, 179, 182, 177, 180, 183], [32, 9, 15, 32, 9, 9, 22, 17, 15, 32, 71, 13, 97, 71, 10, 20, 46, 117, 97, 114, 104, 98, 111, 18, 59, 107, 118, 115, 105, 109, 112, 100, 102, 108, 116, 106, 110, 113, 101, 103], [43], [17, 17, 9, 15, 17, 32, 422, 79, 13, 16, 84, 457, 243, 14, 157, 732, 299, 227, 733, 300], [32, 9, 22, 17, 9, 22, 17, 22, 17, 9, 71, 20, 46, 79, 20, 46, 54, 46, 79, 295, 59, 66, 137, 59, 47, 140, 66, 296, 60, 126, 138, 121, 293, 141, 297, 291, 127, 139, 292, 294], [15, 17, 9, 15, 17, 17, 16, 79, 13, 16, 422, 229, 243, 14, 485, 919, 299, 504, 920, 921], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "3\n",
      "[1, 35, 30, 40, 45, 20, 1, 40, 20, 20, 40, 1, 20, 40, 20, 1]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "channel = 'stroke'\n",
    "\n",
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(channel, Max_Ngram=5, useStartEnd=True)\n",
    "print(info)\n",
    "print(leng_st)\n",
    "print(leng_tk)\n",
    "print(max_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence from other position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.咳嗽查因:咳嗽变异性哮喘?迁延性支气管炎?\n",
      "<st New (tokenNum: 23) >\n",
      "Get LookUp Table for Channel: stroke\n",
      "\tNo LookUp Table is found for channel:   stroke Turn to the orignal way... (tk.getGrainTensor)\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke.lkp\n",
      "[[6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 11, 9, 13, 9, 11, 10, 10, 13, 0, 0, 0, 0, 0], [12, 11, 9, 9, 12, 11, 9, 12, 10, 13, 10, 11, 10, 13], [9, 12, 10, 13, 12, 11, 9, 9, 9, 0, 0, 0, 0, 0], [12, 11, 9, 10, 13, 9, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 11, 9, 13, 9, 11, 10, 10, 13, 0, 0, 0, 0, 0], [12, 11, 9, 9, 12, 11, 9, 12, 10, 13, 10, 11, 10, 13], [13, 9, 12, 12, 10, 13, 11, 13, 0, 0, 0, 0, 0, 0], [11, 9, 11, 9, 10, 12, 0, 0, 0, 0, 0, 0, 0, 0], [13, 13, 12, 10, 9, 9, 12, 9, 0, 0, 0, 0, 0, 0], [12, 11, 9, 9, 12, 9, 10, 11, 12, 9, 0, 0, 0, 0], [12, 11, 9, 12, 11, 12, 9, 10, 12, 11, 12, 12, 0, 0], [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 9, 12, 13, 11, 13, 0, 0, 0, 0, 0, 0, 0, 0], [10, 12, 9, 11, 11, 13, 0, 0, 0, 0, 0, 0, 0, 0], [13, 13, 12, 10, 9, 9, 12, 9, 0, 0, 0, 0, 0, 0], [9, 12, 11, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 9, 9, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 9, 13, 10, 9, 13, 13, 13, 11, 12, 11, 9, 11, 9], [13, 10, 10, 13, 13, 10, 10, 13, 0, 0, 0, 0, 0, 0], [34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "23\n",
      "[1, 1, 9, 14, 9, 6, 1, 9, 14, 8, 6, 8, 10, 12, 1, 6, 6, 8, 4, 4, 14, 8, 1]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "st = Sentence(sentence = '1.咳嗽查因:咳嗽变异性哮喘?迁延性支气管炎?')\n",
    "print(st.sentence)\n",
    "print(st)\n",
    "\n",
    "\n",
    "channel = 'stroke'\n",
    "\n",
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(channel)\n",
    "print(info)\n",
    "print(leng_st)\n",
    "print(leng_tk)\n",
    "print(max_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get LookUp Table for Channel: stroke2\n",
      "\tNo LookUp Table is found for channel:   stroke2 Turn to the orignal way... (tk.getGrainTensor)\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke2\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke2.voc\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke2.tsv\n",
      "\t\tWrite to: data/medpos/char/Token718/GrainUnique/stroke2.lkp\n",
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [[6], [24], [16, 14, 9, 20, 9, 14, 12, 12, 20, 27, 35, 32, 33, 45, 40, 53, 38], [16, 14, 9, 9, 16, 14, 9, 16, 12, 20, 12, 14, 12, 20, 27, 35, 10, 15, 27, 35, 15, 18, 38, 34, 13, 40, 38], [9, 16, 12, 20, 16, 14, 9, 9, 9, 15, 18, 38, 21, 27, 35, 10, 10], [16, 14, 9, 12, 20, 9, 27, 35, 11, 38, 33], [7], [16, 14, 9, 20, 9, 14, 12, 12, 20, 27, 35, 32, 33, 45, 40, 53, 38], [16, 14, 9, 9, 16, 14, 9, 16, 12, 20, 12, 14, 12, 20, 27, 35, 10, 15, 27, 35, 15, 18, 38, 34, 13, 40, 38], [20, 9, 16, 16, 12, 20, 14, 20, 33, 15, 26, 18, 38, 36, 37], [14, 9, 14, 9, 12, 16, 35, 45, 35, 11, 28], [20, 20, 16, 12, 9, 9, 16, 9, 22, 21, 18, 19, 10, 15, 17], [16, 14, 9, 9, 16, 9, 12, 14, 16, 9, 27, 35, 10, 15, 17, 11, 13, 29, 17], [16, 14, 9, 16, 14, 16, 9, 12, 16, 14, 16, 16, 27, 35, 15, 27, 29, 17, 11, 28, 27, 29, 26], [59], [12, 9, 16, 20, 14, 20, 19, 15, 41, 36, 37], [12, 16, 9, 14, 14, 20, 28, 17, 45, 55, 37], [20, 20, 16, 12, 9, 9, 16, 9, 22, 21, 18, 19, 10, 15, 17], [9, 16, 14, 20, 15, 27, 37], [12, 9, 9, 14, 19, 10, 45], [12, 9, 20, 12, 9, 20, 20, 20, 14, 16, 14, 9, 14, 9, 19, 32, 34, 19, 32, 22, 22, 36, 29, 27, 35, 45, 35], [20, 12, 12, 20, 20, 12, 12, 20, 34, 53, 38, 22, 34, 53, 38], [59], 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "3\n",
      "[1, 1, 1, 17, 27, 17, 11, 1, 17, 27, 15, 11, 15, 19, 23, 1, 11, 11, 15, 7, 7, 27, 15, 1, 1]\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "channel = 'stroke'\n",
    "info, leng_st, leng_tk, max_gr = st.getGrainTensor(channel, Max_Ngram=2, useStartEnd=True)\n",
    "print(info)\n",
    "print(leng_st)\n",
    "print(leng_tk)\n",
    "print(max_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
