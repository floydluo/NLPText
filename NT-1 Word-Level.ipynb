{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': <function nlptext.utils.pyramid.textFileReader(folderPath, fileNames, anno=False, sep='\\t', notZeroIndex=1, notRightOpen=0, **kwargs)>,\n",
       " 'line': <function nlptext.utils.pyramid.textLineReader(folderPath, fileNames, anno=False, **kwargs)>,\n",
       " 'block': <function nlptext.utils.pyramid.textBlockReader(folderPath, fileNames, anno=True, **kwargs)>,\n",
       " 'element': <function nlptext.utils.pyramid.textElementReader(folderPath, fileNames, anno=False, **kwargs)>,\n",
       " 'json': <function nlptext.utils.pyramid.textJsonReader(folderPath, fileNames, anno='.json', **kwargs)>,\n",
       " 'word': <function nlptext.utils.pyramid.textWordReader(folderPath, fileNames, anno='.xlsx', **kwargs)>}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.pyramid import CorpusFoldersReader, FolderTextsReaders\n",
    "\n",
    "FolderTextsReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.pyramid import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus/annotated/': ['ner-dataset.xlsx']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def CorpusFoldersReader(CORPUSPath, iden = None):\n",
    "    # file is the priority\n",
    "    if iden:\n",
    "        corpusFiles = [i for i in os.listdir(CORPUSPath) if iden in i]\n",
    "        return {os.path.join(CORPUSPath, fd): '' for fd in corpusFiles}, 'File'\n",
    "    else:\n",
    "        results = [x for x in os.walk(CORPUSPath) if x[2]]\n",
    "        return {i[0]: i[2] for i in results},                            'Dir'\n",
    "    \n",
    "    \n",
    "CORPUSPath = 'corpus/annotated/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.', ' ', 'Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined', 'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans', 'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop', 'the', 'Bombings', '.', '\"', ' ', 'They', 'marched', 'from', 'the', 'Houses', 'of', 'Parliament', 'to', 'a', 'rally', 'in', 'Hyde', 'Park', '.']\n",
      "[['Thousands', 0, 'O'], ['of', 1, 'O'], ['demonstrators', 2, 'O'], ['have', 3, 'O'], ['marched', 4, 'O'], ['through', 5, 'O'], ['London', 6, 'B-geo'], ['to', 7, 'O'], ['protest', 8, 'O'], ['the', 9, 'O'], ['war', 10, 'O'], ['in', 11, 'O'], ['Iraq', 12, 'B-geo'], ['and', 13, 'O'], ['demand', 14, 'O'], ['the', 15, 'O'], ['withdrawal', 16, 'O'], ['of', 17, 'O'], ['British', 18, 'B-gpe'], ['troops', 19, 'O'], ['from', 20, 'O'], ['that', 21, 'O'], ['country', 22, 'O'], ['.', 23, 'O'], [' ', -1, 'T2S'], ['Families', 24, 'O'], ['of', 25, 'O'], ['soldiers', 26, 'O'], ['killed', 27, 'O'], ['in', 28, 'O'], ['the', 29, 'O'], ['conflict', 30, 'O'], ['joined', 31, 'O'], ['the', 32, 'O'], ['protesters', 33, 'O'], ['who', 34, 'O'], ['carried', 35, 'O'], ['banners', 36, 'O'], ['with', 37, 'O'], ['such', 38, 'O'], ['slogans', 39, 'O'], ['as', 40, 'O'], ['\"', 41, 'O'], ['Bush', 42, 'B-per'], ['Number', 43, 'O'], ['One', 44, 'O'], ['Terrorist', 45, 'O'], ['\"', 46, 'O'], ['and', 47, 'O'], ['\"', 48, 'O'], ['Stop', 49, 'O'], ['the', 50, 'O'], ['Bombings', 51, 'O'], ['.', 52, 'O'], ['\"', 53, 'O'], [' ', -1, 'T2S'], ['They', 54, 'O'], ['marched', 55, 'O'], ['from', 56, 'O'], ['the', 57, 'O'], ['Houses', 58, 'O'], ['of', 59, 'O'], ['Parliament', 60, 'O'], ['to', 61, 'O'], ['a', 62, 'O'], ['rally', 63, 'O'], ['in', 64, 'O'], ['Hyde', 65, 'B-geo'], ['Park', 66, 'I-geo'], ['.', 67, 'O']]\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "CORPUSPath = 'corpus/annotated/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = '.xlsx')[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'word'\n",
    "\n",
    "TOKENLevel = 'word'\n",
    "anno = '.xlsx'\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "\n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/annotated/ner-dataset.xlsx\n",
      "25\n",
      "56\n",
      "Total Num of All    Tokens 68\n",
      "The Total Number of Tokens: 68\n",
      "Counting the number unique Tokens...          \t 2019-06-24 10:35:04.466569\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-24 10:35:04.467406\n",
      "\t\tThe length of DTU is: 53 \t 2019-06-24 10:35:04.467406\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-24 10:35:04.468403\n",
      "\t\tThe idx of token is: 0 \t 2019-06-24 10:35:04.468403\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 53\n",
      "CORPUS\tit is Dumped into file: data\\annotated\\word\\Token53\\Pyramid\\CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data\\annotated\\word\\Token53\\Pyramid\\FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data\\annotated\\word\\Token53\\Pyramid\\TEXT.p\n",
      "TEXT\tthe length of it is   : 1\n",
      "SENT\tit is Dumped into file: data\\annotated\\word\\Token53\\Pyramid\\SENT.p\n",
      "SENT\tthe length of it is   : 3\n",
      "TOKEN\tit is Dumped into file: data\\annotated\\word\\Token53\\Pyramid\\TOKEN.p\n",
      "TOKEN\tthe length of it is   : 68\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data\\annotated\\word\\Token53\\GrainUnique\\token.voc\n",
      "token\tthe length of it is   : 53\n",
      "\t\tWrite to: data\\annotated\\word\\Token53\\GrainUnique\\token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "########### ANNOTATED ###########\n",
    "CORPUSPath = 'corpus/annotated/'\n",
    "corpusFileIden = '.xlsx'\n",
    "textType   = 'word'\n",
    "Text2SentMethod  = 'token'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'word'\n",
    "anno = '.xlsx'\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLPBA 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus/NLPBA 2004/': ['evalIOB2.pl',\n",
       "  'Genia4ERtask1.iob2',\n",
       "  'Genia4ERtask2.iob2',\n",
       "  'LICENSE',\n",
       "  'README.txt',\n",
       "  'sampletest1.iob2',\n",
       "  'sampletest1.raw',\n",
       "  'sampletest2.iob2',\n",
       "  'sampletest2.raw']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUSPath = 'corpus/NLPBA 2004/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = None)[0]\n",
    "CorpusFolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CORPUSPath = 'corpus/NLPBA 2004/'\n",
    "CorpusFolders = CorpusFoldersReader(CORPUSPath, iden = '.iob2')[0]\n",
    "\n",
    "# print(CorpusFolders)\n",
    "\n",
    "folderPath =  list(CorpusFolders.keys())[0]\n",
    "fileNames  = CorpusFolders[folderPath]\n",
    "\n",
    "textType = 'word'\n",
    "\n",
    "TOKENLevel = 'word'\n",
    "anno = '.iob2'\n",
    "\n",
    "FolderTexts = FolderTextsReaders[textType](folderPath, fileNames, anno)\n",
    "\n",
    "strText_SSET_O_A = list(FolderTexts)[0]\n",
    "\n",
    "strText, SSETText, origTextName, annoTextName = strText_SSET_O_A\n",
    "print(strText)\n",
    "print(SSETText)\n",
    "print(origTextName)\n",
    "print(annoTextName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NLPBA 2004/Genia4ERtask1.iob2\n",
      "corpus/NLPBA 2004/Genia4ERtask2.iob2\n",
      "corpus/NLPBA 2004/sampletest1.iob2\n",
      "corpus/NLPBA 2004/sampletest2.iob2\n",
      "Total Num of All    Tokens 992711\n",
      "The Total Number of Tokens: 992711\n",
      "Counting the number unique Tokens...          \t 2019-06-24 10:49:43.740344\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-24 10:49:44.145380\n",
      "\t\tThe length of DTU is: 24176 \t 2019-06-24 10:49:44.167551\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-24 10:49:44.167551\n",
      "\t\tThe idx of token is: 0 \t 2019-06-24 10:49:44.167551\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 24176\n",
      "CORPUS\tit is Dumped into file: data\\NLPBA 2004\\word\\Token24176\\Pyramid\\CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data\\NLPBA 2004\\word\\Token24176\\Pyramid\\FOLDER.p\n",
      "FOLDER\tthe length of it is   : 4\n",
      "TEXT\tit is Dumped into file: data\\NLPBA 2004\\word\\Token24176\\Pyramid\\TEXT.p\n",
      "TEXT\tthe length of it is   : 4\n",
      "SENT\tit is Dumped into file: data\\NLPBA 2004\\word\\Token24176\\Pyramid\\SENT.p\n",
      "SENT\tthe length of it is   : 39316\n",
      "TOKEN\tit is Dumped into file: data\\NLPBA 2004\\word\\Token24176\\Pyramid\\TOKEN.p\n",
      "TOKEN\tthe length of it is   : 992711\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data\\NLPBA 2004\\word\\Token24176\\GrainUnique\\token.voc\n",
      "token\tthe length of it is   : 24176\n",
      "\t\tWrite to: data\\NLPBA 2004\\word\\Token24176\\GrainUnique\\token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "########### ANNOTATED ###########\n",
    "CORPUSPath = 'corpus/NLPBA 2004/'\n",
    "corpusFileIden = '.iob2'\n",
    "textType   = 'word'\n",
    "Text2SentMethod  = 'token'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'word'\n",
    "anno = '.iob2'\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
