{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Fields Description\n",
    "\n",
    "TODO: about hyper fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Information\n",
    "\n",
    "\n",
    "Token is made up by Chars.\n",
    "\n",
    "Word-Token is made up by several Chars, while Char-Token has only one Char.\n",
    "\n",
    "\n",
    "Use Word-Token as an example:\n",
    "\n",
    "\n",
    "北京 --> 北 京 \n",
    "\n",
    "Beijing --> B e i j i n g\n",
    "\n",
    "\n",
    "In this section, we want to derive more information for char only. (Not the whole token)\n",
    "\n",
    "## Char Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京']\n",
      "['j']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def charGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    info = [char]\n",
    "    if end_grain:\n",
    "        info = info + ['ch0']\n",
    "    return info\n",
    "\n",
    "\n",
    "print(charGrainChar('京'))\n",
    "print(charGrainChar('j'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京', 'ch0']\n",
      "['j', 'ch0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(charGrainChar('京', end_grain= True))\n",
    "print(charGrainChar('j', end_grain= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Char's SubComp\n",
    "\n",
    "For English char (letter), only return itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c67', 'c119', 'c159']\n",
      "['j']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('nlptext/sources/CharSubComp.p', 'rb') as handle:\n",
    "    CharSubCompInfos = pickle.load(handle)\n",
    "\n",
    "def subcompGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    if char in CharSubCompInfos:\n",
    "        info = CharSubCompInfos[char]\n",
    "        if info:\n",
    "            info = ['c' + i for i in info ]\n",
    "        else:\n",
    "            info = ['c' + char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['c0']\n",
    "    return info\n",
    "\n",
    "\n",
    "print(subcompGrainChar('京'))\n",
    "print(subcompGrainChar('j'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c67', 'c119', 'c159', 'c0']\n",
      "['j', 'c0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(subcompGrainChar('京', end_grain= True))\n",
    "print(subcompGrainChar('j', end_grain= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap as a Token Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('nlptext/sources/CharSubComp.p', 'rb') as handle:\n",
    "    CharSubCompInfos = pickle.load(handle)\n",
    "\n",
    "def subcompGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    if char in CharSubCompInfos:\n",
    "        info = CharSubCompInfos[char]\n",
    "        if info:\n",
    "            info = ['c' + i for i in info ]\n",
    "        else:\n",
    "            info = ['c' + char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['c0']\n",
    "    return info\n",
    "\n",
    "def subcompGrainToken(token, end_grain = False):\n",
    "    info = sum([subcompGrainChar(char, end_grain) for char in token], [])\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c117', 'c24', 'c0', 'c67', 'c119', 'c159', 'c0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "end_grain = True\n",
    "\n",
    "subcompGrainToken(token, end_grain = end_grain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'e', 'i', 'j', 'i', 'n', 'g']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijing'\n",
    "end_grain = False\n",
    "\n",
    "subcompGrainToken(token,  end_grain = end_grain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bei', 'jing']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyphen\n",
    "\n",
    "\n",
    "def syllableGrainToken(token, end_grain = False):\n",
    "    \n",
    "    # pyphen.LANGUAGES\n",
    "    dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "    # token = 'tomorrow'\n",
    "    return dic.inserted(token).split('-')\n",
    "\n",
    "\n",
    "token = 'Beijing'\n",
    "syllableGrainToken(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('nlptext/sources/WordPhoneme.p', 'rb') as handle:\n",
    "    WordPhenomeInfo = pickle.load(handle)\n",
    "\n",
    "def phonemeGrainToken(token, end_grain = False): \n",
    "    try:\n",
    "        phonemes = WordPhenomeInfo[token.lower()]\n",
    "    except:\n",
    "        phonemes = ['']\n",
    "    return phonemes\n",
    "\n",
    "token = 'Beijing'\n",
    "phonemeGrainToken(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Token-Based Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char': <function nlptext.utils.channel.charGrainToken(token, end_grain=False)>,\n",
       " 'basic': <function nlptext.utils.channel.basicGrainToken(token, end_grain=False)>,\n",
       " 'medical': <function nlptext.utils.channel.medicalGrainToken(token, end_grain=False)>,\n",
       " 'radical': <function nlptext.utils.channel.radicalGrainToken(token, end_grain=False)>,\n",
       " 'subcomp': <function nlptext.utils.channel.subcompGrainToken(token, end_grain=False)>,\n",
       " 'stroke': <function nlptext.utils.channel.strokeGrainToken(token, end_grain=False)>,\n",
       " 'pinyin': <function nlptext.utils.channel.pinyinGrainToken(token, end_grain=False)>,\n",
       " 'syllable': <function nlptext.utils.channel.syllableGrainToken(token, end_grain=False)>,\n",
       " 'phoneme': <function nlptext.utils.channel.phonemeGrainToken(token, end_grain=False)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import Channel_Ind_Methods\n",
    "\n",
    "Channel_Ind_Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = Channel_Ind_Methods['phoneme']\n",
    "method('Beijing', end_grain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGrainNgrams(subword_infos, n):\n",
    "    if n == 1:\n",
    "        return [i for i in subword_infos]\n",
    "    if n > len(subword_infos):\n",
    "        # How to deal this when the length is not so long\n",
    "        # Condition: where n is larger than the infos\n",
    "        return [] \n",
    "    l = [subword_infos[i:n+i] for i in range(len(subword_infos) - n + 1)]\n",
    "    l = ['-'.join(i) for i in l]\n",
    "    return l\n",
    "\n",
    "def grainToken(token, grainTokenFunction, Ngram = 1,Max_Ngram = None, end_grain = True):\n",
    "    infos =  grainTokenFunction(token, end_grain = end_grain) \n",
    "    if not Max_Ngram:\n",
    "        return getGrainNgrams(infos, Ngram)\n",
    "    else:\n",
    "        return sum([getGrainNgrams(infos, idx+1) for idx in range(Max_Ngram)], [])\n",
    "\n",
    "def getChannelGrain4Token(token, channel, Ngram = 1, Max_Ngram = None,  end_grain = False):\n",
    "    if channel == 'token':\n",
    "        return [token]\n",
    "    elif channel in Channel_Ind_Methods:\n",
    "        return grainToken(token, Channel_Ind_Methods[channel], Ngram = Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    else:\n",
    "        print('The Channel \"', channel, '\" is not available currently!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c67-c119', 'c119-c159']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "getChannelGrain4Token(token, channel, Ngram = 2, Max_Ngram = None,  end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bei', 'jng', 'Bei-jng']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijng'\n",
    "channel = 'syllable'\n",
    "getChannelGrain4Token(token, channel, Ngram =1, Max_Ngram = 2,  end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijing'\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Ngram =1, Max_Ngram = None, end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getChannelGrain4Token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelGrain4Token\n",
    "\n",
    "token = 'Beijing'\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Ngram =1, Max_Ngram = None, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c117-c24', 'c24-c67', 'c67-c119', 'c119-c159']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "getChannelGrain4Token(token, channel, Ngram = 2, Max_Ngram = None,  end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getChannelGrain4Sent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京', '是', '中国', '的', '首都']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelGrain4Sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '是', '中国', '的', '首都']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['c117-c24', 'c24-c67', 'c67-c119', 'c119-c159'],\n",
       " ['c209-c1', 'c1-c207'],\n",
       " ['c214-c120', 'c120-c173', 'c173-c6'],\n",
       " ['c331-c64', 'c64-c6'],\n",
       " ['c140-c392', 'c392-c180', 'c180-c209', 'c209-c166']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '北京 是 中国 的 首都'.split(' ')\n",
    "print(sent)\n",
    "channel = 'subcomp'\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, Ngram = 2, Max_Ngram = None, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beijing', 'is', 'the', 'capital', 'of', 'China']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['B', 'EY2', 'ZH', 'IH1', 'NG'],\n",
       " ['IH1', 'Z'],\n",
       " ['DH', 'AH0'],\n",
       " ['K', 'AE1', 'P', 'AH0', 'T', 'AH0', 'L'],\n",
       " ['AH1', 'V'],\n",
       " ['CH', 'AY1', 'N', 'AH0']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Beijing is the capital of China'.split(' ')\n",
    "print(sent)\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, Ngram = 1, Max_Ngram = None, end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get  Vocab and Freq of Sub Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.602 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 41843\n",
      "Total Num of Unique Tokens 5965\n",
      "CORPUS\tit is Dumped into file: data/wiki/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 41843\n",
      "**************************************** \n",
      "\n",
      "pos-es\tis Dumped into file: data/wiki/word/Vocab/pos-es.voc\n",
      "pos-es\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki/word/Vocab/pos-es.tsv\n",
      "token\tis Dumped into file: data/wiki/word/Vocab/token.voc\n",
      "token\tthe length of it is   : 5965\n",
      "\t\tWrite to: data/wiki/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "\n",
    "textType   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "# sentence to tokens\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "use_hyper = True\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, use_hyper = use_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTU, DTU = BasicObject.TokenVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3218, 2356, 1415, ...,    1,    1,    1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2freq = BasicObject.idx2freq\n",
    "\n",
    "idx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_freq(idx2freq, max_vocab_token_num = None, min_token_freq = 1):\n",
    "    if min_token_freq:\n",
    "        max_vocab_token_num = len(idx2freq[idx2freq >= min_token_freq])\n",
    "        \n",
    "    elif max_vocab_token_num:\n",
    "        if max_vocab_token_num > len(idx2freq):\n",
    "            max_vocab_token_num,  min_token_freq = len(idx2freq), 1\n",
    "        else:\n",
    "            min_token_freq = max_vocab_token_num[max_vocab_token_num]\n",
    "    else:\n",
    "        raise('Error in max_vocab_token_num and min_token_freq')\n",
    "        \n",
    "    print('max_vocab_token_num  is:', max_vocab_token_num)\n",
    "    print('min_token_freq       is:', min_token_freq)\n",
    "    print('Corpus coverage rate is:', np.sum(idx2freq[:max_vocab_token_num]) / np.sum(idx2freq))\n",
    "    print('Token  coverage rate is:', max_vocab_token_num / len(idx2freq))\n",
    "    return max_vocab_token_num,  min_token_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5965"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################LTU_LGU-LT\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "def get_GU_or_LKP(TokenVocab, tkidx2freq, channel= 'char', Max_Ngram = 1, end_grain = False, \n",
    "                  max_vocab_token_num = None, min_token_freq = 1, min_grain_freq = 1):\n",
    "\n",
    "    # ListGrainUnique = []\n",
    "    LTU, DTU = TokenVocab\n",
    "    max_vocab_token_num, min_token_freq = get_num_freq(tkidx2freq, max_vocab_token_num = max_vocab_token_num, \n",
    "                                                       min_token_freq = min_token_freq)\n",
    "    LTU = LTU[:max_vocab_token_num]\n",
    "    \n",
    "    # the containers to store our results\n",
    "    oldLGU = []\n",
    "    oldDGU = {}\n",
    "    oldidx2freq = []\n",
    "    LKP = []\n",
    "    \n",
    "    print('For channel: |', channel, '| build GrainUnique and LookUp')\n",
    "    for idx, token in enumerate(LTU):\n",
    "        token_freq  = idx2freq[DTU[token]]\n",
    "        ChN = getChannelGrain4Token(token, channel, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "        grain2number = dict(collections.Counter(ChN).most_common())\n",
    "        for gr in grain2number:\n",
    "            if gr in oldDGU:\n",
    "                oldidx2freq[oldDGU[gr]] = oldidx2freq[oldDGU[gr]] + grain2number[gr] * token_freq\n",
    "            else:\n",
    "                oldDGU[gr] = len(oldDGU)\n",
    "                oldLGU.append(gr)\n",
    "                oldidx2freq.append(grain2number[gr] * token_freq)\n",
    "\n",
    "        LKP.append([oldDGU[gr] for gr in ChN])\n",
    "        if idx % 100000 == 0:\n",
    "            print('\\t\\tFor Channel:', channel, '\\t', idx, datetime.now())\n",
    "\n",
    "    # remove some high and low frequency grains.\n",
    "    # how to deal with the high freqency grains?\n",
    "    # notice that the grain freq is based on vocab instead of corpus.\n",
    "    assert len(LKP) == len(LTU)\n",
    "    \n",
    "    # sort the LGU, DGU and renew LKP\n",
    "    oldidx2freq = np.array(oldidx2freq)\n",
    "    max_grain_num = len(oldidx2freq[oldidx2freq >= min_grain_freq])\n",
    "    \n",
    "    del oldDGU \n",
    "    grainidx2freq = np.sort(oldidx2freq)[::-1]\n",
    "    newidx2oldidx = np.argsort(oldidx2freq)[::-1]\n",
    "    del oldidx2freq\n",
    "\n",
    "    oldidx2newidx = np.zeros(len(newidx2oldidx), dtype= int) \n",
    "    for new_idx, old_idx in enumerate(newidx2oldidx):\n",
    "        oldidx2newidx[old_idx] = new_idx\n",
    "    \n",
    "    for tkidx, grainlist in enumerate(LKP):\n",
    "        new_grainlist = []\n",
    "        for oldidx in grainlist:\n",
    "            newidx = oldidx2newidx[oldidx]\n",
    "            # throw away the low frequency grains\n",
    "            if grainidx2freq[newidx] < min_grain_freq:\n",
    "                continue\n",
    "            new_grainlist.append(newidx)\n",
    "        LKP[tkidx] = new_grainlist \n",
    "    del oldidx2newidx\n",
    "\n",
    "    LGU = []\n",
    "    for new_idx in range(max_grain_num):\n",
    "        # to filter some grains\n",
    "        LGU.append(oldLGU[newidx2oldidx[new_idx]])\n",
    "    del oldLGU\n",
    "    del newidx2oldidx\n",
    "\n",
    "    DGU = {}\n",
    "    for new_idx, token in enumerate(LGU):\n",
    "        DGU[token] = new_idx\n",
    "        \n",
    "    grainidx2freq = grainidx2freq[:max_grain_num]\n",
    "    \n",
    "    return (LGU, DGU), LKP, grainidx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_vocab_token_num  is: 2780\n",
      "min_token_freq       is: 2\n",
      "Corpus coverage rate is: 0.9238821308223598\n",
      "Token  coverage rate is: 0.4660519698239732\n",
      "For channel: | subcomp | build GrainUnique and LookUp\n",
      "\t\tFor Channel: subcomp \t 0 2019-07-02 18:29:02.661262\n"
     ]
    }
   ],
   "source": [
    "(LGU, DGU), LKP, grainidx2freq = get_GU_or_LKP(BasicObject.TokenVocab, BasicObject.idx2freq, \n",
    "                  channel= 'subcomp', Max_Ngram = 1, end_grain = False, \n",
    "                  max_vocab_token_num = None, min_token_freq = 2, min_grain_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c6',\n",
       " 'c1',\n",
       " 'c119',\n",
       " 'c1111',\n",
       " 'c64',\n",
       " 'c331',\n",
       " 'c209',\n",
       " 'c5',\n",
       " 'c45',\n",
       " 'c46',\n",
       " 'c67',\n",
       " ',',\n",
       " 'c99',\n",
       " 'c23',\n",
       " 'c20',\n",
       " 'c2',\n",
       " 'c139',\n",
       " 'c72',\n",
       " 'c277',\n",
       " 'c3',\n",
       " 'c207',\n",
       " 'c162',\n",
       " 'c113',\n",
       " 'c88',\n",
       " 'c326',\n",
       " '。',\n",
       " 'c41',\n",
       " 'c84',\n",
       " 'c83',\n",
       " 'c253',\n",
       " 'c49',\n",
       " 'c47',\n",
       " 'c11',\n",
       " 'c19',\n",
       " 'c238',\n",
       " 'c220',\n",
       " 'c82',\n",
       " 'c106',\n",
       " 'c185',\n",
       " 'c221',\n",
       " 'c140',\n",
       " 'c108',\n",
       " 'c17',\n",
       " 'c405',\n",
       " 'c159',\n",
       " 'c232',\n",
       " 'c127',\n",
       " 'c173',\n",
       " 'c42',\n",
       " '、',\n",
       " 'c135',\n",
       " 'c257',\n",
       " 'c339',\n",
       " 'c39',\n",
       " 'c156',\n",
       " 'c208',\n",
       " 'c154',\n",
       " 'c32',\n",
       " 'c27',\n",
       " 'c358',\n",
       " 'c48',\n",
       " 'c43',\n",
       " 'c247',\n",
       " 'c174',\n",
       " 'c55',\n",
       " 'c214',\n",
       " 'c24',\n",
       " 'c391',\n",
       " 'c105',\n",
       " 'c180',\n",
       " 'c120',\n",
       " 'c101',\n",
       " 'c71',\n",
       " 'c297',\n",
       " 'c202',\n",
       " 'c70',\n",
       " 'c166',\n",
       " 'c429',\n",
       " 'c63',\n",
       " 'c85',\n",
       " 'c298',\n",
       " 'c81',\n",
       " 'c9',\n",
       " 'c100',\n",
       " 'c142',\n",
       " 'c191',\n",
       " 'c44',\n",
       " 'c366',\n",
       " 'c61',\n",
       " 'c291',\n",
       " 'c62',\n",
       " 'c287',\n",
       " 'c301',\n",
       " 'c322',\n",
       " 'c38',\n",
       " 'c128',\n",
       " 'c50',\n",
       " 'c89',\n",
       " 'c80',\n",
       " 'c69',\n",
       " 'c122',\n",
       " 'c130',\n",
       " 'c93',\n",
       " 'c114',\n",
       " 'c97',\n",
       " 'c248',\n",
       " 'c22',\n",
       " 'c183',\n",
       " 'c138',\n",
       " 'c53',\n",
       " 'c90',\n",
       " 'c295',\n",
       " 'c4',\n",
       " 'c382',\n",
       " 'c152',\n",
       " 'c204',\n",
       " 'c161',\n",
       " 'c96',\n",
       " 'c426',\n",
       " 'c186',\n",
       " 'c155',\n",
       " 'c178',\n",
       " '(',\n",
       " 'c259',\n",
       " ')',\n",
       " 'c307',\n",
       " 'c246',\n",
       " 'c54',\n",
       " 'c335',\n",
       " 'c302',\n",
       " 'c40',\n",
       " 'c35',\n",
       " 'c145',\n",
       " 'c26',\n",
       " 'c57',\n",
       " 'c10',\n",
       " 'c143',\n",
       " 'c58',\n",
       " 'c254',\n",
       " 'c187',\n",
       " 'c360',\n",
       " 'c324',\n",
       " 'c236',\n",
       " 'c284',\n",
       " 'c206',\n",
       " 'c314',\n",
       " 'c349',\n",
       " 'c107',\n",
       " 'c95',\n",
       " 'c369',\n",
       " 'c357',\n",
       " 'c251',\n",
       " 'c403',\n",
       " 'c306',\n",
       " 'c392',\n",
       " 'c416',\n",
       " 'c103',\n",
       " 'c225',\n",
       " 'c126',\n",
       " 'c348',\n",
       " 'c37',\n",
       " 'c104',\n",
       " 'c224',\n",
       " 'c193',\n",
       " 'c25',\n",
       " 'c102',\n",
       " '·',\n",
       " 'c467',\n",
       " 'c211',\n",
       " '」',\n",
       " '「',\n",
       " 'c383',\n",
       " 'c373',\n",
       " 'c234',\n",
       " 'c7',\n",
       " 'c413',\n",
       " 'c412',\n",
       " 'c215',\n",
       " 'c421',\n",
       " 'c286',\n",
       " 'c184',\n",
       " 'c195',\n",
       " 'c245',\n",
       " '“',\n",
       " '”',\n",
       " 'c387',\n",
       " 'c480',\n",
       " 'c242',\n",
       " 'c299',\n",
       " 'c75',\n",
       " 'c158',\n",
       " 'c363',\n",
       " 'c16',\n",
       " ':',\n",
       " 'c454',\n",
       " 'c175',\n",
       " 'c121',\n",
       " '1',\n",
       " '》',\n",
       " '《',\n",
       " 'c226',\n",
       " 'c400',\n",
       " 'c283',\n",
       " 'c51',\n",
       " 'c149',\n",
       " 'c406',\n",
       " 'c243',\n",
       " 'c237',\n",
       " 'c163',\n",
       " 'c182',\n",
       " 'c116',\n",
       " 'c442',\n",
       " 'c264',\n",
       " 'c375',\n",
       " 'c458',\n",
       " 'c115',\n",
       " 'c197',\n",
       " 'c76',\n",
       " 'c87',\n",
       " '9',\n",
       " 'c157',\n",
       " 'c410',\n",
       " '0',\n",
       " 'c388',\n",
       " 'c200',\n",
       " 'c144',\n",
       " 'c341',\n",
       " 'c463',\n",
       " 'c244',\n",
       " 'c176',\n",
       " '\"',\n",
       " 'c460',\n",
       " 'c361',\n",
       " 'c131',\n",
       " 'c321',\n",
       " 'e',\n",
       " 'c268',\n",
       " 'c452',\n",
       " 'c285',\n",
       " 'o',\n",
       " 'c427',\n",
       " 'c476',\n",
       " 'c98',\n",
       " 'i',\n",
       " 'c278',\n",
       " 'a',\n",
       " '-',\n",
       " 'c368',\n",
       " 'n',\n",
       " 'c31',\n",
       " 'c194',\n",
       " 't',\n",
       " 'c407',\n",
       " 'c304',\n",
       " 'c172',\n",
       " 'c313',\n",
       " 'c364',\n",
       " 'c401',\n",
       " 'c269',\n",
       " 'c29',\n",
       " 'c165',\n",
       " 'c397',\n",
       " ';',\n",
       " 'c424',\n",
       " 'c305',\n",
       " 'c73',\n",
       " 'c281',\n",
       " 'c263',\n",
       " 'c425',\n",
       " 'c303',\n",
       " 'c169',\n",
       " 'c8',\n",
       " 'c192',\n",
       " '2',\n",
       " 'c347',\n",
       " 'c188',\n",
       " 'c271',\n",
       " 'c160',\n",
       " 'c65',\n",
       " 'c346',\n",
       " 'c219',\n",
       " 'c437',\n",
       " 'c252',\n",
       " 'c164',\n",
       " 'c34',\n",
       " 'c118',\n",
       " 'c308',\n",
       " 'c148',\n",
       " 'c240',\n",
       " 'c150',\n",
       " 'c265',\n",
       " 'c439',\n",
       " 'c374',\n",
       " 'c249',\n",
       " 'c14',\n",
       " 'c435',\n",
       " 'c404',\n",
       " 'c79',\n",
       " 'c15',\n",
       " 'c279',\n",
       " 'c256',\n",
       " 'c21',\n",
       " 'c274',\n",
       " '6',\n",
       " 'c111',\n",
       " 'c408',\n",
       " 'm',\n",
       " 'r',\n",
       " 'c227',\n",
       " 'c86',\n",
       " 'c92',\n",
       " 'h',\n",
       " 'c77',\n",
       " 's',\n",
       " 'c218',\n",
       " 'c13',\n",
       " 'c328',\n",
       " 'c68',\n",
       " 'c409',\n",
       " 'c',\n",
       " 'l',\n",
       " 'c296',\n",
       " '3',\n",
       " 'c469',\n",
       " 'c464',\n",
       " 'c177',\n",
       " 'c318',\n",
       " '7',\n",
       " 'c280',\n",
       " 'c198',\n",
       " 'c60',\n",
       " '8',\n",
       " 'c28',\n",
       " 'c475',\n",
       " 'c509',\n",
       " 'u',\n",
       " 'c231',\n",
       " 'c471',\n",
       " 'c74',\n",
       " 'c228',\n",
       " 'c230',\n",
       " 'c440',\n",
       " '4',\n",
       " 'p',\n",
       " '5',\n",
       " 'c136',\n",
       " 'c310',\n",
       " 'c334',\n",
       " 'c210',\n",
       " '.',\n",
       " 'c415',\n",
       " 'c482',\n",
       " 'c56',\n",
       " '—',\n",
       " 'C',\n",
       " 'P',\n",
       " 'd',\n",
       " 'g',\n",
       " 'c235',\n",
       " 'c33',\n",
       " 'b',\n",
       " 'c372',\n",
       " 'f',\n",
       " 'c276',\n",
       " 'c356',\n",
       " 'c199',\n",
       " 'I',\n",
       " 'c384',\n",
       " 'c495',\n",
       " 'c423',\n",
       " 'c125',\n",
       " 'c309',\n",
       " 'H',\n",
       " 'M',\n",
       " 'c261',\n",
       " '/',\n",
       " 'B',\n",
       " 'c223',\n",
       " 'c386',\n",
       " 'c275',\n",
       " 'c141',\n",
       " 'c250',\n",
       " 'c329',\n",
       " '…',\n",
       " 'y',\n",
       " 'c300',\n",
       " 'A',\n",
       " 'c472',\n",
       " 'N',\n",
       " 'c493',\n",
       " 'F',\n",
       " 'c340',\n",
       " 'c393',\n",
       " 'c342',\n",
       " ']',\n",
       " 'c153',\n",
       " '[',\n",
       " 'c239',\n",
       " 'c447',\n",
       " 'c117',\n",
       " '『',\n",
       " '』',\n",
       " 'c402',\n",
       " '•',\n",
       " 'c332',\n",
       " \"'\",\n",
       " 'c282',\n",
       " 'c353',\n",
       " '?',\n",
       " '%',\n",
       " 'c359',\n",
       " 'v',\n",
       " 'c395',\n",
       " 'c266',\n",
       " 'c365',\n",
       " 'S',\n",
       " 'w',\n",
       " 'G',\n",
       " 'c147',\n",
       " 'c146',\n",
       " 'c137',\n",
       " 'c420',\n",
       " 'c171',\n",
       " 'c91',\n",
       " 'c431',\n",
       " 'R',\n",
       " '*',\n",
       " 'c229',\n",
       " '’',\n",
       " '・',\n",
       " 'T',\n",
       " 'E',\n",
       " 'c418',\n",
       " 'c203',\n",
       " 'c241',\n",
       " 'c294',\n",
       " 'c477',\n",
       " 'ʔ',\n",
       " 'c352',\n",
       " 'c30',\n",
       " 'c396',\n",
       " 'c450',\n",
       " 'c179',\n",
       " 'c78',\n",
       " 'K',\n",
       " '〉',\n",
       " '〈',\n",
       " 'z',\n",
       " '‘',\n",
       " 'J',\n",
       " 'W',\n",
       " 'c167',\n",
       " 'c213',\n",
       " 'c327',\n",
       " '–',\n",
       " 'j',\n",
       " 'D']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c6': 0,\n",
       " 'c1': 1,\n",
       " 'c119': 2,\n",
       " 'c1111': 3,\n",
       " 'c64': 4,\n",
       " 'c331': 5,\n",
       " 'c209': 6,\n",
       " 'c5': 7,\n",
       " 'c45': 8,\n",
       " 'c46': 9,\n",
       " 'c67': 10,\n",
       " ',': 11,\n",
       " 'c99': 12,\n",
       " 'c23': 13,\n",
       " 'c20': 14,\n",
       " 'c2': 15,\n",
       " 'c139': 16,\n",
       " 'c72': 17,\n",
       " 'c277': 18,\n",
       " 'c3': 19,\n",
       " 'c207': 20,\n",
       " 'c162': 21,\n",
       " 'c113': 22,\n",
       " 'c88': 23,\n",
       " 'c326': 24,\n",
       " '。': 25,\n",
       " 'c41': 26,\n",
       " 'c84': 27,\n",
       " 'c83': 28,\n",
       " 'c253': 29,\n",
       " 'c49': 30,\n",
       " 'c47': 31,\n",
       " 'c11': 32,\n",
       " 'c19': 33,\n",
       " 'c238': 34,\n",
       " 'c220': 35,\n",
       " 'c82': 36,\n",
       " 'c106': 37,\n",
       " 'c185': 38,\n",
       " 'c221': 39,\n",
       " 'c140': 40,\n",
       " 'c108': 41,\n",
       " 'c17': 42,\n",
       " 'c405': 43,\n",
       " 'c159': 44,\n",
       " 'c232': 45,\n",
       " 'c127': 46,\n",
       " 'c173': 47,\n",
       " 'c42': 48,\n",
       " '、': 49,\n",
       " 'c135': 50,\n",
       " 'c257': 51,\n",
       " 'c339': 52,\n",
       " 'c39': 53,\n",
       " 'c156': 54,\n",
       " 'c208': 55,\n",
       " 'c154': 56,\n",
       " 'c32': 57,\n",
       " 'c27': 58,\n",
       " 'c358': 59,\n",
       " 'c48': 60,\n",
       " 'c43': 61,\n",
       " 'c247': 62,\n",
       " 'c174': 63,\n",
       " 'c55': 64,\n",
       " 'c214': 65,\n",
       " 'c24': 66,\n",
       " 'c391': 67,\n",
       " 'c105': 68,\n",
       " 'c180': 69,\n",
       " 'c120': 70,\n",
       " 'c101': 71,\n",
       " 'c71': 72,\n",
       " 'c297': 73,\n",
       " 'c202': 74,\n",
       " 'c70': 75,\n",
       " 'c166': 76,\n",
       " 'c429': 77,\n",
       " 'c63': 78,\n",
       " 'c85': 79,\n",
       " 'c298': 80,\n",
       " 'c81': 81,\n",
       " 'c9': 82,\n",
       " 'c100': 83,\n",
       " 'c142': 84,\n",
       " 'c191': 85,\n",
       " 'c44': 86,\n",
       " 'c366': 87,\n",
       " 'c61': 88,\n",
       " 'c291': 89,\n",
       " 'c62': 90,\n",
       " 'c287': 91,\n",
       " 'c301': 92,\n",
       " 'c322': 93,\n",
       " 'c38': 94,\n",
       " 'c128': 95,\n",
       " 'c50': 96,\n",
       " 'c89': 97,\n",
       " 'c80': 98,\n",
       " 'c69': 99,\n",
       " 'c122': 100,\n",
       " 'c130': 101,\n",
       " 'c93': 102,\n",
       " 'c114': 103,\n",
       " 'c97': 104,\n",
       " 'c248': 105,\n",
       " 'c22': 106,\n",
       " 'c183': 107,\n",
       " 'c138': 108,\n",
       " 'c53': 109,\n",
       " 'c90': 110,\n",
       " 'c295': 111,\n",
       " 'c4': 112,\n",
       " 'c382': 113,\n",
       " 'c152': 114,\n",
       " 'c204': 115,\n",
       " 'c161': 116,\n",
       " 'c96': 117,\n",
       " 'c426': 118,\n",
       " 'c186': 119,\n",
       " 'c155': 120,\n",
       " 'c178': 121,\n",
       " '(': 122,\n",
       " 'c259': 123,\n",
       " ')': 124,\n",
       " 'c307': 125,\n",
       " 'c246': 126,\n",
       " 'c54': 127,\n",
       " 'c335': 128,\n",
       " 'c302': 129,\n",
       " 'c40': 130,\n",
       " 'c35': 131,\n",
       " 'c145': 132,\n",
       " 'c26': 133,\n",
       " 'c57': 134,\n",
       " 'c10': 135,\n",
       " 'c143': 136,\n",
       " 'c58': 137,\n",
       " 'c254': 138,\n",
       " 'c187': 139,\n",
       " 'c360': 140,\n",
       " 'c324': 141,\n",
       " 'c236': 142,\n",
       " 'c284': 143,\n",
       " 'c206': 144,\n",
       " 'c314': 145,\n",
       " 'c349': 146,\n",
       " 'c107': 147,\n",
       " 'c95': 148,\n",
       " 'c369': 149,\n",
       " 'c357': 150,\n",
       " 'c251': 151,\n",
       " 'c403': 152,\n",
       " 'c306': 153,\n",
       " 'c392': 154,\n",
       " 'c416': 155,\n",
       " 'c103': 156,\n",
       " 'c225': 157,\n",
       " 'c126': 158,\n",
       " 'c348': 159,\n",
       " 'c37': 160,\n",
       " 'c104': 161,\n",
       " 'c224': 162,\n",
       " 'c193': 163,\n",
       " 'c25': 164,\n",
       " 'c102': 165,\n",
       " '·': 166,\n",
       " 'c467': 167,\n",
       " 'c211': 168,\n",
       " '」': 169,\n",
       " '「': 170,\n",
       " 'c383': 171,\n",
       " 'c373': 172,\n",
       " 'c234': 173,\n",
       " 'c7': 174,\n",
       " 'c413': 175,\n",
       " 'c412': 176,\n",
       " 'c215': 177,\n",
       " 'c421': 178,\n",
       " 'c286': 179,\n",
       " 'c184': 180,\n",
       " 'c195': 181,\n",
       " 'c245': 182,\n",
       " '“': 183,\n",
       " '”': 184,\n",
       " 'c387': 185,\n",
       " 'c480': 186,\n",
       " 'c242': 187,\n",
       " 'c299': 188,\n",
       " 'c75': 189,\n",
       " 'c158': 190,\n",
       " 'c363': 191,\n",
       " 'c16': 192,\n",
       " ':': 193,\n",
       " 'c454': 194,\n",
       " 'c175': 195,\n",
       " 'c121': 196,\n",
       " '1': 197,\n",
       " '》': 198,\n",
       " '《': 199,\n",
       " 'c226': 200,\n",
       " 'c400': 201,\n",
       " 'c283': 202,\n",
       " 'c51': 203,\n",
       " 'c149': 204,\n",
       " 'c406': 205,\n",
       " 'c243': 206,\n",
       " 'c237': 207,\n",
       " 'c163': 208,\n",
       " 'c182': 209,\n",
       " 'c116': 210,\n",
       " 'c442': 211,\n",
       " 'c264': 212,\n",
       " 'c375': 213,\n",
       " 'c458': 214,\n",
       " 'c115': 215,\n",
       " 'c197': 216,\n",
       " 'c76': 217,\n",
       " 'c87': 218,\n",
       " '9': 219,\n",
       " 'c157': 220,\n",
       " 'c410': 221,\n",
       " '0': 222,\n",
       " 'c388': 223,\n",
       " 'c200': 224,\n",
       " 'c144': 225,\n",
       " 'c341': 226,\n",
       " 'c463': 227,\n",
       " 'c244': 228,\n",
       " 'c176': 229,\n",
       " '\"': 230,\n",
       " 'c460': 231,\n",
       " 'c361': 232,\n",
       " 'c131': 233,\n",
       " 'c321': 234,\n",
       " 'e': 235,\n",
       " 'c268': 236,\n",
       " 'c452': 237,\n",
       " 'c285': 238,\n",
       " 'o': 239,\n",
       " 'c427': 240,\n",
       " 'c476': 241,\n",
       " 'c98': 242,\n",
       " 'i': 243,\n",
       " 'c278': 244,\n",
       " 'a': 245,\n",
       " '-': 246,\n",
       " 'c368': 247,\n",
       " 'n': 248,\n",
       " 'c31': 249,\n",
       " 'c194': 250,\n",
       " 't': 251,\n",
       " 'c407': 252,\n",
       " 'c304': 253,\n",
       " 'c172': 254,\n",
       " 'c313': 255,\n",
       " 'c364': 256,\n",
       " 'c401': 257,\n",
       " 'c269': 258,\n",
       " 'c29': 259,\n",
       " 'c165': 260,\n",
       " 'c397': 261,\n",
       " ';': 262,\n",
       " 'c424': 263,\n",
       " 'c305': 264,\n",
       " 'c73': 265,\n",
       " 'c281': 266,\n",
       " 'c263': 267,\n",
       " 'c425': 268,\n",
       " 'c303': 269,\n",
       " 'c169': 270,\n",
       " 'c8': 271,\n",
       " 'c192': 272,\n",
       " '2': 273,\n",
       " 'c347': 274,\n",
       " 'c188': 275,\n",
       " 'c271': 276,\n",
       " 'c160': 277,\n",
       " 'c65': 278,\n",
       " 'c346': 279,\n",
       " 'c219': 280,\n",
       " 'c437': 281,\n",
       " 'c252': 282,\n",
       " 'c164': 283,\n",
       " 'c34': 284,\n",
       " 'c118': 285,\n",
       " 'c308': 286,\n",
       " 'c148': 287,\n",
       " 'c240': 288,\n",
       " 'c150': 289,\n",
       " 'c265': 290,\n",
       " 'c439': 291,\n",
       " 'c374': 292,\n",
       " 'c249': 293,\n",
       " 'c14': 294,\n",
       " 'c435': 295,\n",
       " 'c404': 296,\n",
       " 'c79': 297,\n",
       " 'c15': 298,\n",
       " 'c279': 299,\n",
       " 'c256': 300,\n",
       " 'c21': 301,\n",
       " 'c274': 302,\n",
       " '6': 303,\n",
       " 'c111': 304,\n",
       " 'c408': 305,\n",
       " 'm': 306,\n",
       " 'r': 307,\n",
       " 'c227': 308,\n",
       " 'c86': 309,\n",
       " 'c92': 310,\n",
       " 'h': 311,\n",
       " 'c77': 312,\n",
       " 's': 313,\n",
       " 'c218': 314,\n",
       " 'c13': 315,\n",
       " 'c328': 316,\n",
       " 'c68': 317,\n",
       " 'c409': 318,\n",
       " 'c': 319,\n",
       " 'l': 320,\n",
       " 'c296': 321,\n",
       " '3': 322,\n",
       " 'c469': 323,\n",
       " 'c464': 324,\n",
       " 'c177': 325,\n",
       " 'c318': 326,\n",
       " '7': 327,\n",
       " 'c280': 328,\n",
       " 'c198': 329,\n",
       " 'c60': 330,\n",
       " '8': 331,\n",
       " 'c28': 332,\n",
       " 'c475': 333,\n",
       " 'c509': 334,\n",
       " 'u': 335,\n",
       " 'c231': 336,\n",
       " 'c471': 337,\n",
       " 'c74': 338,\n",
       " 'c228': 339,\n",
       " 'c230': 340,\n",
       " 'c440': 341,\n",
       " '4': 342,\n",
       " 'p': 343,\n",
       " '5': 344,\n",
       " 'c136': 345,\n",
       " 'c310': 346,\n",
       " 'c334': 347,\n",
       " 'c210': 348,\n",
       " '.': 349,\n",
       " 'c415': 350,\n",
       " 'c482': 351,\n",
       " 'c56': 352,\n",
       " '—': 353,\n",
       " 'C': 354,\n",
       " 'P': 355,\n",
       " 'd': 356,\n",
       " 'g': 357,\n",
       " 'c235': 358,\n",
       " 'c33': 359,\n",
       " 'b': 360,\n",
       " 'c372': 361,\n",
       " 'f': 362,\n",
       " 'c276': 363,\n",
       " 'c356': 364,\n",
       " 'c199': 365,\n",
       " 'I': 366,\n",
       " 'c384': 367,\n",
       " 'c495': 368,\n",
       " 'c423': 369,\n",
       " 'c125': 370,\n",
       " 'c309': 371,\n",
       " 'H': 372,\n",
       " 'M': 373,\n",
       " 'c261': 374,\n",
       " '/': 375,\n",
       " 'B': 376,\n",
       " 'c223': 377,\n",
       " 'c386': 378,\n",
       " 'c275': 379,\n",
       " 'c141': 380,\n",
       " 'c250': 381,\n",
       " 'c329': 382,\n",
       " '…': 383,\n",
       " 'y': 384,\n",
       " 'c300': 385,\n",
       " 'A': 386,\n",
       " 'c472': 387,\n",
       " 'N': 388,\n",
       " 'c493': 389,\n",
       " 'F': 390,\n",
       " 'c340': 391,\n",
       " 'c393': 392,\n",
       " 'c342': 393,\n",
       " ']': 394,\n",
       " 'c153': 395,\n",
       " '[': 396,\n",
       " 'c239': 397,\n",
       " 'c447': 398,\n",
       " 'c117': 399,\n",
       " '『': 400,\n",
       " '』': 401,\n",
       " 'c402': 402,\n",
       " '•': 403,\n",
       " 'c332': 404,\n",
       " \"'\": 405,\n",
       " 'c282': 406,\n",
       " 'c353': 407,\n",
       " '?': 408,\n",
       " '%': 409,\n",
       " 'c359': 410,\n",
       " 'v': 411,\n",
       " 'c395': 412,\n",
       " 'c266': 413,\n",
       " 'c365': 414,\n",
       " 'S': 415,\n",
       " 'w': 416,\n",
       " 'G': 417,\n",
       " 'c147': 418,\n",
       " 'c146': 419,\n",
       " 'c137': 420,\n",
       " 'c420': 421,\n",
       " 'c171': 422,\n",
       " 'c91': 423,\n",
       " 'c431': 424,\n",
       " 'R': 425,\n",
       " '*': 426,\n",
       " 'c229': 427,\n",
       " '’': 428,\n",
       " '・': 429,\n",
       " 'T': 430,\n",
       " 'E': 431,\n",
       " 'c418': 432,\n",
       " 'c203': 433,\n",
       " 'c241': 434,\n",
       " 'c294': 435,\n",
       " 'c477': 436,\n",
       " 'ʔ': 437,\n",
       " 'c352': 438,\n",
       " 'c30': 439,\n",
       " 'c396': 440,\n",
       " 'c450': 441,\n",
       " 'c179': 442,\n",
       " 'c78': 443,\n",
       " 'K': 444,\n",
       " '〉': 445,\n",
       " '〈': 446,\n",
       " 'z': 447,\n",
       " '‘': 448,\n",
       " 'J': 449,\n",
       " 'W': 450,\n",
       " 'c167': 451,\n",
       " 'c213': 452,\n",
       " 'c327': 453,\n",
       " '–': 454,\n",
       " 'j': 455,\n",
       " 'D': 456}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DGU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12015,  9213,  6980,  4868,  3565,  3463,  3430,  3429,  2791,\n",
       "        2755,  2478,  2356,  2131,  1868,  1788,  1781,  1621,  1599,\n",
       "        1520,  1498,  1483,  1479,  1459,  1437,  1424,  1415,  1328,\n",
       "        1291,  1250,  1187,  1182,  1177,  1151,  1118,  1067,  1058,\n",
       "        1042,   960,   958,   941,   921,   888,   818,   801,   785,\n",
       "         772,   748,   739,   733,   729,   722,   677,   674,   672,\n",
       "         666,   662,   647,   629,   629,   628,   619,   619,   606,\n",
       "         586,   585,   585,   582,   562,   557,   555,   551,   550,\n",
       "         541,   538,   536,   533,   530,   520,   516,   498,   494,\n",
       "         484,   475,   462,   457,   447,   438,   426,   421,   417,\n",
       "         416,   414,   406,   405,   400,   400,   398,   396,   390,\n",
       "         388,   386,   383,   373,   373,   368,   353,   352,   344,\n",
       "         343,   333,   332,   326,   316,   315,   315,   308,   301,\n",
       "         296,   286,   284,   273,   271,   270,   270,   269,   267,\n",
       "         262,   260,   259,   256,   240,   238,   234,   234,   234,\n",
       "         231,   231,   227,   226,   226,   223,   221,   219,   216,\n",
       "         214,   213,   211,   210,   209,   209,   207,   205,   204,\n",
       "         201,   201,   191,   186,   186,   183,   182,   180,   179,\n",
       "         167,   167,   161,   160,   160,   159,   157,   155,   155,\n",
       "         155,   153,   152,   151,   150,   149,   149,   149,   147,\n",
       "         145,   140,   137,   135,   134,   132,   126,   124,   124,\n",
       "         124,   122,   122,   119,   115,   111,   110,   105,   105,\n",
       "         104,   104,   102,   102,    99,    98,    95,    94,    93,\n",
       "          92,    91,    91,    89,    89,    86,    86,    86,    79,\n",
       "          78,    78,    77,    76,    75,    75,    75,    75,    74,\n",
       "          73,    73,    73,    71,    70,    68,    68,    68,    68,\n",
       "          68,    68,    67,    66,    65,    65,    65,    64,    64,\n",
       "          60,    60,    60,    59,    59,    59,    57,    57,    57,\n",
       "          56,    56,    55,    55,    54,    53,    52,    52,    52,\n",
       "          51,    51,    50,    50,    47,    47,    46,    46,    45,\n",
       "          45,    45,    44,    44,    44,    43,    42,    42,    42,\n",
       "          41,    41,    41,    40,    40,    39,    39,    38,    38,\n",
       "          38,    38,    37,    37,    37,    37,    36,    36,    35,\n",
       "          35,    34,    34,    32,    32,    31,    31,    30,    28,\n",
       "          28,    28,    27,    27,    26,    26,    26,    26,    26,\n",
       "          25,    24,    24,    24,    24,    24,    24,    23,    23,\n",
       "          23,    22,    22,    22,    22,    22,    22,    22,    21,\n",
       "          20,    20,    20,    19,    19,    19,    18,    18,    17,\n",
       "          17,    17,    17,    17,    16,    16,    16,    16,    16,\n",
       "          16,    16,    16,    15,    15,    15,    13,    13,    12,\n",
       "          12,    12,    12,    12,    12,    12,    11,    11,    11,\n",
       "          11,    11,    11,    10,    10,     9,     9,     9,     9,\n",
       "           8,     8,     8,     8,     7,     7,     7,     7,     7,\n",
       "           7,     7,     6,     6,     6,     6,     6,     6,     6,\n",
       "           6,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "           5,     5,     5,     5,     5,     4,     4,     4,     4,\n",
       "           4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "           4,     4,     4,     3,     3,     3,     3,     3,     3,\n",
       "           3,     3,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grainidx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################################################\n",
    "def getChannelName(channel, Max_Ngram = 1,  end_grain = False, tagScheme = 'BIO', min_grain_freq = 1,\n",
    "                   style = 'normal', channel_name = None, channel_name_abbr = None, **kwargs):\n",
    "\n",
    "    if style == 'normal':\n",
    "        MN = str(Max_Ngram) if Max_Ngram > 1 else ''\n",
    "        e  = 'e'            if end_grain else ''\n",
    "        f  = '-f' + str(min_grain_freq) if min_grain_freq>1 else ''\n",
    "        tS = '-' + tagScheme.replace('BIO', '').lower() if tagScheme != 'BIO' else ''\n",
    "        return channel + MN + e + tS + f \n",
    "\n",
    "    elif style == 'abbr':\n",
    "        channel = CHANNEL_ABBR[channel] # if abbr else channel\n",
    "        MN = str(Max_Ngram) if Max_Ngram > 1 else ''\n",
    "        e  = 'e'            if end_grain else ''\n",
    "        f  = '-f' + str(min_grain_freq) if min_grain_freq>1 else ''\n",
    "        tS = '-' + tagScheme.replace('BIO', '').lower() if tagScheme != 'BIO' else ''\n",
    "        return channel + MN + e + tS + f \n",
    "\n",
    "    elif channel_name and style == 'extract':\n",
    "        assert channel in channel_name\n",
    "\n",
    "        if '-f' in channel_name:\n",
    "            channel_name, min_grain_freq = channel_name.split('-f')\n",
    "            min_grain_freq = str(min_grain_freq)\n",
    "        else:\n",
    "            min_grain_freq = 1\n",
    "\n",
    "        MN_e_tS = channel_name[len(channel):]\n",
    "        if len(MN_e_tS) == 0:\n",
    "            return channel, Max_Ngram, end_grain, tagScheme\n",
    "        if MN_e_tS[0] in '23456789':\n",
    "            Max_Ngram = int(MN_e_tS[0])\n",
    "            e_ts = MN_e_tS[1:]\n",
    "            if len(e_ts) == 0:\n",
    "                return channel, Max_Ngram, end_grain, tagScheme\n",
    "        else:\n",
    "            Max_Ngram = 1\n",
    "            e_ts = MN_e_tS\n",
    "        \n",
    "        if e_ts[0] == 'e':\n",
    "            end_grain = True\n",
    "            ts = e_ts[1:]\n",
    "        else:\n",
    "            end_grain = False\n",
    "            ts = e_ts\n",
    "        if ts.upper() in ['-ES', '-E', '-S']:\n",
    "            tagScheme = 'BIO' + ts.upper()[1:]\n",
    "        else:\n",
    "            tagScheme = 'BIO'\n",
    "        return channel, Max_Ngram, end_grain, tagScheme, min_grain_freq\n",
    "        \n",
    "    elif channel_name_abbr and style == 'extract':\n",
    "        channel_abbr = CHANNEL_ABBR[channel]\n",
    "\n",
    "        if '-f' in channel_name_abbr:\n",
    "            channel_name_abbr, min_grain_freq = channel_name_abbr.split('-f')\n",
    "            min_grain_freq = str(min_grain_freq)\n",
    "        else:\n",
    "            min_grain_freq = 1\n",
    "\n",
    "\n",
    "        MN_e_tS = channel_name_abbr[len(channel_abbr): ]\n",
    "        if len(MN_e_tS) == 0:\n",
    "            return channel, Max_Ngram, end_grain, tagScheme\n",
    "        if MN_e_tS[0] in '23456789':\n",
    "            Max_Ngram = int(MN_e_tS[0])\n",
    "            e_ts = MN_e_tS[1:]\n",
    "            if len(e_ts) == 0:\n",
    "                return channel, Max_Ngram, end_grain, tagScheme\n",
    "        else:\n",
    "            Max_Ngram = 1\n",
    "            e_ts = MN_e_tS\n",
    "        \n",
    "        if e_ts[0] == 'e':\n",
    "            end_grain = True\n",
    "            ts = e_ts[1:]\n",
    "        else:\n",
    "            end_grain = False\n",
    "            ts = e_ts\n",
    "        if ts.upper() in ['-ES', '-E', '-S']:\n",
    "            tagScheme = 'BIO' + ts.upper()[1:]\n",
    "        else:\n",
    "            tagScheme = 'BIO'\n",
    "        \n",
    "        return channel, Max_Ngram, end_grain, tagScheme, min_grain_freq\n",
    "\n",
    "    else:\n",
    "        print('Error in getChannelName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subcomp9e-f3'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import CHANNEL_ABBR\n",
    "\n",
    "getChannelName(channel = 'subcomp', Min_Ngram = 1, Max_Ngram = 9,  end_grain = True, tagScheme = 'BIO', min_grain_freq = 3,\n",
    "                   style = 'normal', channel_name = None, channel_name_abbr = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
