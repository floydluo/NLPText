{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.560 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 8414\n",
      "Total Num of Unique Tokens 2003\n",
      "CORPUS\tit is Dumped into file: data/wiki/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8414\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki/word/Vocab/pos-bioes.tsv\n",
      "pos-bioes\tis Dumped into file: data/wiki/word/Vocab/token.voc\n",
      "pos-bioes\tthe length of it is   : 2003\n",
      "\t\tWrite to: data/wiki/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = True\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, annoKW = {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getGrainVocab` and `VOCAB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char-n1t3-f2\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-07-03 11:13:09.085533\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/char-n1t3-f2.voc\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/char-n1t3-f2.tsv\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/char-n1t3-f2.lkp\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/char-n1t3-f2.freq\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channel = 'char'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 2}\n",
    "\n",
    "GrainVocab = BasicObject.getGrainVocab(channel, **channel_setting)\n",
    "# print(GrainVocab[0])\n",
    "# print(GrainVocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild Grain Uniqe and LookUp Table for channel: subcomp-n1t3-f3\n",
      "For channel: | subcomp | build GrainUnique and LookUp\n",
      "\t\tFor Channel: subcomp \t 0 2019-07-03 11:13:46.350851\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/subcomp-n1t3-f3.voc\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/subcomp-n1t3-f3.tsv\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/subcomp-n1t3-f3.lkp\n",
      "\t\tWrite to: data/wiki/word/Vocab/F1/subcomp-n1t3-f3.freq\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channel = 'subcomp'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 3}\n",
    "\n",
    "GrainVocab = BasicObject.getGrainVocab(channel, **channel_setting)\n",
    "# print(GrainVocab[0])\n",
    "# print(GrainVocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Fields Description\n",
    "\n",
    "TODO: about sub fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Information\n",
    "\n",
    "\n",
    "Token is made up by Chars.\n",
    "\n",
    "Word-Token is made up by several Chars, while Char-Token has only one Char.\n",
    "\n",
    "\n",
    "Use Word-Token as an example:\n",
    "\n",
    "\n",
    "北京 --> 北 京 \n",
    "\n",
    "Beijing --> B e i j i n g\n",
    "\n",
    "\n",
    "In this section, we want to derive more information for char only. (Not the whole token)\n",
    "\n",
    "## Char Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京']\n",
      "['j']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def charGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    info = [char]\n",
    "    if end_grain:\n",
    "        info = info + ['ch0']\n",
    "    return info\n",
    "\n",
    "\n",
    "print(charGrainChar('京'))\n",
    "print(charGrainChar('j'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京', 'ch0']\n",
      "['j', 'ch0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(charGrainChar('京', end_grain= True))\n",
    "print(charGrainChar('j', end_grain= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Char's SubComp\n",
    "\n",
    "For English char (letter), only return itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c67', 'c119', 'c159']\n",
      "['j']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('nlptext/sources/CharSubComp.p', 'rb') as handle:\n",
    "    CharSubCompInfos = pickle.load(handle)\n",
    "\n",
    "def subcompGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    if char in CharSubCompInfos:\n",
    "        info = CharSubCompInfos[char]\n",
    "        if info:\n",
    "            info = ['c' + i for i in info ]\n",
    "        else:\n",
    "            info = ['c' + char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['c0']\n",
    "    return info\n",
    "\n",
    "\n",
    "print(subcompGrainChar('京'))\n",
    "print(subcompGrainChar('j'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c67', 'c119', 'c159', 'c0']\n",
      "['j', 'c0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(subcompGrainChar('京', end_grain= True))\n",
    "print(subcompGrainChar('j', end_grain= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap as a Token Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('nlptext/sources/CharSubComp.p', 'rb') as handle:\n",
    "    CharSubCompInfos = pickle.load(handle)\n",
    "\n",
    "def subcompGrainChar(char, end_grain = False):\n",
    "    '''char level only!'''\n",
    "    if char in CharSubCompInfos:\n",
    "        info = CharSubCompInfos[char]\n",
    "        if info:\n",
    "            info = ['c' + i for i in info ]\n",
    "        else:\n",
    "            info = ['c' + char] \n",
    "    else:\n",
    "        info = [char]\n",
    "        \n",
    "    if end_grain:\n",
    "        info = info + ['c0']\n",
    "    return info\n",
    "\n",
    "def subcompGrainToken(token, end_grain = False):\n",
    "    info = sum([subcompGrainChar(char, end_grain) for char in token], [])\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c117', 'c24', 'c0', 'c67', 'c119', 'c159', 'c0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "end_grain = True\n",
    "\n",
    "subcompGrainToken(token, end_grain = end_grain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'e', 'i', 'j', 'i', 'n', 'g']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijing'\n",
    "end_grain = False\n",
    "\n",
    "subcompGrainToken(token,  end_grain = end_grain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bei', 'jing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyphen\n",
    "\n",
    "\n",
    "def syllableGrainToken(token, end_grain = False):\n",
    "    \n",
    "    # pyphen.LANGUAGES\n",
    "    dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "    # token = 'tomorrow'\n",
    "    return dic.inserted(token).split('-')\n",
    "\n",
    "\n",
    "token = 'Beijing'\n",
    "syllableGrainToken(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('nlptext/sources/WordPhoneme.p', 'rb') as handle:\n",
    "    WordPhenomeInfo = pickle.load(handle)\n",
    "\n",
    "def phonemeGrainToken(token, end_grain = False): \n",
    "    try:\n",
    "        phonemes = WordPhenomeInfo[token.lower()]\n",
    "    except:\n",
    "        phonemes = ['']\n",
    "    return phonemes\n",
    "\n",
    "token = 'Beijing'\n",
    "phonemeGrainToken(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Token-Based Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char': <function nlptext.utils.channel.charGrainToken(token, end_grain=False)>,\n",
       " 'basic': <function nlptext.utils.channel.basicGrainToken(token, end_grain=False)>,\n",
       " 'medical': <function nlptext.utils.channel.medicalGrainToken(token, end_grain=False)>,\n",
       " 'radical': <function nlptext.utils.channel.radicalGrainToken(token, end_grain=False)>,\n",
       " 'subcomp': <function nlptext.utils.channel.subcompGrainToken(token, end_grain=False)>,\n",
       " 'stroke': <function nlptext.utils.channel.strokeGrainToken(token, end_grain=False)>,\n",
       " 'pinyin': <function nlptext.utils.channel.pinyinGrainToken(token, end_grain=False)>,\n",
       " 'syllable': <function nlptext.utils.channel.syllableGrainToken(token, end_grain=False)>,\n",
       " 'phoneme': <function nlptext.utils.channel.phonemeGrainToken(token, end_grain=False)>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import Channel_Ind_Methods\n",
    "\n",
    "Channel_Ind_Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = Channel_Ind_Methods['phoneme']\n",
    "method('Beijing', end_grain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGrainNgrams(subword_infos, n):\n",
    "    if n == 1:\n",
    "        return [i for i in subword_infos]\n",
    "    if n > len(subword_infos):\n",
    "        # How to deal this when the length is not so long\n",
    "        # Condition: where n is larger than the infos\n",
    "        return [] \n",
    "    l = [subword_infos[i:n+i] for i in range(len(subword_infos) - n + 1)]\n",
    "    l = ['-'.join(i) for i in l]\n",
    "    return l\n",
    "\n",
    "\n",
    "def grainToken(token, grainTokenFunction, Ngram = None, Min_Ngram = 1, Max_Ngram = 1, end_grain = True):\n",
    "    infos =  grainTokenFunction(token, end_grain = end_grain) \n",
    "    if Ngram: Min_Ngram, Max_Ngram = Ngram, Ngram\n",
    "    return sum([getGrainNgrams(infos, idx) for idx in range(Min_Ngram, Max_Ngram + 1)], [])\n",
    "\n",
    "\n",
    "def getChannelGrain4Token(token, channel, Ngram = None, Min_Ngram = 1, Max_Ngram = 1,  end_grain = False):\n",
    "    if channel == 'token':\n",
    "        return [token]\n",
    "    elif channel in Channel_Ind_Methods:\n",
    "        return grainToken(token, Channel_Ind_Methods[channel], Ngram = Ngram, Min_Ngram = Min_Ngram,Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "    else:\n",
    "        print('The Channel \"', channel, '\" is not available currently!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c117-c24', 'c24-c67', 'c67-c119', 'c119-c159']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "getChannelGrain4Token(token, channel, Ngram = 2, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bei', 'jng', 'Bei-jng']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijng'\n",
    "channel = 'syllable'\n",
    "getChannelGrain4Token(token, channel, Min_Ngram =1, Max_Ngram = 2,  end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijing'\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Ngram =1, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-EY2-ZH', 'EY2-ZH-IH1', 'ZH-IH1-NG', 'B-EY2-ZH-IH1', 'EY2-ZH-IH1-NG']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'Beijing'\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Min_Ngram = 3, Max_Ngram = 4, end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getChannelGrain4Token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'EY2', 'ZH', 'IH1', 'NG']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelGrain4Token\n",
    "\n",
    "token = 'Beijing'\n",
    "channel = 'phoneme'\n",
    "\n",
    "getChannelGrain4Token(token, channel, Ngram =1, Max_Ngram = None, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c117-c24', 'c24-c67', 'c67-c119', 'c119-c159']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = '北京'\n",
    "channel = 'subcomp'\n",
    "getChannelGrain4Token(token, channel, Ngram = 2, Max_Ngram = None,  end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `getChannelGrain4Sent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.channel import getChannelGrain4Sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '是', '中国', '的', '首都']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['c117-c24', 'c24-c67', 'c67-c119', 'c119-c159'],\n",
       " ['c209-c1', 'c1-c207'],\n",
       " ['c214-c120', 'c120-c173', 'c173-c6'],\n",
       " ['c331-c64', 'c64-c6'],\n",
       " ['c140-c392', 'c392-c180', 'c180-c209', 'c209-c166']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '北京 是 中国 的 首都'.split(' ')\n",
    "print(sent)\n",
    "channel = 'subcomp'\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, Ngram = 2, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beijing', 'is', 'the', 'capital', 'of', 'China']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['B', 'e', 'i', 'j', 'i', 'n', 'g', 'B-e', 'e-i', 'i-j', 'j-i', 'i-n', 'n-g'],\n",
       " ['i', 's', 'i-s'],\n",
       " ['t', 'h', 'e', 't-h', 'h-e'],\n",
       " ['c', 'a', 'p', 'i', 't', 'a', 'l', 'c-a', 'a-p', 'p-i', 'i-t', 't-a', 'a-l'],\n",
       " ['o', 'f', 'o-f'],\n",
       " ['C', 'h', 'i', 'n', 'a', 'C-h', 'h-i', 'i-n', 'n-a']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Beijing is the capital of China'.split(' ')\n",
    "print(sent)\n",
    "channel = 'char'\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, Max_Ngram = 2, end_grain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beijing', 'is', 'the', 'capital', 'of', 'China']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Bei', 'jing', 'Bei-jing'],\n",
       " ['is'],\n",
       " ['the'],\n",
       " ['cap', 'i', 'tal', 'cap-i', 'i-tal', 'cap-i-tal'],\n",
       " ['of'],\n",
       " ['Chi', 'na', 'Chi-na']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Beijing is the capital of China'.split(' ')\n",
    "print(sent)\n",
    "channel = 'syllable'\n",
    "\n",
    "getChannelGrain4Sent(sent, channel, Min_Ngram = 1,  Max_Ngram = 3, end_grain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get  Vocab and Freq of Sub Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################LTU_LGU-LT\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from nlptext.utils.channel import getChannelGrain4Token\n",
    "import collections\n",
    "\n",
    "\n",
    "def get_num_freq(idx2freq, min_token_freq = 1):\n",
    "    max_vocab_token_num = len(idx2freq[idx2freq >= min_token_freq])\n",
    "    return max_vocab_token_num\n",
    "\n",
    "def get_GU_or_LKP(TokenVocab, tkidx2freq, \n",
    "                  channel= 'char', Min_Ngram = 1, Max_Ngram = 1, end_grain = False, min_grain_freq = 1):\n",
    "\n",
    "    # ListGrainUnique = []\n",
    "    LTU, DTU = TokenVocab\n",
    "    # max_vocab_token_num = get_num_freq(tkidx2freq, min_token_freq = min_token_freq)\n",
    "    # LTU = LTU[:max_vocab_token_num]\n",
    "    \n",
    "    # the containers to store our results\n",
    "    oldLGU = []\n",
    "    oldDGU = {}\n",
    "    oldidx2freq = []\n",
    "    LKP = []\n",
    "    \n",
    "    print('For channel: |', channel, '| build GrainUnique and LookUp')\n",
    "    for idx, token in enumerate(LTU):\n",
    "        token_freq  = tkidx2freq[DTU[token]]\n",
    "        ChN = getChannelGrain4Token(token, channel, Min_Ngram = Min_Ngram, Max_Ngram = Max_Ngram, end_grain = end_grain)\n",
    "        grain2number = dict(collections.Counter(ChN).most_common())\n",
    "        for gr in grain2number:\n",
    "            if gr in oldDGU:\n",
    "                oldidx2freq[oldDGU[gr]] = oldidx2freq[oldDGU[gr]] + grain2number[gr] * token_freq\n",
    "            else:\n",
    "                oldDGU[gr] = len(oldDGU)\n",
    "                oldLGU.append(gr)\n",
    "                oldidx2freq.append(grain2number[gr] * token_freq)\n",
    "\n",
    "        LKP.append([oldDGU[gr] for gr in ChN])\n",
    "        if idx % 100000 == 0:\n",
    "            print('\\t\\tFor Channel:', channel, '\\t', idx, datetime.now())\n",
    "\n",
    "    # remove some high and low frequency grains.\n",
    "    # how to deal with the high freqency grains?\n",
    "    # notice that the grain freq is based on vocab instead of corpus.\n",
    "    assert len(LKP) == len(LTU)\n",
    "    \n",
    "    # sort the LGU, DGU and renew LKP\n",
    "    oldidx2freq = np.array(oldidx2freq)\n",
    "    max_grain_num = len(oldidx2freq[oldidx2freq >= min_grain_freq])\n",
    "    \n",
    "    del oldDGU \n",
    "    grainidx2freq = np.sort(oldidx2freq)[::-1]\n",
    "    newidx2oldidx = np.argsort(oldidx2freq)[::-1]\n",
    "    del oldidx2freq\n",
    "\n",
    "    oldidx2newidx = np.zeros(len(newidx2oldidx), dtype= int) \n",
    "    for new_idx, old_idx in enumerate(newidx2oldidx):\n",
    "        oldidx2newidx[old_idx] = new_idx\n",
    "    \n",
    "    for tkidx, grainlist in enumerate(LKP):\n",
    "        new_grainlist = []\n",
    "        for oldidx in grainlist:\n",
    "            newidx = oldidx2newidx[oldidx]\n",
    "            # throw away the low frequency grains\n",
    "            if grainidx2freq[newidx] < min_grain_freq:\n",
    "                continue\n",
    "            new_grainlist.append(newidx)\n",
    "        LKP[tkidx] = new_grainlist \n",
    "    del oldidx2newidx\n",
    "\n",
    "    LGU = []\n",
    "    for new_idx in range(max_grain_num):\n",
    "        # to filter some grains\n",
    "        LGU.append(oldLGU[newidx2oldidx[new_idx]])\n",
    "    del oldLGU\n",
    "    del newidx2oldidx\n",
    "\n",
    "    DGU = {}\n",
    "    for new_idx, token in enumerate(LGU):\n",
    "        DGU[token] = new_idx\n",
    "        \n",
    "    grainidx2freq = grainidx2freq[:max_grain_num]\n",
    "    \n",
    "    return (LGU, DGU), LKP, grainidx2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-07-03 11:04:09.787414\n"
     ]
    }
   ],
   "source": [
    "LTU = ['aaabb', 'abb', 'abc']\n",
    "DTU = {tk:idx for idx, tk in enumerate(LTU)}\n",
    "idx2freq = np.array([4, 2, 1])\n",
    "\n",
    "channel = 'char'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 2}\n",
    "\n",
    "\n",
    "(LGU, DGU), LKP, grainidx2freq = get_GU_or_LKP((LTU, DTU), idx2freq, \n",
    "                  channel= 'char', **channel_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'a-a', 'a-b', 'a-b-b', 'b-b', 'a-a-b', 'a-a-a']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'a-a': 2,\n",
       " 'a-b': 3,\n",
       " 'a-b-b': 4,\n",
       " 'b-b': 5,\n",
       " 'a-a-b': 6,\n",
       " 'a-a-a': 7}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DGU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 1, 2, 2, 3, 5, 7, 6, 4], [0, 1, 1, 3, 5, 4], [0, 1, 3]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 13,  8,  7,  6,  6,  4,  4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grainidx2freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Channel Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.utils.channel import CHANNEL_ABBR, CONTEXT_IND_CHANNELS\n",
    "\n",
    "###############################################################################################################\n",
    "def getChannelName(channel, Min_Ngram = 1, Max_Ngram = 1,  end_grain = False, tagScheme = 'BIO', min_grain_freq = 1,\n",
    "                   style = 'normal', channel_name = None, channel_name_abbr = None, **kwargs):\n",
    "    # hyper: channel + '-bio'\n",
    "    # sub  : channel + '-n1t5' + 'e' or '' + 'f5'\n",
    "    if style == 'normal':\n",
    "        if channel == 'token':\n",
    "            return channel \n",
    "        elif channel in CONTEXT_IND_CHANNELS:\n",
    "            MN = '-n' + str(Min_Ngram) + 't' + str(Max_Ngram)\n",
    "            e  = 'e' if end_grain else ''\n",
    "            f  = '-f' + str(min_grain_freq)\n",
    "            return channel + MN + e + f\n",
    "        else:\n",
    "            tS = '-' + tagScheme.lower() \n",
    "            return channel + tS\n",
    "\n",
    "    elif style == 'abbr':\n",
    "        channel_abbr = CHANNEL_ABBR[channel] \n",
    "        if channel == 'token':\n",
    "            return channel_abbr\n",
    "        elif channel in CONTEXT_IND_CHANNELS:\n",
    "            MN = '-n' + str(Min_Ngram) + 't' + str(Max_Ngram)\n",
    "            e  = 'e' if end_grain else ''\n",
    "            f  = '-f' + str(min_grain_freq)\n",
    "            return channel_abbr + MN + e + f\n",
    "        else:\n",
    "            tS = '-' + tagScheme.lower() \n",
    "            return channel_abbr + tS\n",
    "\n",
    "    elif channel_name and style == 'extract':\n",
    "        assert channel in channel_name\n",
    "        if channel == 'token':\n",
    "            return channel, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "\n",
    "        elif channel in CONTEXT_IND_CHANNELS:\n",
    "            channel_name, freq = channel_name.split('-f')\n",
    "            min_grain_freq = int(freq)\n",
    "            MN_e = channel_name.split('-n')[1]\n",
    "            # print(MN_e)\n",
    "            if 'e' in MN_e:\n",
    "                end_grain = True\n",
    "                MN = MN_e[:-1]\n",
    "            else:\n",
    "                end_grain = False\n",
    "                MN = MN_e\n",
    "            Min_Ngram, Max_Ngram = [int(i) for i in MN.split('t')]\n",
    "            return channel, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "        \n",
    "        else:\n",
    "            tagScheme = channel_name.split('-')[-1].upper()\n",
    "            return channel, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "        \n",
    "    elif channel_name_abbr and style == 'extract':\n",
    "        channel_abbr = CHANNEL_ABBR[channel]\n",
    "        assert channel_abbr in channel_name_abbr\n",
    "        if channel == 'token':\n",
    "            return channel_abbr, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "\n",
    "        elif channel in CONTEXT_IND_CHANNELS:\n",
    "            channel_name, freq = channel_name_abbr.split('-f')\n",
    "            min_grain_freq = int(freq)\n",
    "            MN_e = channel_name.split('-n') [1]\n",
    "            if 'e' in MN_e:\n",
    "                end_grain = True\n",
    "                MN = MN_e[:-1]\n",
    "            else:\n",
    "                end_grain = False\n",
    "                MN = MN_e\n",
    "            Min_Ngram, Max_Ngram = [int(i) for i in MN.split('t')]\n",
    "            return channel_abbr, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "        \n",
    "        else:\n",
    "            tagScheme = channel_name.split('-')[-1].upper()\n",
    "            return channel_abbr, Min_Ngram, Max_Ngram, end_grain, min_grain_freq, tagScheme\n",
    "    else:\n",
    "        print('Error in getChannelName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subcomp-n1t9e-f4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getChannelName(channel = 'subcomp', Min_Ngram = 1, Max_Ngram = 9,  end_grain = True, \n",
    "               tagScheme = 'BIO', min_grain_freq = 4,\n",
    "                   style = 'normal', channel_name = None, channel_name_abbr = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'char-n1t3-f2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "channel = 'char'\n",
    "channel_setting = {'Min_Ngram': 1, 'Max_Ngram': 3,'end_grain': False, 'min_grain_freq': 2}\n",
    "\n",
    "getChannelName(channel, **channel_setting)\n",
    "# print(GrainVocab[0])\n",
    "# print(GrainVocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c-n1t9e-f4'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getChannelName(channel = 'subcomp', Min_Ngram = 1, Max_Ngram = 9,  end_grain = True, \n",
    "               tagScheme = 'BIO', min_grain_freq = 4,\n",
    "                   style = 'abbr', channel_name = None, channel_name_abbr = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('subcomp', 1, 9, True, 4, 'BIO')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_name = 'subcomp-n1t9e-f4'\n",
    "getChannelName(channel = 'subcomp',\n",
    "                   style = 'extract', channel_name = channel_name, channel_name_abbr = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c', 1, 9, True, 4, 'BIO')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_name_abbr = 'c-n1t9e-f4'\n",
    "getChannelName(channel = 'subcomp',style = 'extract', channel_name = None, channel_name_abbr = channel_name_abbr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos-bio'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getChannelName(channel = 'pos', Min_Ngram = 1, Max_Ngram = 9,  end_grain = True, \n",
    "               tagScheme = 'BIO', min_grain_freq = 4,\n",
    "               style = 'normal', channel_name = None, channel_name_abbr = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 1, 1, False, 1, 'BIO')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_name = 'pos-bio'\n",
    "getChannelName(channel = 'pos',style = 'extract', channel_name = channel_name, channel_name_abbr = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
