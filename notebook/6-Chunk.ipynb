{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Initialized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.562 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Chunk Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nlptext.sentence import Sentence\n",
    "from nlptext.text import Text\n",
    "\n",
    "def get_chunk_info(BasicObject, Object, BATCH_MAX_NUM = 10000):\n",
    "    \n",
    "    if 'sent' in Object.lower():\n",
    "        Object = Sentence\n",
    "        Objects_Num = BasicObject.SENT['length']\n",
    "        \n",
    "    elif 'text' in Object.lower():\n",
    "        Object = Text\n",
    "        Objects_Num = BasicObject.TEXT['length']\n",
    "    else:\n",
    "        raise('No Such Object:', Object)\n",
    "        \n",
    "        \n",
    "    # channel = 'token' \n",
    "    # chunkidx_2_endbyteidx = [] \n",
    "    \n",
    "    chunkidx_2_endbyteidxs = {}\n",
    "    for channel in BasicObject.Channel_Hyper_Path:\n",
    "        chunkidx_2_endbyteidxs[channel] = []\n",
    "        \n",
    "    chunkidx_2_cumlengoftexts = []\n",
    "    # this can be removed\n",
    "    chunkidx_2_length_count = []\n",
    "\n",
    "\n",
    "    current_chunk_lengoftexts = []\n",
    "    current_chunk_length_count = 0\n",
    "    \n",
    "    for objectidx in range(Objects_Num):\n",
    "        # the objectidx is the locidx\n",
    "        text = Object(objectidx) \n",
    "        text_token_num = text.length\n",
    "        length_with_new_text = current_chunk_length_count + text_token_num\n",
    "        if  length_with_new_text > BATCH_MAX_NUM:\n",
    "            # pay attention to this operation\n",
    "            # this text is exclusive to current chunk, so the end of chunk is the start of this text\n",
    "            \n",
    "            # endbyteidx, _ = text.start_end_position(channel) \n",
    "            # chunkidx_2_endbyteidx.append(endbyteidx)\n",
    "            \n",
    "            for channel in chunkidx_2_endbyteidxs:\n",
    "                endbyteidx, _ = text.start_end_position(channel) \n",
    "                chunkidx_2_endbyteidxs[channel].append(endbyteidx)\n",
    "                \n",
    "            # also current_chunk_lengoftexts doesn't include the information of this text\n",
    "            chunkidx_2_cumlengoftexts.append(np.cumsum(current_chunk_lengoftexts))\n",
    "\n",
    "            # records the chunk length, this can be removed\n",
    "            chunkidx_2_length_count.append(current_chunk_length_count)\n",
    "\n",
    "            # in the following block, the new text matters.\n",
    "            # get the new the current chunk information\n",
    "            current_chunk_length_count = text_token_num\n",
    "            current_chunk_lengoftexts = [text_token_num]\n",
    "\n",
    "        else:\n",
    "            # update current_chunk_length_count when it is <= 10000\n",
    "            current_chunk_length_count = length_with_new_text\n",
    "\n",
    "            # append the object's length in the current_chunk_lengoftexts\n",
    "            current_chunk_lengoftexts.append(text_token_num)\n",
    "\n",
    "\n",
    "    # when the loop is over, we still need the to append \n",
    "    # the last small chunk into the total chunks.\n",
    "    # here, the object is the smallest object.\n",
    "\n",
    "    # notice, that this text is included in the last chuck\n",
    "    # _, endbyteidx = text.start_end_position(channel) \n",
    "    # chunkidx_2_endbyteidx.append(endbyteidx)\n",
    "    \n",
    "    for channel in chunkidx_2_endbyteidxs:\n",
    "        _, endbyteidx = text.start_end_position(channel) \n",
    "        chunkidx_2_endbyteidxs[channel].append(endbyteidx)\n",
    "                \n",
    "\n",
    "    # save the txtidx2endtokenidx, derived from lengoftexts\n",
    "    chunkidx_2_cumlengoftexts.append(np.cumsum(current_chunk_lengoftexts))\n",
    "\n",
    "    # records the chunk length, this can be removed\n",
    "    chunkidx_2_length_count.append(current_chunk_length_count)\n",
    "\n",
    "    return chunkidx_2_endbyteidxs, chunkidx_2_cumlengoftexts, chunkidx_2_length_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'token': [57833, 115292, 171624, 231761, 243352],\n",
       "  'pos': [35378, 70593, 105752, 141103, 147910]},\n",
       " [array([  79,  184,  291,  425,  562,  626,  686,  720,  782,  869,  980,\n",
       "         1177, 1227, 1320, 1438, 1558, 1708, 1834, 1969, 2075, 2243, 2454,\n",
       "         2571, 2587, 2650, 2690, 2760, 2871, 2990, 3013, 3026, 3074, 3293,\n",
       "         3451, 3581, 3622, 3748, 3923, 4008, 4080, 4196, 4205, 4238, 4290,\n",
       "         4327, 4363, 4408, 4428, 4442, 4478, 4544, 4635, 4705, 4797, 4875,\n",
       "         5059, 5088, 5194, 5325, 5451, 5561, 5672, 5702, 5758, 5835, 5918,\n",
       "         5950, 6104, 6146, 6260, 6391, 6478, 6613, 6656, 6782, 6832, 6837,\n",
       "         6945, 6988, 7055, 7206, 7304, 7349, 7422, 7580, 7623, 7711, 7794,\n",
       "         7804, 7835, 7912, 7930, 7965, 8018, 8092, 8144, 8198, 8245, 8372,\n",
       "         8414, 8459, 8484, 8555, 8564, 8572, 8575, 8578, 8581, 8586, 8588,\n",
       "         8591, 8593, 8596, 8599, 8602, 8604, 8606, 8610, 8613, 8616, 8618,\n",
       "         8620, 8622, 8704, 8985, 9095, 9152, 9226, 9378, 9521, 9632, 9687,\n",
       "         9793, 9837, 9877, 9895, 9965, 9986]),\n",
       "  array([ 108,  173,  505,  608,  664,  804,  897,  944, 1029, 1051, 1068,\n",
       "         1149, 1201, 1255, 1335, 1406, 1471, 1530, 1693, 1828, 1965, 2048,\n",
       "         2119, 2145, 2237, 2278, 2296, 2506, 2591, 2900, 3027, 3080, 3258,\n",
       "         3478, 3616, 3776, 3828, 3850, 3984, 4054, 4309, 4442, 4519, 4634,\n",
       "         4760, 4890, 4973, 5000, 5029, 5166, 5190, 5265, 5364, 5385, 5466,\n",
       "         5539, 5678, 5736, 5802, 5885, 5899, 5975, 6009, 6057, 6107, 6145,\n",
       "         6267, 6355, 6398, 6443, 6531, 6587, 6628, 6801, 6831, 6841, 6973,\n",
       "         7093, 7275, 7423, 7595, 7814, 8060, 8199, 8384, 8549, 8656, 8810,\n",
       "         8899, 9115, 9237, 9350, 9455, 9547, 9575, 9584, 9596, 9694, 9858,\n",
       "         9882, 9982], dtype=uint64),\n",
       "  array([  34,  158,  286,  358,  383,  429,  549,  568,  624,  661,  670,\n",
       "          762,  888,  943,  950, 1067, 1095, 1149, 1266, 1329, 1347, 1406,\n",
       "         1511, 1533, 1648, 1671, 1712, 1744, 1804, 1821, 1854, 1917, 1948,\n",
       "         2043, 2102, 2114, 2206, 2266, 2305, 2506, 2603, 2667, 2681, 2733,\n",
       "         2757, 2777, 2892, 2912, 2995, 3015, 3108, 3128, 3137, 3191, 3241,\n",
       "         3302, 3390, 3438, 3627, 3712, 3773, 3880, 3980, 4068, 4144, 4186,\n",
       "         4275, 4422, 4511, 4706, 4960, 5005, 5145, 5323, 5404, 5440, 5469,\n",
       "         5530, 5554, 5642, 5716, 5855, 6017, 6057, 6121, 6126, 6278, 6354,\n",
       "         6383, 6429, 6460, 6553, 6614, 6692, 6744, 6792, 6810, 6906, 6967,\n",
       "         7064, 7110, 7183, 7186, 7259, 7265, 7268, 7344, 7433, 7511, 7582,\n",
       "         7674, 7737, 7779, 7855, 7952, 8017, 8096, 8152, 8236, 8279, 8308,\n",
       "         8409, 8446, 8486, 8522, 8642, 8720, 8855, 9046, 9130, 9242, 9402,\n",
       "         9463, 9545, 9648, 9830, 9963], dtype=uint64),\n",
       "  array([  38,  205,  388,  495,  559,  683,  801, 1025, 1150, 1242, 1350,\n",
       "         1551, 1666, 1739, 1832, 1911, 2000, 2087, 2280, 2353, 2609, 2648,\n",
       "         2678, 2729, 2761, 2861, 2969, 3060, 3154, 3215, 3288, 3333, 3360,\n",
       "         3424, 3465, 3607, 3718, 3813, 3912, 4154, 4269, 4371, 4401, 4532,\n",
       "         4664, 4738, 4782, 4838, 4915, 4971, 5045, 5116, 5227, 5319, 5417,\n",
       "         5561, 5664, 5721, 5812, 5883, 5934, 5968, 6031, 6110, 6185, 6284,\n",
       "         6388, 6486, 6530, 6593, 6723, 6799, 7004, 7144, 7244, 7372, 7448,\n",
       "         7544, 7655, 7766, 7880, 7997, 8060, 8127, 8242, 8302, 8355, 8431,\n",
       "         8586, 8664, 8739, 8790, 8851, 9104, 9157, 9263, 9356, 9477, 9608,\n",
       "         9745, 9808, 9985], dtype=uint64),\n",
       "  array([  71,  220,  337,  374,  448,  528,  620,  748,  830,  971, 1049,\n",
       "         1153, 1255, 1420, 1515, 1533, 1645, 1663, 1751, 1819, 1879, 1927],\n",
       "        dtype=uint64)],\n",
       " [9986, 9982, 9963, 9985, 1927])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "Object = 'text'\n",
    "get_chunk_info(BasicObject, Object, BATCH_MAX_NUM = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': [57833, 115292, 171624, 231761, 243352],\n",
       " 'pos': [35378, 70593, 105752, 141103, 147910]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkidx_2_endbyteidxs, chunkidx_2_cumlengoftexts, chunkidx_2_length_count = get_chunk_info(BasicObject, Object, BATCH_MAX_NUM = 10000)\n",
    "\n",
    "chunkidx_2_endbyteidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  79,  184,  291,  425,  562,  626,  686,  720,  782,  869,  980,\n",
       "        1177, 1227, 1320, 1438, 1558, 1708, 1834, 1969, 2075, 2243, 2454,\n",
       "        2571, 2587, 2650, 2690, 2760, 2871, 2990, 3013, 3026, 3074, 3293,\n",
       "        3451, 3581, 3622, 3748, 3923, 4008, 4080, 4196, 4205, 4238, 4290,\n",
       "        4327, 4363, 4408, 4428, 4442, 4478, 4544, 4635, 4705, 4797, 4875,\n",
       "        5059, 5088, 5194, 5325, 5451, 5561, 5672, 5702, 5758, 5835, 5918,\n",
       "        5950, 6104, 6146, 6260, 6391, 6478, 6613, 6656, 6782, 6832, 6837,\n",
       "        6945, 6988, 7055, 7206, 7304, 7349, 7422, 7580, 7623, 7711, 7794,\n",
       "        7804, 7835, 7912, 7930, 7965, 8018, 8092, 8144, 8198, 8245, 8372,\n",
       "        8414, 8459, 8484, 8555, 8564, 8572, 8575, 8578, 8581, 8586, 8588,\n",
       "        8591, 8593, 8596, 8599, 8602, 8604, 8606, 8610, 8613, 8616, 8618,\n",
       "        8620, 8622, 8704, 8985, 9095, 9152, 9226, 9378, 9521, 9632, 9687,\n",
       "        9793, 9837, 9877, 9895, 9965, 9986]),\n",
       " array([ 108,  173,  505,  608,  664,  804,  897,  944, 1029, 1051, 1068,\n",
       "        1149, 1201, 1255, 1335, 1406, 1471, 1530, 1693, 1828, 1965, 2048,\n",
       "        2119, 2145, 2237, 2278, 2296, 2506, 2591, 2900, 3027, 3080, 3258,\n",
       "        3478, 3616, 3776, 3828, 3850, 3984, 4054, 4309, 4442, 4519, 4634,\n",
       "        4760, 4890, 4973, 5000, 5029, 5166, 5190, 5265, 5364, 5385, 5466,\n",
       "        5539, 5678, 5736, 5802, 5885, 5899, 5975, 6009, 6057, 6107, 6145,\n",
       "        6267, 6355, 6398, 6443, 6531, 6587, 6628, 6801, 6831, 6841, 6973,\n",
       "        7093, 7275, 7423, 7595, 7814, 8060, 8199, 8384, 8549, 8656, 8810,\n",
       "        8899, 9115, 9237, 9350, 9455, 9547, 9575, 9584, 9596, 9694, 9858,\n",
       "        9882, 9982], dtype=uint64),\n",
       " array([  34,  158,  286,  358,  383,  429,  549,  568,  624,  661,  670,\n",
       "         762,  888,  943,  950, 1067, 1095, 1149, 1266, 1329, 1347, 1406,\n",
       "        1511, 1533, 1648, 1671, 1712, 1744, 1804, 1821, 1854, 1917, 1948,\n",
       "        2043, 2102, 2114, 2206, 2266, 2305, 2506, 2603, 2667, 2681, 2733,\n",
       "        2757, 2777, 2892, 2912, 2995, 3015, 3108, 3128, 3137, 3191, 3241,\n",
       "        3302, 3390, 3438, 3627, 3712, 3773, 3880, 3980, 4068, 4144, 4186,\n",
       "        4275, 4422, 4511, 4706, 4960, 5005, 5145, 5323, 5404, 5440, 5469,\n",
       "        5530, 5554, 5642, 5716, 5855, 6017, 6057, 6121, 6126, 6278, 6354,\n",
       "        6383, 6429, 6460, 6553, 6614, 6692, 6744, 6792, 6810, 6906, 6967,\n",
       "        7064, 7110, 7183, 7186, 7259, 7265, 7268, 7344, 7433, 7511, 7582,\n",
       "        7674, 7737, 7779, 7855, 7952, 8017, 8096, 8152, 8236, 8279, 8308,\n",
       "        8409, 8446, 8486, 8522, 8642, 8720, 8855, 9046, 9130, 9242, 9402,\n",
       "        9463, 9545, 9648, 9830, 9963], dtype=uint64),\n",
       " array([  38,  205,  388,  495,  559,  683,  801, 1025, 1150, 1242, 1350,\n",
       "        1551, 1666, 1739, 1832, 1911, 2000, 2087, 2280, 2353, 2609, 2648,\n",
       "        2678, 2729, 2761, 2861, 2969, 3060, 3154, 3215, 3288, 3333, 3360,\n",
       "        3424, 3465, 3607, 3718, 3813, 3912, 4154, 4269, 4371, 4401, 4532,\n",
       "        4664, 4738, 4782, 4838, 4915, 4971, 5045, 5116, 5227, 5319, 5417,\n",
       "        5561, 5664, 5721, 5812, 5883, 5934, 5968, 6031, 6110, 6185, 6284,\n",
       "        6388, 6486, 6530, 6593, 6723, 6799, 7004, 7144, 7244, 7372, 7448,\n",
       "        7544, 7655, 7766, 7880, 7997, 8060, 8127, 8242, 8302, 8355, 8431,\n",
       "        8586, 8664, 8739, 8790, 8851, 9104, 9157, 9263, 9356, 9477, 9608,\n",
       "        9745, 9808, 9985], dtype=uint64),\n",
       " array([  71,  220,  337,  374,  448,  528,  620,  748,  830,  971, 1049,\n",
       "        1153, 1255, 1420, 1515, 1533, 1645, 1663, 1751, 1819, 1879, 1927],\n",
       "       dtype=uint64)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkidx_2_cumlengoftexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9986, 9986), (9982, 9982), (9963, 9963), (9985, 9985), (1927, 1927)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_count = [i[-1] for i in chunkidx_2_cumlengoftexts]\n",
    "list(zip(new_count, chunkidx_2_length_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wiki/word/Pyramid/_file/token.txt\n"
     ]
    }
   ],
   "source": [
    "path = BasicObject.Channel_Hyper_Path['token']\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实用 经济学 理 对于 公共政策 的 研究 大多 聚焦 于 如何 增进 一个 经济 的 效率 , 找出 如何 调整 社会 组织 以 达成 最 有效 的 资源 生产 , 被 认为 是 「 经济学 的 本质 」\n",
      "专精 是 一种 无论 在 经济 理论 上 和 观察 上 都 已经 被 认定 是 经济 效率 重要 来源 的 现象 , 不同 的 个体 和 国家 可能 对 生产 各种 产品 和 服务 有 不同 ....\n"
     ]
    }
   ],
   "source": [
    "# for example, let's check a chunk with its information\n",
    "from nlptext.utils.pyramid import read_file_chunk_string\n",
    "import re\n",
    "\n",
    "chunk_idx = 3\n",
    "channel = 'token'\n",
    "\n",
    "\n",
    "start_position = 0 if chunk_idx == 0 else chunkidx_2_endbyteidxs[channel][chunk_idx - 1] \n",
    "end_postion = chunkidx_2_endbyteidxs[channel][chunk_idx] \n",
    "\n",
    "chunk_string = read_file_chunk_string(path, start_position, end_postion)\n",
    "print(chunk_string[:200] + '....')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9985\n",
      "9985\n"
     ]
    }
   ],
   "source": [
    "print(len(re.split(' |\\n', chunk_string)))\n",
    "print(chunkidx_2_length_count[chunk_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
