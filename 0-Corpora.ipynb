{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese Sample \n",
    "\n",
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.867 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n",
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594589\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "min_token_freq = 10\n",
    "\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'许 多 哲 学 家 相 信 数 学 在 经 验 上 不 具 可 否 证 性 , 且 因 此 不 是 卡 尔 · 波 普 尔 所 定 义 的 科 学 。 但 在 1 9 3 0 年 代 时 , 在 数 理 逻 辑 上 的 重 大 进 展 显 示 数 学 不 能 归 并 至 逻 辑 内 , 且 波 普 尔 推 断 「 大 部 份 的 数 学 定 律 , 如 物 理 及 生 物 学 一 样 , 是 假 设 演 绎 的 : 纯 数 学 因 此 变 得 更 接 近 其 假 设 为 猜 测 的 自 然 科 学 , 比 它 现 在 看 起 来 更 接 近 。 」 然 而 , 其 他 的 思 想 家 , 如 较 著 名 的 拉 卡 托 斯 , 便 提 供 了 一 个 关 于 数 学 本 身 的 可 否 证 性 版 本 。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 19\n",
    "obj = Sentence(locidx)\n",
    "obj.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254754734\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/word/'\n",
    "min_token_freq = 10\n",
    "\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'许多 哲学家 相信 数学 在 经验 上 不具 可否 证性 , 且 因此 不是 卡尔 · 波普尔 所 定义 的 科学 。 但 在 1930 年代 时 , 在 数理逻辑 上 的 重大进展 显示 数学 不能 归并 至 逻辑 内 , 且 波普尔 推断 「 大部份 的 数学 定律 , 如 物理 及 生物学 一样 , 是 假设 演绎 的 : 纯数学 因此 变得 更 接近 其 假设 为 猜测 的 自然科学 , 比 它 现在 看起来 更 接近 。 」 然而 , 其他 的 思想家 , 如 较 著名 的 拉 卡托斯 , 便 提供 了 一个 关于 数学 本身 的 可否 证性 版本 。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 19\n",
    "obj = Sentence(locidx)\n",
    "obj.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_en_sample/wiki_en_sample.txt\n",
      "Total Num of All    Tokens 47166\n",
      "Total Num of Unique Tokens 8620\n",
      "CORPUS\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 47166\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/wiki_en_sample/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_en_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 8620\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_en_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/WikiEnglish/wiki_en.txt\n",
      "100000 -- 1 774 2019-07-19 19:24:13.705785\n",
      "200000 -- 1 123 2019-07-19 19:30:13.092273\n",
      "300000 -- 1 891 2019-07-19 19:36:28.051328\n",
      "400000 -- 1 515 2019-07-19 19:42:43.424323\n",
      "500000 -- 1 369 2019-07-19 19:47:54.881302\n",
      "600000 -- 1 1157 2019-07-19 19:53:06.361913\n",
      "700000 -- 1 442 2019-07-19 19:58:12.257847\n",
      "800000 -- 1 137 2019-07-19 20:03:30.904199\n",
      "900000 -- 1 62 2019-07-19 20:08:38.107492\n",
      "1000000 -- 1 281 2019-07-19 20:13:30.982384\n",
      "1100000 -- 1 586 2019-07-19 20:18:30.864032\n",
      "1200000 -- 1 703 2019-07-19 20:23:29.207500\n",
      "1300000 -- 1 289 2019-07-19 20:27:52.216206\n",
      "1400000 -- 1 519 2019-07-19 20:32:10.637779\n",
      "1500000 -- 1 497 2019-07-19 20:35:51.995841\n",
      "1600000 -- 1 107 2019-07-19 20:39:25.707543\n",
      "1700000 -- 1 174 2019-07-19 20:43:09.481530\n",
      "1800000 -- 1 91 2019-07-19 20:46:50.461888\n",
      "1900000 -- 1 439 2019-07-19 20:51:36.862276\n",
      "2000000 -- 1 517 2019-07-19 20:56:27.465919\n",
      "2100000 -- 1 1068 2019-07-19 21:01:26.105536\n",
      "2200000 -- 1 412 2019-07-19 21:06:10.757536\n",
      "2300000 -- 1 97 2019-07-19 21:10:57.649080\n",
      "2400000 -- 1 1249 2019-07-19 21:15:53.999138\n",
      "2500000 -- 1 52 2019-07-19 21:20:33.468414\n",
      "2600000 -- 1 415 2019-07-19 21:25:24.400295\n",
      "2700000 -- 1 160 2019-07-19 21:30:10.140468\n",
      "2800000 -- 1 610 2019-07-19 21:34:50.033871\n",
      "2900000 -- 1 189 2019-07-19 21:39:31.865663\n",
      "3000000 -- 1 106 2019-07-19 21:44:19.450269\n",
      "3100000 -- 1 236 2019-07-19 21:48:54.565162\n",
      "3200000 -- 1 333 2019-07-19 21:53:36.141433\n",
      "3300000 -- 1 53 2019-07-19 21:58:20.913018\n",
      "3400000 -- 1 422 2019-07-19 22:02:58.265015\n",
      "3500000 -- 1 252 2019-07-19 22:07:26.291757\n",
      "3600000 -- 1 814 2019-07-19 22:11:59.593964\n",
      "3700000 -- 1 100 2019-07-19 22:16:38.056986\n",
      "3800000 -- 1 312 2019-07-19 22:21:13.535446\n",
      "3900000 -- 1 338 2019-07-19 22:25:46.794070\n",
      "4000000 -- 1 661 2019-07-19 22:30:18.515220\n",
      "4100000 -- 1 312 2019-07-19 22:34:55.880767\n",
      "4200000 -- 1 193 2019-07-19 22:39:28.922921\n",
      "4300000 -- 1 247 2019-07-19 22:44:01.514168\n",
      "4400000 -- 1 273 2019-07-19 22:48:35.089905\n",
      "4500000 -- 1 103 2019-07-19 22:53:04.017607\n",
      "4600000 -- 1 69 2019-07-19 22:57:31.322003\n",
      "4700000 -- 1 180 2019-07-19 23:01:53.523570\n",
      "4800000 -- 1 600 2019-07-19 23:06:24.775935\n",
      "4900000 -- 1 324 2019-07-19 23:10:58.063650\n",
      "5000000 -- 1 447 2019-07-19 23:15:29.410242\n",
      "5100000 -- 1 400 2019-07-19 23:19:56.360718\n",
      "5200000 -- 1 676 2019-07-19 23:24:24.451658\n",
      "5300000 -- 1 2843 2019-07-19 23:28:52.244439\n",
      "5400000 -- 1 322 2019-07-19 23:33:16.574802\n",
      "5500000 -- 1 57 2019-07-19 23:37:50.078648\n",
      "5600000 -- 1 321 2019-07-19 23:42:11.322300\n",
      "5700000 -- 1 465 2019-07-19 23:46:38.957889\n",
      "5800000 -- 1 558 2019-07-19 23:51:05.191768\n",
      "5900000 -- 1 569 2019-07-19 23:55:26.588338\n",
      "6000000 -- 1 260 2019-07-19 23:59:50.398998\n",
      "6100000 -- 1 988 2019-07-20 00:04:09.884134\n",
      "6200000 -- 1 210 2019-07-20 00:08:35.037489\n",
      "6300000 -- 1 739 2019-07-20 00:13:01.521263\n",
      "6400000 -- 1 269 2019-07-20 00:17:28.029451\n",
      "6500000 -- 1 319 2019-07-20 00:21:48.209124\n",
      "6600000 -- 1 829 2019-07-20 00:26:07.677893\n",
      "6700000 -- 1 334 2019-07-20 00:30:20.963921\n",
      "6800000 -- 1 296 2019-07-20 00:34:39.764021\n",
      "6900000 -- 1 288 2019-07-20 00:38:55.371609\n",
      "7000000 -- 1 134 2019-07-20 00:43:15.255456\n",
      "7100000 -- 1 404 2019-07-20 00:47:38.858380\n",
      "7200000 -- 1 72 2019-07-20 00:51:59.081838\n",
      "7300000 -- 1 73 2019-07-20 00:56:13.572566\n",
      "7400000 -- 1 398 2019-07-20 01:00:18.614961\n",
      "7500000 -- 1 80 2019-07-20 01:04:28.513872\n",
      "7600000 -- 1 291 2019-07-20 01:08:44.647491\n",
      "7700000 -- 1 603 2019-07-20 01:13:06.729246\n",
      "7800000 -- 1 630 2019-07-20 01:17:25.385406\n",
      "7900000 -- 1 187 2019-07-20 01:21:40.658175\n",
      "8000000 -- 1 257 2019-07-20 01:25:55.499749\n",
      "8100000 -- 1 383 2019-07-20 01:30:07.522415\n",
      "8200000 -- 1 811 2019-07-20 01:34:23.357084\n",
      "8300000 -- 1 533 2019-07-20 01:38:39.570446\n",
      "8400000 -- 1 564 2019-07-20 01:42:56.548561\n",
      "8500000 -- 1 446 2019-07-20 01:47:09.905396\n",
      "8600000 -- 1 294 2019-07-20 01:51:25.023560\n",
      "8700000 -- 1 1449 2019-07-20 01:55:41.723460\n",
      "8800000 -- 1 152 2019-07-20 01:59:55.749147\n",
      "8900000 -- 1 30 2019-07-20 02:04:05.601142\n",
      "9000000 -- 1 88 2019-07-20 02:08:20.127949\n",
      "9100000 -- 1 147 2019-07-20 02:12:30.411700\n",
      "9200000 -- 1 194 2019-07-20 02:16:41.502321\n",
      "9300000 -- 1 386 2019-07-20 02:20:53.750835\n",
      "9400000 -- 1 550 2019-07-20 02:25:03.498246\n",
      "9500000 -- 1 170 2019-07-20 02:29:12.044441\n",
      "9600000 -- 1 190 2019-07-20 02:33:17.884254\n",
      "9700000 -- 1 251 2019-07-20 02:37:29.971621\n",
      "9800000 -- 1 265 2019-07-20 02:41:34.635959\n",
      "9900000 -- 1 527 2019-07-20 02:45:38.597468\n",
      "10000000 -- 1 7 2019-07-20 02:49:42.724088\n",
      "10100000 -- 1 1073 2019-07-20 02:53:45.752869\n",
      "10200000 -- 1 15 2019-07-20 02:57:44.997770\n",
      "10300000 -- 1 481 2019-07-20 03:01:51.604526\n",
      "10400000 -- 1 384 2019-07-20 03:05:56.931304\n",
      "10500000 -- 1 27 2019-07-20 03:10:04.703829\n",
      "10600000 -- 1 781 2019-07-20 03:14:08.430210\n",
      "10700000 -- 1 640 2019-07-20 03:18:16.788578\n",
      "10800000 -- 1 154 2019-07-20 03:22:23.886715\n",
      "10900000 -- 1 187 2019-07-20 03:26:27.963982\n",
      "11000000 -- 1 365 2019-07-20 03:30:35.144714\n",
      "11100000 -- 1 10 2019-07-20 03:34:37.173410\n",
      "11200000 -- 1 236 2019-07-20 03:38:46.594977\n",
      "11300000 -- 1 354 2019-07-20 03:42:48.051588\n",
      "11400000 -- 1 234 2019-07-20 03:46:50.747081\n",
      "11500000 -- 1 271 2019-07-20 03:50:55.430589\n",
      "11600000 -- 1 191 2019-07-20 03:55:06.270409\n",
      "11700000 -- 1 222 2019-07-20 03:59:09.605932\n",
      "11800000 -- 1 697 2019-07-20 04:03:12.595747\n",
      "11900000 -- 1 91 2019-07-20 04:07:23.416377\n",
      "12000000 -- 1 159 2019-07-20 04:11:25.882462\n",
      "12100000 -- 1 389 2019-07-20 04:15:29.524488\n",
      "12200000 -- 1 115 2019-07-20 04:19:30.328035\n",
      "12300000 -- 1 388 2019-07-20 04:23:33.687258\n",
      "12400000 -- 1 242 2019-07-20 04:27:29.334612\n",
      "12500000 -- 1 558 2019-07-20 04:31:22.638690\n",
      "12600000 -- 1 229 2019-07-20 04:35:08.062838\n",
      "12700000 -- 1 635 2019-07-20 04:39:00.867137\n",
      "12800000 -- 1 250 2019-07-20 04:42:59.832164\n",
      "12900000 -- 1 467 2019-07-20 04:46:56.188220\n",
      "13000000 -- 1 435 2019-07-20 04:50:46.197580\n",
      "13100000 -- 1 214 2019-07-20 04:54:45.968790\n",
      "13200000 -- 1 196 2019-07-20 04:58:46.791722\n",
      "13300000 -- 1 29 2019-07-20 05:02:43.161856\n",
      "13400000 -- 1 822 2019-07-20 05:06:31.598255\n",
      "13500000 -- 1 10 2019-07-20 05:10:25.041023\n",
      "13600000 -- 1 882 2019-07-20 05:14:26.117158\n",
      "13700000 -- 1 137 2019-07-20 05:18:21.818711\n",
      "13800000 -- 1 61 2019-07-20 05:22:19.612231\n",
      "13900000 -- 1 131 2019-07-20 05:26:10.942683\n",
      "14000000 -- 1 1022 2019-07-20 05:30:10.579818\n",
      "14100000 -- 1 156 2019-07-20 05:34:10.496230\n",
      "14200000 -- 1 63 2019-07-20 05:37:59.598007\n",
      "14300000 -- 1 146 2019-07-20 05:41:54.686640\n",
      "14400000 -- 1 646 2019-07-20 05:45:50.293251\n",
      "14500000 -- 1 276 2019-07-20 05:49:48.977404\n",
      "14600000 -- 1 1064 2019-07-20 05:53:47.323624\n",
      "14700000 -- 1 219 2019-07-20 05:57:43.467308\n",
      "14800000 -- 1 53 2019-07-20 06:01:43.169838\n",
      "14900000 -- 1 411 2019-07-20 06:05:43.036100\n",
      "15000000 -- 1 177 2019-07-20 06:09:41.789115\n",
      "15100000 -- 1 835 2019-07-20 06:13:33.050816\n",
      "15200000 -- 1 389 2019-07-20 06:17:26.577362\n",
      "15300000 -- 1 221 2019-07-20 06:21:18.112218\n",
      "15400000 -- 1 480 2019-07-20 06:25:12.838808\n",
      "15500000 -- 1 106 2019-07-20 06:29:10.188230\n",
      "15600000 -- 1 202 2019-07-20 06:32:56.863714\n",
      "15700000 -- 1 455 2019-07-20 06:36:50.146301\n",
      "15800000 -- 1 11 2019-07-20 06:40:46.271814\n",
      "15900000 -- 1 806 2019-07-20 06:44:38.734590\n",
      "16000000 -- 1 296 2019-07-20 06:48:03.150618\n",
      "16100000 -- 1 42 2019-07-20 06:51:57.132588\n",
      "16200000 -- 1 341 2019-07-20 06:55:53.085756\n",
      "16300000 -- 1 163 2019-07-20 06:59:46.423444\n",
      "16400000 -- 1 45 2019-07-20 07:03:14.054409\n",
      "16500000 -- 1 185 2019-07-20 07:07:01.010511\n",
      "16600000 -- 1 251 2019-07-20 07:10:48.434553\n",
      "16700000 -- 1 20 2019-07-20 07:14:38.794930\n",
      "16800000 -- 1 306 2019-07-20 07:18:20.762624\n",
      "16900000 -- 1 372 2019-07-20 07:21:56.985797\n",
      "17000000 -- 1 294 2019-07-20 07:25:30.180484\n",
      "17100000 -- 1 233 2019-07-20 07:29:15.688224\n",
      "17200000 -- 1 73 2019-07-20 07:32:57.613442\n",
      "17300000 -- 1 1246 2019-07-20 07:36:41.971796\n",
      "17400000 -- 1 36 2019-07-20 07:40:21.599372\n",
      "17500000 -- 1 681 2019-07-20 07:44:09.092388\n",
      "17600000 -- 1 92 2019-07-20 07:47:57.461939\n",
      "17700000 -- 1 233 2019-07-20 07:51:35.689911\n",
      "17800000 -- 1 320 2019-07-20 07:55:14.798856\n",
      "17900000 -- 1 139 2019-07-20 07:59:05.978317\n",
      "18000000 -- 1 247 2019-07-20 08:02:58.630541\n",
      "18100000 -- 1 138 2019-07-20 08:06:50.705748\n",
      "18200000 -- 1 981 2019-07-20 08:10:38.627445\n",
      "18300000 -- 1 56 2019-07-20 08:14:22.726181\n",
      "18400000 -- 1 326 2019-07-20 08:18:07.021124\n",
      "18500000 -- 1 501 2019-07-20 08:21:47.027775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18600000 -- 1 14 2019-07-20 08:25:25.267355\n",
      "18700000 -- 1 197 2019-07-20 08:28:59.449989\n",
      "18800000 -- 1 116 2019-07-20 08:32:41.321801\n",
      "18900000 -- 1 292 2019-07-20 08:36:20.771394\n",
      "19000000 -- 1 281 2019-07-20 08:40:03.198812\n",
      "19100000 -- 1 145 2019-07-20 08:43:47.789213\n",
      "19200000 -- 1 478 2019-07-20 08:47:32.958196\n",
      "19300000 -- 1 84 2019-07-20 08:51:23.657478\n",
      "19400000 -- 1 238 2019-07-20 08:55:14.675333\n",
      "19500000 -- 1 1090 2019-07-20 08:59:04.696768\n",
      "19600000 -- 1 135 2019-07-20 09:02:57.176857\n",
      "19700000 -- 1 174 2019-07-20 09:06:37.698661\n",
      "19800000 -- 1 206 2019-07-20 09:10:21.102127\n",
      "19900000 -- 1 417 2019-07-20 09:14:01.224369\n",
      "20000000 -- 1 25 2019-07-20 09:17:46.340956\n",
      "20100000 -- 1 258 2019-07-20 09:21:28.299771\n",
      "20200000 -- 1 470 2019-07-20 09:25:13.969078\n",
      "20300000 -- 1 547 2019-07-20 09:29:06.357159\n",
      "20400000 -- 1 317 2019-07-20 09:33:05.277416\n",
      "20500000 -- 1 597 2019-07-20 09:36:52.946722\n",
      "20600000 -- 1 114 2019-07-20 09:40:34.060832\n",
      "20700000 -- 1 1144 2019-07-20 09:44:23.281695\n",
      "20800000 -- 1 388 2019-07-20 09:48:14.400664\n",
      "20900000 -- 1 94 2019-07-20 09:52:07.617299\n",
      "21000000 -- 1 209 2019-07-20 09:55:56.981353\n",
      "21100000 -- 1 546 2019-07-20 09:59:52.207824\n",
      "21200000 -- 1 728 2019-07-20 10:03:42.809163\n",
      "21300000 -- 1 454 2019-07-20 10:07:31.537997\n",
      "21400000 -- 1 681 2019-07-20 10:11:19.189991\n",
      "21500000 -- 1 277 2019-07-20 10:15:01.028607\n",
      "21600000 -- 1 533 2019-07-20 10:18:37.559466\n",
      "21700000 -- 1 277 2019-07-20 10:22:11.289292\n",
      "21800000 -- 1 21 2019-07-20 10:25:57.505223\n",
      "21900000 -- 1 890 2019-07-20 10:29:40.689997\n",
      "22000000 -- 1 358 2019-07-20 10:33:25.048208\n",
      "22100000 -- 1 33 2019-07-20 10:37:12.473316\n",
      "22200000 -- 1 461 2019-07-20 10:40:56.612253\n",
      "22300000 -- 1 239 2019-07-20 10:44:48.962537\n",
      "22400000 -- 1 248 2019-07-20 10:48:32.917030\n",
      "22500000 -- 1 381 2019-07-20 10:52:17.122972\n",
      "22600000 -- 1 314 2019-07-20 10:56:05.277831\n",
      "22700000 -- 1 89 2019-07-20 10:59:37.756541\n",
      "22800000 -- 1 603 2019-07-20 11:03:31.434913\n",
      "22900000 -- 1 279 2019-07-20 11:07:20.103467\n",
      "23000000 -- 1 865 2019-07-20 11:11:07.622973\n",
      "23100000 -- 1 319 2019-07-20 11:14:58.466647\n",
      "23200000 -- 1 384 2019-07-20 11:18:49.447474\n",
      "23300000 -- 1 94 2019-07-20 11:22:48.050930\n",
      "23400000 -- 1 155 2019-07-20 11:26:26.483033\n",
      "23500000 -- 1 328 2019-07-20 11:30:15.251032\n",
      "23600000 -- 1 163 2019-07-20 11:34:05.865649\n",
      "23700000 -- 1 638 2019-07-20 11:37:49.533574\n",
      "23800000 -- 1 284 2019-07-20 11:41:40.651095\n",
      "23900000 -- 1 71 2019-07-20 11:45:30.070125\n",
      "24000000 -- 1 18 2019-07-20 11:49:12.797334\n",
      "24100000 -- 1 133 2019-07-20 11:52:47.052878\n",
      "24200000 -- 1 90 2019-07-20 11:56:26.775805\n",
      "24300000 -- 1 311 2019-07-20 12:00:13.248138\n",
      "24400000 -- 1 20 2019-07-20 12:04:08.033909\n",
      "24500000 -- 1 355 2019-07-20 12:07:57.327268\n",
      "24600000 -- 1 209 2019-07-20 12:11:41.588416\n",
      "24700000 -- 1 499 2019-07-20 12:15:25.445777\n",
      "24800000 -- 1 282 2019-07-20 12:19:06.449814\n",
      "24900000 -- 1 1042 2019-07-20 12:22:51.596660\n",
      "25000000 -- 1 374 2019-07-20 12:26:39.434275\n",
      "25100000 -- 1 166 2019-07-20 12:30:27.068611\n",
      "25200000 -- 1 141 2019-07-20 12:34:13.802904\n",
      "25300000 -- 1 346 2019-07-20 12:38:03.963385\n",
      "25400000 -- 1 392 2019-07-20 12:41:50.908606\n",
      "25500000 -- 1 104 2019-07-20 12:45:40.265740\n",
      "25600000 -- 1 478 2019-07-20 12:49:28.956392\n",
      "25700000 -- 1 1264 2019-07-20 12:53:36.708412\n",
      "25800000 -- 1 58 2019-07-20 12:57:24.683688\n",
      "25900000 -- 1 556 2019-07-20 13:01:17.686210\n",
      "26000000 -- 1 517 2019-07-20 13:05:01.938366\n",
      "26100000 -- 1 278 2019-07-20 13:08:53.120270\n",
      "26200000 -- 1 42 2019-07-20 13:12:39.773608\n",
      "26300000 -- 1 365 2019-07-20 13:16:27.545650\n",
      "26400000 -- 1 321 2019-07-20 13:20:11.731705\n",
      "26500000 -- 1 97 2019-07-20 13:23:56.590338\n",
      "26600000 -- 1 211 2019-07-20 13:27:40.687637\n",
      "26700000 -- 1 134 2019-07-20 13:31:17.961147\n",
      "26800000 -- 1 251 2019-07-20 13:35:03.162775\n",
      "26900000 -- 1 47 2019-07-20 13:38:44.379581\n",
      "27000000 -- 1 590 2019-07-20 13:42:32.960416\n",
      "27100000 -- 1 181 2019-07-20 13:46:24.401832\n",
      "27200000 -- 1 159 2019-07-20 13:50:12.154589\n",
      "27300000 -- 1 127 2019-07-20 13:53:53.689712\n",
      "27400000 -- 1 266 2019-07-20 13:57:42.595490\n",
      "27500000 -- 1 565 2019-07-20 14:01:30.920306\n",
      "27600000 -- 1 546 2019-07-20 14:05:20.112612\n",
      "27700000 -- 1 370 2019-07-20 14:09:08.201129\n",
      "27800000 -- 1 1044 2019-07-20 14:12:53.671461\n",
      "27900000 -- 1 510 2019-07-20 14:16:35.838996\n",
      "28000000 -- 1 114 2019-07-20 14:20:16.520844\n",
      "28100000 -- 1 98 2019-07-20 14:23:54.299330\n",
      "28200000 -- 1 684 2019-07-20 14:27:30.831683\n",
      "28300000 -- 1 23 2019-07-20 14:31:15.820206\n",
      "28400000 -- 1 42 2019-07-20 14:34:56.092193\n",
      "28500000 -- 1 449 2019-07-20 14:38:39.060172\n",
      "28600000 -- 1 90 2019-07-20 14:42:24.375037\n",
      "28700000 -- 1 88 2019-07-20 14:46:13.323200\n",
      "28800000 -- 1 101 2019-07-20 14:50:02.646506\n",
      "28900000 -- 1 275 2019-07-20 14:53:48.479521\n",
      "29000000 -- 1 767 2019-07-20 14:57:35.198099\n",
      "29100000 -- 1 927 2019-07-20 15:01:28.259667\n",
      "29200000 -- 1 953 2019-07-20 15:05:14.989262\n",
      "29300000 -- 1 384 2019-07-20 15:08:52.876431\n",
      "29400000 -- 1 345 2019-07-20 15:12:39.275925\n",
      "29500000 -- 1 9 2019-07-20 15:16:26.069213\n",
      "29600000 -- 1 165 2019-07-20 15:20:11.979265\n",
      "29700000 -- 1 45 2019-07-20 15:23:53.194750\n",
      "29800000 -- 1 268 2019-07-20 15:27:37.915340\n",
      "29900000 -- 1 227 2019-07-20 15:31:22.862255\n",
      "30000000 -- 1 84 2019-07-20 15:35:08.275583\n",
      "30100000 -- 1 55 2019-07-20 15:38:52.843451\n",
      "30200000 -- 1 384 2019-07-20 15:42:39.741875\n",
      "30300000 -- 1 274 2019-07-20 15:46:24.264553\n",
      "30400000 -- 1 199 2019-07-20 15:50:10.005066\n",
      "30500000 -- 1 151 2019-07-20 15:53:53.088993\n",
      "30600000 -- 1 250 2019-07-20 15:57:32.191847\n",
      "30700000 -- 1 231 2019-07-20 16:01:19.829259\n",
      "30800000 -- 1 222 2019-07-20 16:05:00.126326\n",
      "30900000 -- 1 279 2019-07-20 16:08:35.993899\n",
      "31000000 -- 1 585 2019-07-20 16:12:04.241485\n",
      "31100000 -- 1 206 2019-07-20 16:15:37.950906\n",
      "31200000 -- 1 162 2019-07-20 16:19:13.345565\n",
      "31300000 -- 1 349 2019-07-20 16:22:55.952351\n",
      "31400000 -- 1 374 2019-07-20 16:26:33.216838\n",
      "31500000 -- 1 177 2019-07-20 16:30:15.613849\n",
      "31600000 -- 1 160 2019-07-20 16:34:02.890639\n",
      "31700000 -- 1 153 2019-07-20 16:37:40.378992\n",
      "31800000 -- 1 232 2019-07-20 16:41:20.496686\n",
      "31900000 -- 1 106 2019-07-20 16:45:00.853947\n",
      "32000000 -- 1 584 2019-07-20 16:48:46.470260\n",
      "32100000 -- 1 339 2019-07-20 16:52:17.736370\n",
      "32200000 -- 1 571 2019-07-20 16:56:04.069032\n",
      "32300000 -- 1 595 2019-07-20 16:59:44.283087\n",
      "32400000 -- 1 312 2019-07-20 17:03:18.192610\n",
      "32500000 -- 1 244 2019-07-20 17:06:55.190457\n",
      "32600000 -- 1 308 2019-07-20 17:10:32.119514\n",
      "32700000 -- 1 602 2019-07-20 17:14:19.179292\n",
      "32800000 -- 1 233 2019-07-20 17:18:03.264582\n",
      "32900000 -- 1 882 2019-07-20 17:21:45.295970\n",
      "33000000 -- 1 947 2019-07-20 17:25:28.949915\n",
      "33100000 -- 1 902 2019-07-20 17:29:11.081363\n",
      "33200000 -- 1 17 2019-07-20 17:32:53.052210\n",
      "33300000 -- 1 990 2019-07-20 17:36:30.429202\n",
      "33400000 -- 1 633 2019-07-20 17:40:10.611501\n",
      "33500000 -- 1 329 2019-07-20 17:43:52.559072\n",
      "33600000 -- 1 225 2019-07-20 17:47:29.089350\n",
      "33700000 -- 1 369 2019-07-20 17:51:12.521473\n",
      "33800000 -- 1 85 2019-07-20 17:54:49.381582\n",
      "33900000 -- 1 93 2019-07-20 17:58:19.446879\n",
      "34000000 -- 1 451 2019-07-20 18:01:53.337559\n",
      "34100000 -- 1 515 2019-07-20 18:05:21.711079\n",
      "34200000 -- 1 1034 2019-07-20 18:08:57.437405\n",
      "34300000 -- 1 702 2019-07-20 18:12:33.232256\n",
      "34400000 -- 1 595 2019-07-20 18:16:03.974424\n",
      "34500000 -- 1 41 2019-07-20 18:19:40.816055\n",
      "34600000 -- 1 490 2019-07-20 18:23:18.178629\n",
      "34700000 -- 1 175 2019-07-20 18:26:51.494810\n",
      "34800000 -- 1 460 2019-07-20 18:30:28.746616\n",
      "34900000 -- 1 115 2019-07-20 18:34:07.550199\n",
      "35000000 -- 1 216 2019-07-20 18:37:39.709644\n",
      "35100000 -- 1 42 2019-07-20 18:41:04.119379\n",
      "35200000 -- 1 150 2019-07-20 18:44:33.594755\n",
      "35300000 -- 1 278 2019-07-20 18:48:08.539570\n",
      "35400000 -- 1 346 2019-07-20 18:51:42.418332\n",
      "35500000 -- 1 15 2019-07-20 18:55:23.114790\n",
      "35600000 -- 1 123 2019-07-20 18:58:54.667841\n",
      "35700000 -- 1 337 2019-07-20 19:02:21.464908\n",
      "35800000 -- 1 64 2019-07-20 19:05:50.437364\n",
      "35900000 -- 1 349 2019-07-20 19:09:21.871187\n",
      "36000000 -- 1 152 2019-07-20 19:12:29.122057\n",
      "36100000 -- 1 583 2019-07-20 19:16:02.730964\n",
      "36200000 -- 1 70 2019-07-20 19:19:36.099539\n",
      "36300000 -- 1 446 2019-07-20 19:23:07.434879\n",
      "36400000 -- 1 327 2019-07-20 19:26:42.198695\n",
      "36500000 -- 1 494 2019-07-20 19:30:19.187116\n",
      "36600000 -- 1 120 2019-07-20 19:33:56.958790\n",
      "36700000 -- 1 482 2019-07-20 19:37:29.139042\n",
      "36800000 -- 1 245 2019-07-20 19:41:08.506030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36900000 -- 1 580 2019-07-20 19:44:46.187595\n",
      "37000000 -- 1 61 2019-07-20 19:48:16.124963\n",
      "37100000 -- 1 168 2019-07-20 19:51:44.448948\n",
      "Total Num of All    Tokens 2429172233\n",
      "Total Num of Unique Tokens 11279824\n",
      "CORPUS\tit is Dumped into file: data/WikiEnglish/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/WikiEnglish/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/WikiEnglish/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 37101011\n",
      "SENT\tit is Dumped into file: data/WikiEnglish/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 37101011\n",
      "TOKEN\tit is Dumped into file: data/WikiEnglish/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2429172233\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/WikiEnglish/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/WikiEnglish/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 11279824\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiEnglish/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CORPUS\tread from pickle file : data/WikiEnglish/word/Pyramid/CORPUS.p\n",
    "CORPUS\tthe length of it is   : 1\n",
    "GROUP\tread from pickle file : data/WikiEnglish/word/Pyramid/GROUP.p\n",
    "GROUP\tthe length of it is   : 1\n",
    "TEXT\tread from pickle file : data/WikiEnglish/word/Pyramid/TEXT.p\n",
    "TEXT\tthe length of it is   : 4672758\n",
    "SENT\tread from pickle file : data/WikiEnglish/word/Pyramid/SENT.p\n",
    "SENT\tthe length of it is   : 4672758\n",
    "TOKEN\tread from pickle file : data/WikiEnglish/word/Pyramid/TOKEN.p\n",
    "TOKEN\tthe length of it is   : 2618873604\n",
    "**************************************** \n",
    "\n",
    "Deal with the Channel: token\n",
    "Current Channel is        \t token\n",
    "Current Channel Max_Ngram \t 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu EMR Data (Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boson (Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeCN (Chinese )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCKS (Chinese, Medical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Named Entity Recogniton (Chinese, Medical, Private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Part-of-Speech Segmentation (Chinese, Medical, Private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL2003 (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeEn (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPBA2004 (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
