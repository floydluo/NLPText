{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese Sample \n",
    "\n",
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:09.033721Z",
     "start_time": "2019-08-27T13:14:05.815286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.996 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:20.622831Z",
     "start_time": "2019-08-27T13:14:20.606740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki_cn_sample/word/Pyramid/_file/token.txt',\n",
       " 'pos': 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:16:05.599625Z",
     "start_time": "2019-08-27T13:16:05.594457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'pos',\n",
       " 'TOKENLevel': 'word',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki_cn_sample/word/Pyramid/_file/token.txt',\n",
       "  'pos': 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt'},\n",
       " 'length': 8407}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:41.451415Z",
     "start_time": "2019-08-27T13:14:41.445707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  79,  184,  291,  425,  562,  626,  686,  720,  782,  869,  977,\n",
       "        1174, 1224, 1317, 1435, 1555, 1705, 1831, 1966, 2072, 2238, 2449,\n",
       "        2566, 2582, 2645, 2685, 2755, 2866, 2985, 3008, 3021, 3069, 3288,\n",
       "        3446, 3576, 3617, 3743, 3918, 4003, 4075, 4191, 4200, 4233, 4285,\n",
       "        4322, 4357, 4402, 4422, 4436, 4472, 4538, 4629, 4699, 4791, 4868,\n",
       "        5052, 5081, 5187, 5318, 5444, 5554, 5665, 5695, 5751, 5828, 5911,\n",
       "        5943, 6097, 6139, 6253, 6384, 6471, 6606, 6649, 6775, 6825, 6830,\n",
       "        6938, 6981, 7048, 7199, 7297, 7342, 7415, 7573, 7616, 7704, 7787,\n",
       "        7797, 7828, 7905, 7923, 7958, 8011, 8085, 8137, 8191, 8238, 8365,\n",
       "        8407]),\n",
       " 'data/wiki_cn_sample/word/Pyramid/_file/token.txt': array([  472,  1087,  1724,  2376,  3089,  3407,  3740,  3943,  4297,\n",
       "         4802,  5408,  6530,  6820,  7344,  8042,  8708,  9540, 10263,\n",
       "        10994, 11608, 12532, 13711, 14359, 14454, 14794, 15020, 15426,\n",
       "        16057, 16666, 16805, 16879, 17099, 18343, 19230, 19965, 20189,\n",
       "        20933, 21916, 22395, 22768, 23369, 23421, 23607, 23908, 24123,\n",
       "        24352, 24599, 24731, 24832, 25046, 25431, 25974, 26399, 26958,\n",
       "        27396, 28423, 28593, 29233, 29977, 30742, 31397, 32062, 32252,\n",
       "        32582, 33069, 33555, 33734, 34699, 34957, 35701, 36532, 37077,\n",
       "        37891, 38143, 38943, 39235, 39262, 39901, 40147, 40563, 41503,\n",
       "        42082, 42351, 42766, 43693, 43961, 44490, 44895, 44954, 45139,\n",
       "        45575, 45676, 45869, 46193, 46610, 46929, 47227, 47496, 48196,\n",
       "        48421]),\n",
       " 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt': array([  279,   649,  1027,  1516,  2011,  2249,  2462,  2579,  2801,\n",
       "         3103,  3476,  4169,  4346,  4674,  5087,  5509,  6036,  6473,\n",
       "         6954,  7334,  7916,  8665,  9075,  9132,  9348,  9481,  9727,\n",
       "        10120, 10544, 10628, 10674, 10847, 11628, 12185, 12633, 12775,\n",
       "        13228, 13838, 14135, 14397, 14815, 14847, 14965, 15148, 15277,\n",
       "        15401, 15561, 15629, 15677, 15802, 16037, 16356, 16604, 16935,\n",
       "        17214, 17864, 17970, 18348, 18819, 19265, 19664, 20048, 20152,\n",
       "        20353, 20631, 20930, 21043, 21604, 21752, 22154, 22622, 22937,\n",
       "        23413, 23566, 24008, 24192, 24209, 24591, 24737, 24976, 25518,\n",
       "        25869, 26023, 26283, 26851, 27006, 27316, 27598, 27634, 27749,\n",
       "        28025, 28091, 28215, 28405, 28669, 28847, 29039, 29209, 29662,\n",
       "        29817]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_en_sample/wiki_en_sample.txt\n",
      "Total Num of All    Tokens 47166\n",
      "Total Num of Unique Tokens 8620\n",
      "CORPUS\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 47166\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/wiki_en_sample/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_en_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 8620\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_en_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T18:02:40.940475Z",
     "start_time": "2019-08-29T13:04:00.771489Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/WikiChinese/zhwiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.637 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 -- 109 2019-08-29 21:11:20.086077 \t 14309025 \t 8647 8647 8647\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 229 2019-08-29 21:18:29.305268 \t 27054049 \t 10018 10018 10018\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 89 2019-08-29 21:28:27.699765 \t 39007078 \t 10947 10947 10947\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "400000 -- 51 2019-08-29 21:38:31.200964 \t 50755371 \t 11728 11728 11728\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "500000 -- 180 2019-08-29 21:48:06.176919 \t 62454145 \t 12306 12306 12306\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "600000 -- 23 2019-08-29 21:57:14.940143 \t 73773949 \t 12708 12708 12708\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "700000 -- 66 2019-08-29 22:06:42.479607 \t 84920503 \t 13172 13172 13172\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "800000 -- 283 2019-08-29 22:15:44.236144 \t 95600635 \t 13594 13594 13594\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "900000 -- 224 2019-08-29 22:24:56.213239 \t 105906774 \t 14001 14001 14001\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1000000 -- 148 2019-08-29 22:33:18.793415 \t 116116367 \t 14284 14284 14284\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1100000 -- 24 2019-08-29 22:42:33.524280 \t 126860053 \t 14533 14533 14533\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1200000 -- 56 2019-08-29 22:51:19.237393 \t 137105385 \t 14818 14818 14818\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1300000 -- 50 2019-08-29 23:00:00.890198 \t 147072557 \t 15083 15083 15083\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1400000 -- 15 2019-08-29 23:07:31.102643 \t 157407979 \t 15446 15446 15446\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1500000 -- 154 2019-08-29 23:13:45.685027 \t 167074979 \t 15634 15634 15634\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1600000 -- 42 2019-08-29 23:20:02.858721 \t 177009511 \t 16489 16489 16489\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1700000 -- 4 2019-08-29 23:26:09.308126 \t 186387768 \t 16664 16664 16664\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1800000 -- 83 2019-08-29 23:32:29.109867 \t 196234799 \t 16820 16820 16820\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1900000 -- 202 2019-08-29 23:38:44.561362 \t 206146558 \t 17022 17022 17022\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2000000 -- 53 2019-08-29 23:44:58.538516 \t 215888935 \t 17280 17280 17280\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2100000 -- 86 2019-08-29 23:51:39.597352 \t 225881745 \t 17436 17436 17436\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2200000 -- 82 2019-08-29 23:58:01.342594 \t 235659763 \t 17590 17590 17590\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2300000 -- 49 2019-08-30 00:04:05.978313 \t 245616705 \t 17703 17703 17703\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2400000 -- 7 2019-08-30 00:09:04.180148 \t 253814755 \t 17847 17847 17847\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2500000 -- 85 2019-08-30 00:12:01.666521 \t 258689969 \t 17924 17924 17924\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2600000 -- 86 2019-08-30 00:17:22.828123 \t 267578831 \t 18027 18027 18027\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2700000 -- 110 2019-08-30 00:22:27.074834 \t 275901904 \t 18106 18106 18106\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2800000 -- 85 2019-08-30 00:27:09.321282 \t 282415417 \t 18137 18137 18137\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2900000 -- 65 2019-08-30 00:31:35.004765 \t 289629858 \t 18213 18213 18213\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3000000 -- 66 2019-08-30 00:36:53.423466 \t 298947711 \t 18318 18318 18318\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3100000 -- 40 2019-08-30 00:42:48.065362 \t 308323976 \t 18430 18430 18430\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3200000 -- 151 2019-08-30 00:48:53.405724 \t 318422310 \t 18535 18535 18535\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3300000 -- 33 2019-08-30 00:55:01.026431 \t 328772138 \t 18638 18638 18638\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3400000 -- 217 2019-08-30 01:01:08.743846 \t 338846262 \t 18754 18754 18754\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3500000 -- 72 2019-08-30 01:07:15.333983 \t 348969630 \t 18867 18867 18867\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3600000 -- 219 2019-08-30 01:13:16.147061 \t 359379559 \t 18985 18985 18985\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3700000 -- 25 2019-08-30 01:19:23.672840 \t 369324191 \t 19106 19106 19106\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3800000 -- 256 2019-08-30 01:25:41.608379 \t 379153049 \t 19274 19274 19274\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3900000 -- 63 2019-08-30 01:32:05.426158 \t 389331809 \t 19372 19372 19372\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4000000 -- 21 2019-08-30 01:38:25.280387 \t 399482719 \t 19492 19492 19492\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4100000 -- 32 2019-08-30 01:44:08.309493 \t 408562199 \t 19670 19670 19670\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4200000 -- 88 2019-08-30 01:50:20.292630 \t 418511781 \t 19838 19838 19838\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4300000 -- 3 2019-08-30 01:56:35.004905 \t 428711196 \t 19971 19971 19971\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4400000 -- 29 2019-08-30 02:01:37.770222 \t 437812808 \t 20085 20085 20085\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "Total Num of All    Tokens 439594499\n",
      "Total Num of Unique Tokens 20134\n",
      "CORPUS\tit is Dumped into file: data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tit is Dumped into file: data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tit is Dumped into file: data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594499\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/WikiChinese/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/WikiChinese/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 20134\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:01.467619Z",
     "start_time": "2019-09-04T10:02:01.103775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594499\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:02.100700Z",
     "start_time": "2019-09-04T10:02:02.086128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11146"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:20.748054Z",
     "start_time": "2019-09-04T10:02:19.087062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: pinyin\n",
      "Current Channel is        \t pinyin\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 6\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke-n3t6-f236\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tFor Channel: stroke \t 0 2019-09-04 18:02:19.095709\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.voc\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.tsv\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.lkp\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.freq\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'pinyin':  {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 10},\n",
    "    'subcomp': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 1152},\n",
    "    'stroke': { 'use': True, 'Min_Ngram': 3, 'Max_Ngram': 6, 'end_grain': False,  'min_grain_freq' : 236},\n",
    "    # 'pos':     {'use': True, 'tagScheme': 'BIOES'}\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:23.872444Z",
     "start_time": "2019-09-04T10:02:23.833558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinyin\n",
      "GrainNum: 5333\n",
      "count    11146.000000\n",
      "mean         3.095371\n",
      "std          2.109298\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          3.000000\n",
      "max         10.000000\n",
      "dtype: float64\n",
      "34501\n",
      "[64553112 45514885 33382420 ...       10       10       10]\n",
      "======================================\n",
      "\n",
      "\n",
      "subcomp\n",
      "GrainNum: 10001\n",
      "count    11146.000000\n",
      "mean         5.738651\n",
      "std          5.190094\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          5.000000\n",
      "75%          9.000000\n",
      "max         50.000000\n",
      "dtype: float64\n",
      "63963\n",
      "[53308092 44094327 42421777 ...     1155     1155     1152]\n",
      "1152\n",
      "======================================\n",
      "\n",
      "\n",
      "stroke\n",
      "GrainNum: 10005\n",
      "count    11146.000000\n",
      "mean        23.517046\n",
      "std         17.826554\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%         22.000000\n",
      "75%         37.000000\n",
      "max        106.000000\n",
      "dtype: float64\n",
      "262121\n",
      "[122034762  83201490  78056695 ...       236       236       236]\n",
      "236\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    try:\n",
    "        print(BasicObject.getFreq(fld, **CS[fld])[10000])\n",
    "    except:\n",
    "        pass\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T01:30:55.004508Z",
     "start_time": "2019-08-30T01:30:54.995241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'就 此 , 社 会 和 历 史 的 现 象 , 便 被 赋 予 一 种 在 哲 学 史 上 还 是 崭 新 的 显 赫 地 位 。 他 还 将 伦 理 学 划 归 到 这 个 领 域 , 从 而 在 伦 理 学 理 论 和 对 思 想 的 理 解 中 提 出 重 要 的 路 线 。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(100)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T13:05:15.303996Z",
     "start_time": "2019-08-26T13:05:12.115548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254754734\n",
      "**************************************** \n",
      "\n",
      "390106\n",
      "[16697636 10295030  9546380 ...       10       10       10]\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "##########################\n",
    "min_token_freq = 10\n",
    "##########################\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/word/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n",
    "\n",
    "print(len(BasicObject.TokenVocab[0]))\n",
    "print(BasicObject.idx2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T11:50:26.513936Z",
     "start_time": "2019-08-26T11:50:15.087545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char-n1t1-f15\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-08-26 19:50:15.093016\n",
      "\t\tFor Channel: char \t 100000 2019-08-26 19:50:16.215839\n",
      "\t\tFor Channel: char \t 200000 2019-08-26 19:50:17.275196\n",
      "\t\tFor Channel: char \t 300000 2019-08-26 19:50:18.376059\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.voc\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.tsv\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.lkp\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.freq\n",
      "Deal with the Channel: pinyin\n",
      "Current Channel is        \t pinyin\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 6\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 1, 'end_grain': False,  'min_grain_freq' : 15},\n",
    "    'pinyin':  {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 15856},\n",
    "    'subcomp': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 21759},\n",
    "    'stroke':  {'use': True, 'Min_Ngram': 3, 'Max_Ngram': 6, 'end_grain': False,  'min_grain_freq' : 19719},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T11:50:32.477202Z",
     "start_time": "2019-08-26T11:50:31.920671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char\n",
      "GrainNum: 10085\n",
      "count    390106.000000\n",
      "mean          2.960295\n",
      "std           1.583327\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max          43.000000\n",
      "dtype: float64\n",
      "1154829\n",
      "[16697636 10417042  9546380 ...       15       15       15]\n",
      "15\n",
      "======================================\n",
      "\n",
      "\n",
      "pinyin\n",
      "GrainNum: 10002\n",
      "count    390106.000000\n",
      "mean         12.527826\n",
      "std           6.721878\n",
      "min           0.000000\n",
      "25%           8.000000\n",
      "50%          11.000000\n",
      "75%          16.000000\n",
      "max         142.000000\n",
      "dtype: float64\n",
      "4887180\n",
      "[62864529 44459023 32781276 ...    15856    15856    15856]\n",
      "15856\n",
      "======================================\n",
      "\n",
      "\n",
      "subcomp\n",
      "GrainNum: 10002\n",
      "count    390106.000000\n",
      "mean         14.062181\n",
      "std           9.237083\n",
      "min           0.000000\n",
      "25%           7.000000\n",
      "50%          12.000000\n",
      "75%          18.000000\n",
      "max         149.000000\n",
      "dtype: float64\n",
      "5485741\n",
      "[52176468 43015399 41913615 ...    21760    21759    21759]\n",
      "21759\n",
      "======================================\n",
      "\n",
      "\n",
      "stroke\n",
      "GrainNum: 10001\n",
      "count    390106.000000\n",
      "mean         54.171041\n",
      "std          37.389900\n",
      "min           0.000000\n",
      "25%          30.000000\n",
      "50%          54.000000\n",
      "75%          77.000000\n",
      "max         493.000000\n",
      "dtype: float64\n",
      "21132448\n",
      "[120506684  85768534  80332806 ...     19736     19727     19719]\n",
      "19719\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    print(BasicObject.getFreq(fld, **CS[fld])[10000])\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init from origin corpus\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiEnglish/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 10\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:35:18.190191Z",
     "start_time": "2019-08-29T08:35:09.436457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiEnglish/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiEnglish/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiEnglish/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 37101011\n",
      "SENT\tread from pickle file : data/WikiEnglish/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 37101011\n",
      "TOKEN\tread from pickle file : data/WikiEnglish/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2429172233\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init from preprocessed data\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiEnglish/word/'\n",
    "min_token_freq = 30\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:35:18.195307Z",
     "start_time": "2019-08-29T08:35:18.191858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649068\n",
      "[131996922 123504303  98456073 ...        30        30        30]\n"
     ]
    }
   ],
   "source": [
    "print(len(BasicObject.TokenVocab[0]))\n",
    "print(BasicObject.idx2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:36:47.368714Z",
     "start_time": "2019-08-29T08:35:18.197598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char-n1t3-f2514\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-08-29 16:35:18.212042\n",
      "\t\tFor Channel: char \t 100000 2019-08-29 16:35:20.853349\n",
      "\t\tFor Channel: char \t 200000 2019-08-29 16:35:23.620032\n",
      "\t\tFor Channel: char \t 300000 2019-08-29 16:35:26.439164\n",
      "\t\tFor Channel: char \t 400000 2019-08-29 16:35:29.129255\n",
      "\t\tFor Channel: char \t 500000 2019-08-29 16:35:32.058197\n",
      "\t\tFor Channel: char \t 600000 2019-08-29 16:35:34.759977\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.voc\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.tsv\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.lkp\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.freq\n",
      "Deal with the Channel: phoneme\n",
      "Current Channel is        \t phoneme\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: phoneme-n1t3-f1979\n",
      "For channel: | phoneme | build GrainUnique and LookUp\n",
      "\t\tFor Channel: phoneme \t 0 2019-08-29 16:36:28.778297\n",
      "\t\tFor Channel: phoneme \t 100000 2019-08-29 16:36:30.528603\n",
      "\t\tFor Channel: phoneme \t 200000 2019-08-29 16:36:31.695531\n",
      "\t\tFor Channel: phoneme \t 300000 2019-08-29 16:36:32.661209\n",
      "\t\tFor Channel: phoneme \t 400000 2019-08-29 16:36:33.540937\n",
      "\t\tFor Channel: phoneme \t 500000 2019-08-29 16:36:34.383411\n",
      "\t\tFor Channel: phoneme \t 600000 2019-08-29 16:36:37.300633\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.voc\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.tsv\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.lkp\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.freq\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 3, 'end_grain': False,  'min_grain_freq' : 2514},\n",
    "    'phoneme': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 3, 'end_grain': False,  'min_grain_freq' : 1979},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:37:02.502701Z",
     "start_time": "2019-08-29T08:37:02.259120Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char\n",
      "GrainNum: 20003\n",
      "count    649068.000000\n",
      "mean         19.552623\n",
      "std           9.138646\n",
      "min           0.000000\n",
      "25%          12.000000\n",
      "50%          18.000000\n",
      "75%          24.000000\n",
      "max         243.000000\n",
      "dtype: float64\n",
      "12690982\n",
      "[1163857687  829148338  801646984 ...       2514       2514       2514]\n",
      "2514\n",
      "======================================\n",
      "\n",
      "\n",
      "phoneme\n",
      "GrainNum: 20001\n",
      "count    649068.00000\n",
      "mean          4.10151\n",
      "std           6.71414\n",
      "min           1.00000\n",
      "25%           1.00000\n",
      "50%           1.00000\n",
      "75%           1.00000\n",
      "max          54.00000\n",
      "dtype: float64\n",
      "2662159\n",
      "[773536812 609749367 509849261 ...      1980      1979      1979]\n",
      "1979\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    print(BasicObject.getFreq(fld, **CS[fld])[20000])\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:32:26.869876Z",
     "start_time": "2019-08-29T08:32:26.857005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lincoln removed McClellan in March 1862 , after McClellan offered unsolicited political advice . In July Lincoln elevated Henry Halleck . Lincoln appointed John Pope as head of the new Army of Virginia . Pope complied with Lincoln 's desire to advance on Richmond from the north , thus protecting Washington from counterattack .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(488)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu EMR Data (Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T07:52:50.885096Z",
     "start_time": "2019-11-23T08:59:35.688538Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.785 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_OperName.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/DailyRecDatas_Text.p\n",
      "100000 -- 36 2019-11-23 19:00:44.469411 \t 2961104 \t 2745 2745 2745\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "200000 -- 5 2019-11-23 21:22:15.613412 \t 6870587 \t 3040 3040 3040\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "300000 -- 83 2019-11-23 23:39:55.648548 \t 10738060 \t 3150 3150 3150\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "400000 -- 74 2019-11-24 01:48:11.791051 \t 14511542 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "500000 -- 19 2019-11-24 03:56:35.303130 \t 18180275 \t 3309 3309 3309\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "600000 -- 25 2019-11-24 06:01:41.000549 \t 21304848 \t 3398 3398 3398\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "700000 -- 23 2019-11-24 08:05:36.823319 \t 24283158 \t 3445 3445 3445\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "800000 -- 10 2019-11-24 10:12:28.883879 \t 27522427 \t 3450 3450 3450\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "900000 -- 16 2019-11-24 12:35:30.780034 \t 31600113 \t 3504 3504 3504\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_Text.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "1000000 -- 22 2019-11-24 14:54:13.101761 \t 35033914 \t 3535 3535 3535\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\t 5000000 medpos\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "Total Num of All    Tokens 36615822\n",
      "Total Num of Unique Tokens 3541\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 44\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 229409\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1045319\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36615822\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/pos-bioes.tsv\n",
      "medpos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/medpos-bioes.voc\n",
      "medpos-bioes\tthe length of it is   : 129\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/medpos-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuCorpus/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3541\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### LuohuCorpus ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "\n",
    "Corpus2GroupMethod = '.p'\n",
    "\n",
    "Group2TextMethod   = 'element'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos', 'medpos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T01:46:31.006288Z",
     "start_time": "2019-11-01T01:46:30.953543Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 44\n",
      "TEXT\tread from pickle file : data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 213908\n",
      "SENT\tread from pickle file : data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1005357\n",
      "TOKEN\tread from pickle file : data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36455911\n",
      "**************************************** \n",
      "\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: pinyin\n",
      "Current Channel is        \t pinyin\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 1\n",
      "\n",
      "\n",
      "\n",
      "pinyin\n",
      "GrainNum: 306\n",
      "count    3539.000000\n",
      "mean        3.285391\n",
      "std         0.643214\n",
      "min         2.000000\n",
      "25%         3.000000\n",
      "50%         3.000000\n",
      "75%         4.000000\n",
      "max         5.000000\n",
      "dtype: float64\n",
      "11627\n",
      "[36455911  4448390  3748034  2349979  2282275  2185253  2147825  2127810\n",
      "  1816811  1609552  1589146  1473476  1375193  1373012  1363119  1317256\n",
      "  1276066  1222723  1213718  1203432  1116868  1088924  1085196  1056054\n",
      "  1045331   990588   977448   967295   929112   928012   894035   779306\n",
      "   778748   764602   763619   743325   685117   625828   612371   599291\n",
      "   579057   568397   553261   546665   543298   534471   533155   529982\n",
      "   517403   495600   475978   475733   472250   471645   468970   463155\n",
      "   460204   420077   416697   390841   388934   362272   361188   355574\n",
      "   340440   334266   279448   265119   260193   243072   241585   228625\n",
      "   221461   216948   204644   195678   195018   193036   179038   178278\n",
      "   178180   165761   165353   156899   142053   140079   131404   125613\n",
      "   124815   119121   115850   102782    99531    98611    95477    91850\n",
      "    77865    73027    66671    66433    62249    58224    55144    54574\n",
      "    49350    48749    47597    46205    42503    40399    33195    31722\n",
      "    28925    28916    28606    27285    26329    22550    22536    21504\n",
      "    20159    18983    18606    16680    16622    16462    16370    14696\n",
      "    14331    11154    11050    10220    10041     8561     7826     7017\n",
      "     6338     4936     4554     4257     4188     3566     3392     3098\n",
      "     2623     2562     2361     2322     2319     2313     2127     2095\n",
      "     1752     1611     1523     1475     1309     1270     1011     1000\n",
      "      885      830      825      735      728      664      613      514\n",
      "      478      465      440      427      421      358      357      351\n",
      "      311      311      280      266      255      253      243      235\n",
      "      229      225      216      204      193      188      175      136\n",
      "      132      113      102       95       68       55       45       42\n",
      "       40       37       37       35       33       29       29       29\n",
      "       28       23       20       18       16       16       15       15\n",
      "       14       14       11       11       10        9        9        8\n",
      "        8        7        7        7        7        7        6        6\n",
      "        5        5        5        5        4        4        4        4\n",
      "        4        4        4        4        4        4        4        4\n",
      "        4        4        4        4        4        4        3        3\n",
      "        3        3        3        2        2        2        2        2\n",
      "        2        2        2        2        2        2        2        2\n",
      "        2        2        2        2        2        2        2        2\n",
      "        1        1        1        1        1        1        1        1\n",
      "        1        1        1        1        1        1        1        1\n",
      "        1        1        1        1        1        1        1        1\n",
      "        1        1]\n",
      "======================================\n",
      "\n",
      "\n",
      "subcomp\n",
      "GrainNum: 636\n",
      "count    3539.000000\n",
      "mean        4.211642\n",
      "std         1.463759\n",
      "min         2.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max        15.000000\n",
      "dtype: float64\n",
      "14905\n",
      "[36455911  4093617  3862128  2282275  2140597  2028960  1861154  1671560\n",
      "  1537008  1259596  1078598  1042615  1041950   952017   920199   905397\n",
      "   890562   887410   850126   841219   837236   806599   789176   778748\n",
      "   763619   725711   706747   700176   696822   657071   639183   614552\n",
      "   613619   568397   565719   552674   534471   531109   518075   517359\n",
      "   511562   505170   503247   488747   485935   481739   478491   459598\n",
      "   445540   428381   426145   425521   410482   407745   404208   388539\n",
      "   364291   361188   355574   355193   354439   346982   345116   337484\n",
      "   334266   329448   326974   325801   321760   321717   321044   315377\n",
      "   304484   292703   291087   290049   284362   282177   279304   279255\n",
      "   278475   276150   273765   270599   260680   258926   255381   252173\n",
      "   247905   247833   247830   246508   245983   245245   244504   243072\n",
      "   242282   241585   234649   232600   227626   224151   223624   218753\n",
      "   218350   217749   208358   206395   199608   195678   193179   192786\n",
      "   191661   191554   189192   187960   186526   185653   178278   178180\n",
      "   176348   176202   174701   174200   173250   173131   172026   170242\n",
      "   168600   168462   165761   164781   163151   162609   159658   156899\n",
      "   153372   151147   147852   147398   143542   143419   140079   138105\n",
      "   137433   133462   133180   132632   132412   130720   130001   125613\n",
      "   124815   123455   121862   120397   120090   120000   117067   116793\n",
      "   115850   111332   111271   109476   109144   108375   107776   105631\n",
      "   105216   104425   104205   102782   102029   101989   101723   101230\n",
      "   100629    98611    96319    94751    93842    93537    92442    91660\n",
      "    91639    90119    89471    89323    89144    88326    87020    86163\n",
      "    84328    84313    84188    83736    83272    81894    80928    80529\n",
      "    79769    78525    73919    73853    73365    73283    72783    72762\n",
      "    72499    72485    72304    72119    70210    69786    69612    69160\n",
      "    68787    67632    67363    67320    67308    66671    66433    64668\n",
      "    63518    63169    61177    58224    58099    55684    55144    54975\n",
      "    54764    54526    54054    53289    52691    52076    51076    51035\n",
      "    49787    48749    48404    47975    47875    47597    46205    46042\n",
      "    46015    45911    45898    45368    44852    44376    43575    43064\n",
      "    42770    42503    42206    42078    42052    41235    41076    40399\n",
      "    38483    38476    37107    37004    36218    35406    34756    34727\n",
      "    34597    33841    33195    32782    32381    31722    31386    31094\n",
      "    30765    30348    29892    29838    29522    28925    28916    28863\n",
      "    28788    28785    28606    28543    27942    27611    27551    27278\n",
      "    26156    25891    25449    24996    24776    24714    24565    24280\n",
      "    24257    24208    23935    23822    23785    23747    23655    22813\n",
      "    22661    22636    22550    22536    22476    22345    22296    22236\n",
      "    21616    21504    20875    20783    20748    20396    19464    19391\n",
      "    18606    18415    17917    17430    17115    16855    16680    16622\n",
      "    16578    16462    16429    16370    16354    16118    15879    15851\n",
      "    15337    15258    15064    14724    14696    14331    14198    14026\n",
      "    13851    13764    13713    13641    13363    13174    13033    12949\n",
      "    12728    12723    12679    12523    12469    12328    12074    11840\n",
      "    11647    11553    11182    11154    11094    11050    11017    10775\n",
      "    10705    10487    10050    10041     9897     9822     9527     9326\n",
      "     9235     9176     8989     8619     8561     8476     8261     8222\n",
      "     7826     7724     7718     7507     7289     6959     6712     6684\n",
      "     6646     6607     6338     5996     5427     5355     5284     5223\n",
      "     5125     5110     5031     4936     4901     4830     4554     4528\n",
      "     4261     4257     4188     4100     4027     3954     3701     3667\n",
      "     3566     3537     3392     3110     3098     3057     2996     2989\n",
      "     2786     2758     2672     2623     2564     2562     2527     2514\n",
      "     2361     2322     2319     2313     2311     2294     2218     2202\n",
      "     2182     2127     2019     1965     1860     1828     1812     1785\n",
      "     1752     1722     1683     1654     1611     1523     1475     1463\n",
      "     1430     1309     1307     1011     1004     1000      908      890\n",
      "      885      868      848      844      830      825      812      767\n",
      "      735      728      708      683      668      664      651      613\n",
      "      579      555      547      543      529      492      483      478\n",
      "      465      440      439      428      427      421      387      370\n",
      "      358      357      351      339      318      316      311      311\n",
      "      296      288      285      280      266      255      253      252\n",
      "      245      245      235      231      230      227      225      204\n",
      "      201      193      188      181      178      175      145      143\n",
      "      136      136      132      130      113      106      105      102\n",
      "       95       83       68       68       61       58       56       45\n",
      "       45       42       40       40       37       36       35       33\n",
      "       29       29       29       28       26       24       22       21\n",
      "       18       16       16       15       15       14       14       13\n",
      "       11       11        9        9        9        8        7        7\n",
      "        7        5        5        5        5        4        4        4\n",
      "        4        4        4        4        4        4        4        4\n",
      "        3        3        3        3        2        2        2        2\n",
      "        2        2        2        2        2        2        2        2\n",
      "        2        2        2        2        2        1        1        1\n",
      "        1        1        1        1        1        1        1        1\n",
      "        1        1        1        1]\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/LuohuCorpus/char/'\n",
    "min_token_freq = 1\n",
    "\n",
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'pinyin':  {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 1, 'end_grain': True,  'min_grain_freq' : 0},\n",
    "    'subcomp': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 1, 'end_grain': True,  'min_grain_freq' : 0},\n",
    "    # 'stroke': { 'use': True, 'Min_Ngram': 3, 'Max_Ngram': 6, 'end_grain': False,  'min_grain_freq' : 236},\n",
    "    # 'pos':     {'use': True, 'tagScheme': 'BIOES'}\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    try:\n",
    "        print(BasicObject.getFreq(fld, **CS[fld])[10000])\n",
    "    except:\n",
    "        pass\n",
    "    print('======================================\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T10:08:00.072664Z",
     "start_time": "2019-11-24T10:08:00.057525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sentidx = 4000\n",
    "sent = Sentence(sentidx)\n",
    "sent.IdxGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T10:08:08.816671Z",
     "start_time": "2019-11-24T10:08:08.812193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'定 期 复 查 抗 核 抗 体 谱 、 乙 肝 病 毒 定 量 、 肝 肾 功 能 等 。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T10:08:23.267440Z",
     "start_time": "2019-11-24T10:08:23.254305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild GrainUnique for channel: medpos-bio\n",
      "medpos 1 False BIO\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/medpos-bio.voc\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/medpos-bio.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['定性-B'],\n",
       " ['定性-I'],\n",
       " ['医疗行为-B'],\n",
       " ['医疗行为-I'],\n",
       " ['治疗项目-B'],\n",
       " ['治疗项目-I'],\n",
       " ['药物-B'],\n",
       " ['药物-I'],\n",
       " ['药物-I'],\n",
       " ['连词-B'],\n",
       " ['生物-B'],\n",
       " ['生物-I'],\n",
       " ['生物-I'],\n",
       " ['生物-I'],\n",
       " ['检查项目-B'],\n",
       " ['检查项目-I'],\n",
       " ['连词-B'],\n",
       " ['身体部位-B'],\n",
       " ['身体部位-B'],\n",
       " ['身体功能-B'],\n",
       " ['身体功能-I'],\n",
       " ['虚词-B'],\n",
       " ['标点-B']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.get_grain_str('medpos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fudan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/fudan/C39-Sports\n",
      "corpus/fudan/C31-Enviornment\n",
      "corpus/fudan/C38-Politics\n",
      "corpus/fudan/C34-Economy\n",
      "corpus/fudan/C32-Agriculture\n",
      "Total Num of All    Tokens 17252831\n",
      "Total Num of Unique Tokens 401556\n",
      "CORPUS\tit is Dumped into file: data/fudan/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/fudan/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/fudan/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 5885\n",
      "SENT\tit is Dumped into file: data/fudan/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 5885\n",
      "TOKEN\tit is Dumped into file: data/fudan/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 17252831\n",
      "**************************************** \n",
      "\n",
      "token   \tis Dumped into file: data/fudan/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 401556\n",
      "\t\tWrite to: data/fudan/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = []\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sentidx = 4000\n",
    "sent = Sentence(sentidx)\n",
    "sent.IdxGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.522745Z",
     "start_time": "2019-09-04T06:22:26.837828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/fudan/C39-Sports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.660 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/fudan/C31-Enviornment\n",
      "corpus/fudan/C38-Politics\n",
      "corpus/fudan/C34-Economy\n",
      "corpus/fudan/C32-Agriculture\n",
      "Total Num of All    Tokens 29798646\n",
      "Total Num of Unique Tokens 5828\n",
      "CORPUS\tit is Dumped into file: data/fudan/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/fudan/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/fudan/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 5885\n",
      "SENT\tit is Dumped into file: data/fudan/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 5885\n",
      "TOKEN\tit is Dumped into file: data/fudan/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 29798646\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/fudan/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/fudan/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/fudan/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 5828\n",
      "\t\tWrite to: data/fudan/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:18:45.492616Z",
     "start_time": "2019-09-04T06:18:45.481986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/fudan/C39-Sports\n",
      "1209\n",
      "corpus/fudan/C31-Enviornment\n",
      "1068\n",
      "corpus/fudan/C38-Politics\n",
      "1017\n",
      "corpus/fudan/C34-Economy\n",
      "1596\n",
      "corpus/fudan/C32-Agriculture\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "from nlptext.folder import Folder\n",
    "for i in range(BasicObject.GROUP['length']):\n",
    "    f = Folder(i)\n",
    "    print(f.name)\n",
    "    s, e = f.IdxSentStartEnd\n",
    "    print(e - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.536292Z",
     "start_time": "2019-09-04T06:32:06.524055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'group_idx' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-39c361d20571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                  \u001b[0mSent2TokenMethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENLevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_token_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_token_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                  \u001b[0muse_hyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_hyper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                  anno = False, anno_keywords = anno_keywords)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/nlptext/nlptext/base.py\u001b[0m in \u001b[0;36mINIT\u001b[0;34m(cls, CORPUSPath, Corpus2GroupMethod, Group2TextMethod, Text2SentMethod, Sent2TokenMethod, TOKENLevel, min_token_freq, use_hyper, Channel_Dep_Methods, Channel_Dep_TagSets, anno, anno_keywords)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# if there is an error, folderIdx will be referenced before assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mlenCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mCORPUS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EndIDXGroups'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlenCorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'group_idx' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/newsgroup/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.536918Z",
     "start_time": "2019-09-04T06:22:57.048Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlptext.folder import Folder\n",
    "for i in range(BasicObject.GROUP['length']):\n",
    "    f = Folder(i)\n",
    "    print(f.name)\n",
    "    s, e = f.IdxSentStartEnd\n",
    "    print(e - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhihu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T09:30:35.324511Z",
     "start_time": "2019-10-23T00:28:28.047354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/ZhihuContent/Live\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.574 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/ZhihuContent/Q\n",
      "100000 -- 15 2019-10-23 08:31:44.846642 \t 3435530 \t 108405 108405 108405\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 11 2019-10-23 08:35:08.367620 \t 6483489 \t 162863 162863 162863\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 60 2019-10-23 08:38:30.797520 \t 9546254 \t 203645 203645 203645\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "400000 -- 7 2019-10-23 08:41:58.292255 \t 12582264 \t 238372 238372 238372\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "500000 -- 87 2019-10-23 08:45:23.404024 \t 15657132 \t 268726 268726 268726\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/ZhihuContent/Art\n",
      "600000 -- 19 2019-10-23 08:48:29.650688 \t 18689797 \t 306634 306634 306634\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "700000 -- 32 2019-10-23 08:51:31.108083 \t 21719950 \t 351829 351829 351829\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "800000 -- 16 2019-10-23 08:54:21.888990 \t 24818501 \t 392164 392164 392164\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "900000 -- 29 2019-10-23 08:57:15.080027 \t 27818621 \t 430861 430861 430861\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1000000 -- 116 2019-10-23 09:00:23.937049 \t 30954821 \t 469638 469638 469638\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1100000 -- 54 2019-10-23 09:03:13.165692 \t 34049420 \t 504958 504958 504958\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1200000 -- 21 2019-10-23 09:06:10.139417 \t 37157136 \t 539601 539601 539601\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1300000 -- 12 2019-10-23 09:08:56.334149 \t 40132512 \t 570183 570183 570183\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1400000 -- 42 2019-10-23 09:11:43.806986 \t 43217773 \t 601172 601172 601172\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1500000 -- 33 2019-10-23 09:14:55.909538 \t 46310143 \t 636424 636424 636424\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1600000 -- 10 2019-10-23 09:17:42.921845 \t 49353717 \t 663469 663469 663469\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1700000 -- 113 2019-10-23 09:20:30.388500 \t 52397406 \t 690439 690439 690439\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1800000 -- 4 2019-10-23 09:23:18.341507 \t 55461350 \t 719037 719037 719037\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1900000 -- 60 2019-10-23 09:26:27.502028 \t 58502409 \t 750045 750045 750045\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2000000 -- 3 2019-10-23 09:29:13.556727 \t 61560345 \t 779118 779118 779118\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2100000 -- 40 2019-10-23 09:32:10.452630 \t 64667544 \t 810120 810120 810120\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2200000 -- 7 2019-10-23 09:36:28.860804 \t 67777968 \t 840873 840873 840873\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2300000 -- 54 2019-10-23 09:39:22.873382 \t 70823577 \t 865435 865435 865435\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2400000 -- 16 2019-10-23 09:42:12.167005 \t 73869083 \t 890605 890605 890605\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2500000 -- 62 2019-10-23 09:44:58.279128 \t 76944007 \t 915554 915554 915554\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2600000 -- 19 2019-10-23 09:47:42.113065 \t 79995537 \t 941634 941634 941634\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2700000 -- 30 2019-10-23 09:50:32.184765 \t 83130394 \t 965745 965745 965745\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2800000 -- 31 2019-10-23 09:53:33.883411 \t 86250874 \t 991306 991306 991306\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2900000 -- 34 2019-10-23 09:56:24.948615 \t 89293222 \t 1016761 1016761 1016761\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3000000 -- 36 2019-10-23 09:59:05.953230 \t 92315935 \t 1039230 1039230 1039230\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3100000 -- 33 2019-10-23 10:01:56.533744 \t 95384075 \t 1061635 1061635 1061635\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3200000 -- 32 2019-10-23 10:04:55.521427 \t 98463598 \t 1085418 1085418 1085418\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3300000 -- 20 2019-10-23 10:07:37.312981 \t 101510517 \t 1109268 1109268 1109268\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3400000 -- 23 2019-10-23 10:10:31.999134 \t 104554350 \t 1133055 1133055 1133055\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3500000 -- 37 2019-10-23 10:13:19.874143 \t 107590140 \t 1154704 1154704 1154704\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3600000 -- 40 2019-10-23 10:16:04.621547 \t 110581833 \t 1178619 1178619 1178619\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3700000 -- 7 2019-10-23 10:19:14.586707 \t 113670940 \t 1200287 1200287 1200287\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3800000 -- 28 2019-10-23 10:22:12.737836 \t 116755856 \t 1221811 1221811 1221811\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3900000 -- 36 2019-10-23 10:25:04.416967 \t 119786249 \t 1241913 1241913 1241913\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4000000 -- 43 2019-10-23 10:28:02.209097 \t 122835782 \t 1267076 1267076 1267076\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4100000 -- 23 2019-10-23 10:30:47.248189 \t 125896084 \t 1289595 1289595 1289595\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4200000 -- 29 2019-10-23 10:33:36.139415 \t 128964871 \t 1312271 1312271 1312271\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4300000 -- 6 2019-10-23 10:36:24.348303 \t 132065856 \t 1335076 1335076 1335076\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4400000 -- 19 2019-10-23 10:39:41.916662 \t 135092651 \t 1356571 1356571 1356571\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4500000 -- 34 2019-10-23 10:42:35.707978 \t 138179165 \t 1377856 1377856 1377856\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4600000 -- 66 2019-10-23 10:45:21.351072 \t 141188652 \t 1398032 1398032 1398032\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4700000 -- 34 2019-10-23 10:48:21.877269 \t 144271090 \t 1417393 1417393 1417393\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4800000 -- 68 2019-10-23 10:51:04.809869 \t 147267503 \t 1437529 1437529 1437529\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4900000 -- 13 2019-10-23 10:53:59.128951 \t 150353060 \t 1458663 1458663 1458663\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 5000000\n",
      "\tupdated leng is: 7000000\n",
      "5000000 -- 17 2019-10-23 10:56:44.977816 \t 153346357 \t 1480414 1480414 1480414\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5100000 -- 60 2019-10-23 10:59:42.130986 \t 156416742 \t 1502741 1502741 1502741\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5200000 -- 102 2019-10-23 11:02:49.413669 \t 159523335 \t 1522803 1522803 1522803\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5300000 -- 7 2019-10-23 11:05:47.231353 \t 162563279 \t 1541717 1541717 1541717\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5400000 -- 41 2019-10-23 11:08:49.361286 \t 165620299 \t 1562427 1562427 1562427\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5500000 -- 32 2019-10-23 11:11:33.560600 \t 168663969 \t 1583177 1583177 1583177\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5600000 -- 40 2019-10-23 11:14:19.794693 \t 171696693 \t 1605492 1605492 1605492\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5700000 -- 27 2019-10-23 11:17:10.302801 \t 174760294 \t 1626050 1626050 1626050\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5800000 -- 21 2019-10-23 11:19:59.833546 \t 177793913 \t 1644673 1644673 1644673\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "5900000 -- 67 2019-10-23 11:22:46.312547 \t 180806322 \t 1664423 1664423 1664423\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6000000 -- 22 2019-10-23 11:25:44.169458 \t 183838873 \t 1684325 1684325 1684325\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "corpus/ZhihuContent/Ans\n",
      "6100000 -- 5 2019-10-23 11:28:40.380881 \t 186849984 \t 1700758 1700758 1700758\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6200000 -- 2 2019-10-23 11:32:04.542541 \t 189782339 \t 1714727 1714727 1714727\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6300000 -- 53 2019-10-23 11:35:16.560866 \t 192766381 \t 1729061 1729061 1729061\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6400000 -- 6 2019-10-23 11:38:26.326296 \t 195762450 \t 1743186 1743186 1743186\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6500000 -- 16 2019-10-23 11:41:42.755976 \t 198748663 \t 1755756 1755756 1755756\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6600000 -- 95 2019-10-23 11:44:52.596746 \t 201657623 \t 1768792 1768792 1768792\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6700000 -- 50 2019-10-23 11:48:10.437408 \t 204642156 \t 1782447 1782447 1782447\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6800000 -- 100 2019-10-23 11:51:17.662451 \t 207638599 \t 1797158 1797158 1797158\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "6900000 -- 29 2019-10-23 11:54:42.828861 \t 210691585 \t 1810381 1810381 1810381\n",
      "\t 7000000 token\n",
      "\t 7000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 7000000\n",
      "\tupdated leng is: 9000000\n",
      "7000000 -- 26 2019-10-23 11:57:59.698293 \t 213788673 \t 1824185 1824185 1824185\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7100000 -- 44 2019-10-23 12:01:18.723778 \t 216784424 \t 1836577 1836577 1836577\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7200000 -- 48 2019-10-23 12:04:34.623928 \t 219807129 \t 1849690 1849690 1849690\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7300000 -- 39 2019-10-23 12:07:55.705197 \t 222816685 \t 1864326 1864326 1864326\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7400000 -- 22 2019-10-23 12:11:20.981191 \t 225819280 \t 1877614 1877614 1877614\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7500000 -- 14 2019-10-23 12:14:36.556966 \t 228779194 \t 1889871 1889871 1889871\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600000 -- 26 2019-10-23 12:17:51.621794 \t 231782873 \t 1902302 1902302 1902302\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7700000 -- 24 2019-10-23 12:21:05.380103 \t 234792236 \t 1914690 1914690 1914690\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7800000 -- 27 2019-10-23 12:24:30.721221 \t 237789639 \t 1926914 1926914 1926914\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "7900000 -- 92 2019-10-23 12:27:44.138750 \t 240802100 \t 1939614 1939614 1939614\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8000000 -- 52 2019-10-23 12:31:00.178213 \t 243823847 \t 1952343 1952343 1952343\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8100000 -- 12 2019-10-23 12:34:11.746327 \t 246758587 \t 1963749 1963749 1963749\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8200000 -- 27 2019-10-23 12:37:28.719000 \t 249797933 \t 1975942 1975942 1975942\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8300000 -- 32 2019-10-23 12:40:48.117807 \t 252825976 \t 1988344 1988344 1988344\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8400000 -- 30 2019-10-23 12:43:59.413263 \t 255802458 \t 2000659 2000659 2000659\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8500000 -- 14 2019-10-23 12:47:23.864234 \t 258854329 \t 2014019 2014019 2014019\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8600000 -- 36 2019-10-23 12:50:39.681645 \t 261896050 \t 2025930 2025930 2025930\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8700000 -- 35 2019-10-23 12:54:14.234693 \t 264951230 \t 2039444 2039444 2039444\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8800000 -- 19 2019-10-23 12:57:35.338353 \t 267935126 \t 2050840 2050840 2050840\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "8900000 -- 8 2019-10-23 13:01:01.608780 \t 271001626 \t 2063521 2063521 2063521\n",
      "\t 9000000 token\n",
      "\t 9000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 9000000\n",
      "\tupdated leng is: 11000000\n",
      "9000000 -- 6 2019-10-23 13:04:21.820771 \t 274051334 \t 2075303 2075303 2075303\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9100000 -- 20 2019-10-23 13:07:42.473516 \t 277029931 \t 2086853 2086853 2086853\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9200000 -- 44 2019-10-23 13:11:09.849241 \t 280147200 \t 2099293 2099293 2099293\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9300000 -- 8 2019-10-23 13:14:33.620744 \t 283180651 \t 2110824 2110824 2110824\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9400000 -- 28 2019-10-23 13:17:49.905927 \t 286166169 \t 2121508 2121508 2121508\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9500000 -- 37 2019-10-23 13:21:03.841086 \t 289140488 \t 2133075 2133075 2133075\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9600000 -- 23 2019-10-23 13:24:17.586643 \t 292111635 \t 2144309 2144309 2144309\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9700000 -- 12 2019-10-23 13:27:34.086157 \t 295105241 \t 2155559 2155559 2155559\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9800000 -- 67 2019-10-23 13:30:44.214961 \t 298082401 \t 2169497 2169497 2169497\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "9900000 -- 46 2019-10-23 13:33:50.924447 \t 301107036 \t 2179730 2179730 2179730\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10000000 -- 34 2019-10-23 13:37:00.501273 \t 304084938 \t 2190915 2190915 2190915\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10100000 -- 6 2019-10-23 13:40:13.990248 \t 307035747 \t 2203821 2203821 2203821\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10200000 -- 28 2019-10-23 13:43:35.304285 \t 310011709 \t 2215658 2215658 2215658\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10300000 -- 13 2019-10-23 13:46:55.734272 \t 312998822 \t 2227505 2227505 2227505\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10400000 -- 11 2019-10-23 13:50:12.555071 \t 316022421 \t 2238479 2238479 2238479\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10500000 -- 9 2019-10-23 13:53:29.719155 \t 319011813 \t 2250436 2250436 2250436\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10600000 -- 12 2019-10-23 13:56:43.134813 \t 322036161 \t 2261446 2261446 2261446\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10700000 -- 20 2019-10-23 13:59:56.762333 \t 325066218 \t 2272669 2272669 2272669\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10800000 -- 2 2019-10-23 14:03:16.950033 \t 328078802 \t 2283072 2283072 2283072\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "10900000 -- 82 2019-10-23 14:06:46.926924 \t 331110791 \t 2294656 2294656 2294656\n",
      "\t 11000000 token\n",
      "\t 11000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 11000000\n",
      "\tupdated leng is: 13000000\n",
      "11000000 -- 20 2019-10-23 14:10:02.376993 \t 334089144 \t 2305332 2305332 2305332\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11100000 -- 94 2019-10-23 14:13:18.528530 \t 337099201 \t 2317046 2317046 2317046\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11200000 -- 19 2019-10-23 14:16:33.746310 \t 340127003 \t 2327990 2327990 2327990\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11300000 -- 7 2019-10-23 14:19:50.981677 \t 343153760 \t 2338108 2338108 2338108\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11400000 -- 18 2019-10-23 14:23:02.808606 \t 346141096 \t 2348424 2348424 2348424\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11500000 -- 35 2019-10-23 14:26:19.325101 \t 349141928 \t 2358723 2358723 2358723\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11600000 -- 21 2019-10-23 14:29:37.391115 \t 352110794 \t 2369689 2369689 2369689\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11700000 -- 38 2019-10-23 14:32:52.602759 \t 355072361 \t 2380107 2380107 2380107\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11800000 -- 31 2019-10-23 14:36:12.881180 \t 358064104 \t 2391037 2391037 2391037\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "11900000 -- 12 2019-10-23 14:39:27.691020 \t 361084063 \t 2401211 2401211 2401211\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12000000 -- 24 2019-10-23 14:42:49.050611 \t 364129555 \t 2411474 2411474 2411474\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12100000 -- 45 2019-10-23 14:46:05.814888 \t 367121623 \t 2422522 2422522 2422522\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12200000 -- 12 2019-10-23 14:49:30.350443 \t 370142950 \t 2433888 2433888 2433888\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12300000 -- 33 2019-10-23 14:52:43.062841 \t 373103978 \t 2443645 2443645 2443645\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12400000 -- 10 2019-10-23 14:55:53.859679 \t 376119169 \t 2453643 2453643 2453643\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12500000 -- 20 2019-10-23 14:59:18.538680 \t 379118564 \t 2465078 2465078 2465078\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12600000 -- 32 2019-10-23 15:02:32.665359 \t 382083912 \t 2475251 2475251 2475251\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12700000 -- 28 2019-10-23 15:05:56.905254 \t 385102188 \t 2485975 2485975 2485975\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12800000 -- 29 2019-10-23 15:09:04.447243 \t 388067527 \t 2496016 2496016 2496016\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "12900000 -- 29 2019-10-23 15:12:20.026576 \t 391067795 \t 2506287 2506287 2506287\n",
      "\t 13000000 token\n",
      "\t 13000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 13000000\n",
      "\tupdated leng is: 15000000\n",
      "13000000 -- 4 2019-10-23 15:15:28.499184 \t 394050475 \t 2517035 2517035 2517035\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13100000 -- 37 2019-10-23 15:18:55.380132 \t 397006404 \t 2527106 2527106 2527106\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13200000 -- 21 2019-10-23 15:22:45.832745 \t 400103460 \t 2538080 2538080 2538080\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13300000 -- 48 2019-10-23 15:26:12.608754 \t 403146669 \t 2548195 2548195 2548195\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13400000 -- 5 2019-10-23 15:29:56.978091 \t 406132827 \t 2558593 2558593 2558593\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13500000 -- 27 2019-10-23 15:33:05.726379 \t 409121191 \t 2567932 2567932 2567932\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13600000 -- 27 2019-10-23 15:36:17.152285 \t 412108060 \t 2577469 2577469 2577469\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13700000 -- 29 2019-10-23 15:39:25.862027 \t 415114753 \t 2588200 2588200 2588200\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13800000 -- 23 2019-10-23 15:42:40.860306 \t 418139244 \t 2599737 2599737 2599737\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "13900000 -- 19 2019-10-23 15:45:54.066485 \t 421065836 \t 2609985 2609985 2609985\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14000000 -- 25 2019-10-23 15:49:05.438555 \t 424086181 \t 2619355 2619355 2619355\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14100000 -- 19 2019-10-23 15:52:19.234390 \t 427035217 \t 2629071 2629071 2629071\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14200000 -- 6 2019-10-23 15:55:44.744697 \t 430060500 \t 2638864 2638864 2638864\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14300000 -- 42 2019-10-23 15:59:08.592241 \t 433005445 \t 2648012 2648012 2648012\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14400000 -- 21 2019-10-23 16:02:29.552159 \t 436070019 \t 2659349 2659349 2659349\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14500000 -- 19 2019-10-23 16:05:56.137930 \t 439076552 \t 2669134 2669134 2669134\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14600000 -- 43 2019-10-23 16:09:09.570907 \t 442047709 \t 2678162 2678162 2678162\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14700000 -- 20 2019-10-23 16:12:46.014187 \t 445041231 \t 2688207 2688207 2688207\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14800000 -- 8 2019-10-23 16:16:00.512404 \t 447990623 \t 2698524 2698524 2698524\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "14900000 -- 18 2019-10-23 16:19:15.708498 \t 450994073 \t 2707764 2707764 2707764\n",
      "\t 15000000 token\n",
      "\t 15000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 15000000\n",
      "\tupdated leng is: 17000000\n",
      "15000000 -- 11 2019-10-23 16:22:42.726538 \t 454008067 \t 2718028 2718028 2718028\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15100000 -- 12 2019-10-23 16:25:52.996854 \t 456995939 \t 2727484 2727484 2727484\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15200000 -- 30 2019-10-23 16:29:16.740313 \t 460028820 \t 2738632 2738632 2738632\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15300000 -- 27 2019-10-23 16:32:44.199056 \t 463100478 \t 2748403 2748403 2748403\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15400000 -- 14 2019-10-23 16:36:13.947963 \t 466132815 \t 2759064 2759064 2759064\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15500000 -- 24 2019-10-23 16:39:43.683425 \t 469102353 \t 2769137 2769137 2769137\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15600000 -- 43 2019-10-23 16:43:04.897412 \t 472156533 \t 2779208 2779208 2779208\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15700000 -- 35 2019-10-23 16:46:18.457836 \t 475186344 \t 2788060 2788060 2788060\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15800000 -- 31 2019-10-23 16:49:28.285556 \t 478146445 \t 2796843 2796843 2796843\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "15900000 -- 37 2019-10-23 16:53:17.191602 \t 481210753 \t 2806139 2806139 2806139\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16000000 -- 26 2019-10-23 16:56:44.133128 \t 484213596 \t 2815705 2815705 2815705\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16100000 -- 25 2019-10-23 17:00:02.200719 \t 487176704 \t 2824606 2824606 2824606\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16200000 -- 24 2019-10-23 17:03:37.185134 \t 490245965 \t 2834353 2834353 2834353\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16300000 -- 67 2019-10-23 17:06:51.588784 \t 493235609 \t 2842432 2842432 2842432\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16400000 -- 36 2019-10-23 17:10:03.787113 \t 496230181 \t 2851633 2851633 2851633\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16500000 -- 28 2019-10-23 17:13:36.558441 \t 499235725 \t 2861305 2861305 2861305\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16600000 -- 2 2019-10-23 17:16:59.236546 \t 502263721 \t 2870522 2870522 2870522\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16700000 -- 2 2019-10-23 17:20:17.368347 \t 505286597 \t 2878689 2878689 2878689\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16800000 -- 34 2019-10-23 17:23:40.047511 \t 508323725 \t 2888043 2888043 2888043\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "16900000 -- 31 2019-10-23 17:27:01.371506 \t 511238873 \t 2897841 2897841 2897841\n",
      "\t 17000000 token\n",
      "\t 17000000 pos\n",
      "\tEnlarging the SENT size...\n",
      "\tcurrent leng is: 17000000\n",
      "\tupdated leng is: 19000000\n",
      "17000000 -- 21 2019-10-23 17:30:29.869297 \t 514265180 \t 2907577 2907577 2907577\n",
      "\t 19000000 token\n",
      "\t 19000000 pos\n",
      "Total Num of All    Tokens 514266786\n",
      "Total Num of Unique Tokens 2907578\n",
      "CORPUS\tit is Dumped into file: data/ZhihuContent/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/ZhihuContent/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 4\n",
      "TEXT\tit is Dumped into file: data/ZhihuContent/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 793942\n",
      "SENT\tit is Dumped into file: data/ZhihuContent/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 17000045\n",
      "TOKEN\tit is Dumped into file: data/ZhihuContent/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 514266786\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/ZhihuContent/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/ZhihuContent/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/ZhihuContent/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2907578\n",
      "\t\tWrite to: data/ZhihuContent/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### LuohuCorpus ###########\n",
    "CORPUSPath = 'corpus/ZhihuContent/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 10\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:08.002405Z",
     "start_time": "2019-10-22T01:39:07.992457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<txt Ans569325274.txt 500030>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.text import Text\n",
    "\n",
    "\n",
    "t = Text(500030)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:08.662553Z",
     "start_time": "2019-10-22T01:39:08.655130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['初 中 同 桌 是 女 生 , 我 俩 都 是 全 班 前 三 名 , 我 天 天 跟 人 家 暗 中 较 着 劲 。',\n",
       " '这 天 体 检 , 大 家 都 拿 着 自 己 的 表 排 队 体 检 。',\n",
       " '回 来 后 我 一 把 把 她 的 体 检 表 抢 过 来 , 看 看 有 没 有 超 过 我 的 地 方 。',\n",
       " '然 后 我 发 现 了 新 大 陆 , 顿 时 火 冒 三 丈 , 用 足 以 让 半 个 班 同 学 都 能 听 清 的 音 量 对 她 大 吼 一 声 : X X X ( 同 桌 名 字 ) ! 你 怎 么 能 胸 围 比 我 还 大 ! … …',\n",
       " '同 学 们 : h h h h h h h h h h h h h h … …',\n",
       " '我 : ? ? ? ? ? 班 主 任 也 在 场 。',\n",
       " '她 很 亲 切 地 叫 我 “ 来 办 公 室 ” 。',\n",
       " '我 在 众 人 的 哄 笑 声 中 , 懵 逼 地 跟 在 老 师 后 面 进 了 办 公 室 , 她 让 我 坐 下 来 , 很 和 蔼 地 和 我 简 短 讲 了 一 些 生 理 卫 生 知 识 。',\n",
       " '后 来 当 然 是 懂 了 , 不 过 被 嘲 笑 了 好 长 好 长 好 长 时 间 … …',\n",
       " '至 少 毕 业 十 五 年 聚 会 的 时 候 , 还 t m 有 人 记 得 … …',\n",
       " '我 确 实 是 比 较 后 知 后 觉 吧 , 都 初 中 了 , 什 么 都 不 明 白 。',\n",
       " '我 上 的 是 五 年 制 小 学 , 又 提 前 一 年 入 学 。',\n",
       " '那 一 年 , 我 应 该 是 1 1 岁 。',\n",
       " '三 百 赞 了 啊 … …',\n",
       " '一 千 赞 , 承 蒙 厚 爱 , 感 激 不 尽 … …']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [i.sentence for i in t.Sentences]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:32.513246Z",
     "start_time": "2019-10-22T01:39:32.503746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<st 9933486 (tokenNum: 30) >,\n",
       " <st 9933487 (tokenNum: 19) >,\n",
       " <st 9933488 (tokenNum: 28) >,\n",
       " <st 9933489 (tokenNum: 63) >,\n",
       " <st 9933490 (tokenNum: 20) >,\n",
       " <st 9933491 (tokenNum: 14) >,\n",
       " <st 9933492 (tokenNum: 14) >,\n",
       " <st 9933493 (tokenNum: 51) >,\n",
       " <st 9933494 (tokenNum: 24) >,\n",
       " <st 9933495 (tokenNum: 22) >,\n",
       " <st 9933496 (tokenNum: 24) >,\n",
       " <st 9933497 (tokenNum: 18) >,\n",
       " <st 9933498 (tokenNum: 12) >,\n",
       " <st 9933499 (tokenNum: 7) >,\n",
       " <st 9933500 (tokenNum: 15) >]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = t.Sentences\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:39.800189Z",
     "start_time": "2019-10-22T01:39:39.791698Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:40.283367Z",
     "start_time": "2019-10-22T01:39:40.276488Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(t, selected_pos_list = None):\n",
    "    sents = t.Sentences\n",
    "    for sent in sents: # sent = sents[12]\n",
    "        pos = [i[0] for i in sent.get_grain_str('pos')]\n",
    "        tokens = [i[0] for i in sent.get_grain_str('token')]\n",
    "\n",
    "        L = []\n",
    "        SET = extractSET(pos)\n",
    "        for s_e_t in SET:\n",
    "            s, e, t = s_e_t\n",
    "            # if t in selected_pos_list:\n",
    "            if True:\n",
    "                orig = ''.join(tokens[s:e])\n",
    "                L.append((orig, t))\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:40.852926Z",
     "start_time": "2019-10-22T01:39:40.844080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('该', 'r'),\n",
       " ('如何', 'r'),\n",
       " ('理解', 'v'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('市面上', 'n'),\n",
       " ('越来越', 'd'),\n",
       " ('多', 'm'),\n",
       " ('的', 'uj'),\n",
       " ('车', 'n'),\n",
       " ('开始', 'v'),\n",
       " ('搭载', 'v'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " (',', 'x'),\n",
       " ('但是', 'c'),\n",
       " ('大家', 'n'),\n",
       " ('都', 'd'),\n",
       " ('还', 'd'),\n",
       " ('对', 'p'),\n",
       " ('这', 'r'),\n",
       " ('一新', 'd'),\n",
       " ('技术', 'n'),\n",
       " ('有所', 'n'),\n",
       " ('迟疑', 'd'),\n",
       " (',', 'x'),\n",
       " ('我', 'r'),\n",
       " ('将', 'd'),\n",
       " ('在', 'p'),\n",
       " ('这场', 'mq'),\n",
       " ('live', 'eng'),\n",
       " ('中', 'f'),\n",
       " ('从', 'p'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('运行', 'v'),\n",
       " ('原理', 'n'),\n",
       " ('到', 'v'),\n",
       " ('制造', 'v'),\n",
       " ('工艺', 'n'),\n",
       " ('为', 'p'),\n",
       " ('大家', 'n'),\n",
       " ('答疑', 'v'),\n",
       " ('解惑', 'v'),\n",
       " ('!', 'x'),\n",
       " ('•', 'x'),\n",
       " ('什么', 'r'),\n",
       " ('是', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('•', 'x'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('历史', 'n'),\n",
       " ('•', 'x'),\n",
       " ('什么', 'r'),\n",
       " ('是', 'v'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('•', 'x'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('优势', 'n'),\n",
       " ('•', 'x'),\n",
       " ('国内', 's'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('主要', 'b'),\n",
       " ('供应', 'vn'),\n",
       " ('厂家', 'n'),\n",
       " ('•', 'x'),\n",
       " ('大众', 'n'),\n",
       " ('/', 'x'),\n",
       " ('奥迪', 'nz'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('制造', 'v'),\n",
       " ('工艺', 'n'),\n",
       " ('•', 'x'),\n",
       " ('双', 'n'),\n",
       " ('离合', 'v'),\n",
       " ('变速器', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('未来', 't')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(100)\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:39:41.690817Z",
     "start_time": "2019-10-22T01:39:41.674997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('引子', 'n'),\n",
       " (':', 'x'),\n",
       " ('关于', 'p'),\n",
       " ('本次', 'r'),\n",
       " ('Live', 'eng'),\n",
       " ('的', 'uj'),\n",
       " ('适用人群', 'n'),\n",
       " ('和', 'c'),\n",
       " ('一点', 'm'),\n",
       " ('个人', 'n'),\n",
       " ('看法', 'v'),\n",
       " ('-', 'x'),\n",
       " ('个人', 'n'),\n",
       " ('意见', 'n'),\n",
       " (',', 'x'),\n",
       " ('仅供参考', 'l'),\n",
       " ('第一篇', 'm'),\n",
       " (':', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('是', 'v'),\n",
       " ('为了', 'p'),\n",
       " ('什么', 'r'),\n",
       " ('?', 'x'),\n",
       " ('-', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('最终', 'd'),\n",
       " ('目的', 'n'),\n",
       " ('-', 'x'),\n",
       " ('高效', 'a'),\n",
       " ('的', 'uj'),\n",
       " ('课堂', 'n'),\n",
       " ('学习', 'v'),\n",
       " ('-', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('与', 'p'),\n",
       " ('学生', 'n'),\n",
       " ('响应', 'v'),\n",
       " ('-', 'x'),\n",
       " ('教师', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('角色', 'n'),\n",
       " ('转化', 'v'),\n",
       " ('与', 'p'),\n",
       " ('衔接', 'v'),\n",
       " ('-', 'x'),\n",
       " ('本章', 'r'),\n",
       " ('总结', 'n'),\n",
       " ('第二篇', 'm'),\n",
       " (':', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('策划', 'n'),\n",
       " ('与', 'p'),\n",
       " ('开展', 'v'),\n",
       " ('-', 'x'),\n",
       " ('什么', 'r'),\n",
       " ('是', 'v'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('-', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('最终目标', 'l'),\n",
       " ('-', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('与', 'p'),\n",
       " ('德育', 'nr'),\n",
       " ('的', 'uj'),\n",
       " ('相互', 'd'),\n",
       " ('响应', 'v'),\n",
       " ('-', 'x'),\n",
       " ('学生', 'n'),\n",
       " ('情况', 'n'),\n",
       " ('与', 'p'),\n",
       " ('活动', 'vn'),\n",
       " ('开展', 'v'),\n",
       " ('-', 'x'),\n",
       " ('活动', 'vn'),\n",
       " ('规范', 'n'),\n",
       " ('制定', 'v'),\n",
       " ('与', 'p'),\n",
       " ('学生', 'n'),\n",
       " ('潜能', 'v'),\n",
       " ('释放', 'v'),\n",
       " ('-', 'x'),\n",
       " ('活动', 'vn'),\n",
       " ('效果', 'n'),\n",
       " ('评价', 'n'),\n",
       " ('与', 'p'),\n",
       " ('总结', 'n'),\n",
       " ('-', 'x'),\n",
       " ('活动', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('展示', 'v'),\n",
       " ('与', 'p'),\n",
       " ('交流', 'n'),\n",
       " ('提升', 'v'),\n",
       " ('-', 'x'),\n",
       " ('本章', 'r'),\n",
       " ('总结', 'n'),\n",
       " ('第三篇', 'm'),\n",
       " (':', 'x'),\n",
       " ('自我', 'r'),\n",
       " ('提升', 'v'),\n",
       " (':', 'x'),\n",
       " ('活动', 'vn'),\n",
       " ('策划者', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('专业', 'n'),\n",
       " ('发展', 'vn'),\n",
       " ('-', 'x'),\n",
       " ('教师', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('知识', 'v'),\n",
       " ('体系', 'n'),\n",
       " ('构建', 'v'),\n",
       " ('-', 'x'),\n",
       " ('教师', 'n'),\n",
       " ('的', 'uj'),\n",
       " ('活动', 'vn'),\n",
       " ('参与', 'v'),\n",
       " ('与', 'p'),\n",
       " ('经验交流', 'n'),\n",
       " ('提升', 'v'),\n",
       " ('-', 'x'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('合作', 'vn'),\n",
       " ('与', 'p'),\n",
       " ('双赢', 'nr'),\n",
       " ('-', 'x'),\n",
       " ('在', 'p'),\n",
       " ('教育', 'vn'),\n",
       " ('活动', 'vn'),\n",
       " ('中', 'f'),\n",
       " ('进步', 'd'),\n",
       " ('与', 'p'),\n",
       " ('反思', 'v'),\n",
       " ('-', 'x'),\n",
       " ('做', 'v'),\n",
       " ('复合型', 'b'),\n",
       " ('的', 'uj'),\n",
       " ('老师', 'n'),\n",
       " ('结束语', 'n'),\n",
       " ('-', 'x'),\n",
       " ('相关', 'v'),\n",
       " ('问题', 'n'),\n",
       " ('交流', 'n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(1002)\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:36:50.688775Z",
     "start_time": "2019-10-21T06:36:50.675804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stopwords = [i[0] for i in pd.read_csv('nlptext/sources/stopwords/中文停用词表.txt', header = None).values]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:36:28.201355Z",
     "start_time": "2019-10-21T06:36:28.187868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1395"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:36:32.183323Z",
     "start_time": "2019-10-21T06:36:32.165756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--',\n",
       " '?',\n",
       " '“',\n",
       " '”',\n",
       " '》',\n",
       " '－－',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a's\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " \"c's\",\n",
       " 'currently',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " \"i'd\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " 'thats',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t's\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'zero',\n",
       " 'zt',\n",
       " 'ZT',\n",
       " 'zz',\n",
       " 'ZZ',\n",
       " '一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即或',\n",
       " '即若',\n",
       " '却不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她的',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " ...]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.663px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
