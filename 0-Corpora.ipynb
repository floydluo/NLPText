{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese Sample \n",
    "\n",
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n",
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_en_sample/wiki_en_sample.txt\n",
      "Total Num of All    Tokens 47166\n",
      "Total Num of Unique Tokens 8620\n",
      "CORPUS\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 47166\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/wiki_en_sample/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_en_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 8620\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_en_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4313337\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4313337\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439223084\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254754734\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/word/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'许多 哲学家 相信 数学 在 经验 上 不具 可否 证性 , 且 因此 不是 卡尔 · 波普尔 所 定义 的 科学 。 但 在 1930 年代 时 , 在 数理逻辑 上 的 重大进展 显示 数学 不能 归并 至 逻辑 内 , 且 波普尔 推断 「 大部份 的 数学 定律 , 如 物理 及 生物学 一样 , 是 假设 演绎 的 : 纯数学 因此 变得 更 接近 其 假设 为 猜测 的 自然科学 , 比 它 现在 看起来 更 接近 。 」 然而 , 其他 的 思想家 , 如 较 著名 的 拉 卡托斯 , 便 提供 了 一个 关于 数学 本身 的 可否 证性 版本 。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 19\n",
    "obj = Sentence(locidx)\n",
    "obj.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init from origin corpus\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiEnglish/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 10\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiEnglish/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiEnglish/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiEnglish/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 37101011\n",
      "SENT\tread from pickle file : data/WikiEnglish/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 37101011\n",
      "TOKEN\tread from pickle file : data/WikiEnglish/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2429172233\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init from preprocessed data\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiEnglish/word/'\n",
    "min_token_freq = 10\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lincoln removed McClellan in March 1862 , after McClellan offered unsolicited political advice . In July Lincoln elevated Henry Halleck . Lincoln appointed John Pope as head of the new Army of Virginia . Pope complied with Lincoln 's desire to advance on Richmond from the north , thus protecting Washington from counterattack .\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(488)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu EMR Data (Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n",
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_OperName.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/DailyRecDatas_Text.p\n",
      "100000 -- 24 2019-07-30 16:18:20.403440 \t 3154859 \t 2760 2760 2760\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 68 2019-07-30 16:20:49.838954 \t 7155597 \t 3052 3052 3052\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 7 2019-07-30 16:23:14.390641 \t 11112892 \t 3159 3159 3159\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "400000 -- 39 2019-07-30 16:25:33.842378 \t 14845292 \t 3253 3253 3253\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "500000 -- 43 2019-07-30 16:27:49.810102 \t 18795500 \t 3320 3320 3320\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "600000 -- 21 2019-07-30 16:29:45.730380 \t 21826015 \t 3424 3424 3424\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "700000 -- 8 2019-07-30 16:31:38.027917 \t 25189347 \t 3443 3443 3443\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "800000 -- 72 2019-07-30 16:33:48.805814 \t 28586018 \t 3468 3468 3468\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "900000 -- 23 2019-07-30 16:36:08.149465 \t 32826030 \t 3507 3507 3507\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_Text.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "1000000 -- 43 2019-07-30 16:37:52.993866 \t 36296262 \t 3537 3537 3537\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "Total Num of All    Tokens 36455911\n",
      "Total Num of Unique Tokens 3539\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 44\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 213908\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1005357\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36455911\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuCorpus/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3539\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### LuohuCorpus ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "\n",
    "Corpus2GroupMethod = '.p'\n",
    "\n",
    "Group2TextMethod   = 'element'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/fudan/C39-Sports\n",
      "corpus/fudan/C31-Enviornment\n",
      "corpus/fudan/C38-Politics\n",
      "corpus/fudan/C34-Economy\n",
      "corpus/fudan/C32-Agriculture\n",
      "Total Num of All    Tokens 17252831\n",
      "Total Num of Unique Tokens 401556\n",
      "CORPUS\tit is Dumped into file: data/fudan/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/fudan/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/fudan/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 5885\n",
      "SENT\tit is Dumped into file: data/fudan/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 5885\n",
      "TOKEN\tit is Dumped into file: data/fudan/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 17252831\n",
      "**************************************** \n",
      "\n",
      "token   \tis Dumped into file: data/fudan/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 401556\n",
      "\t\tWrite to: data/fudan/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = []\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sentidx = 4000\n",
    "sent = Sentence(sentidx)\n",
    "sent.IdxGroup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
