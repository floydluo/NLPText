{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese Sample \n",
    "\n",
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:09.033721Z",
     "start_time": "2019-08-27T13:14:05.815286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.996 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:20.622831Z",
     "start_time": "2019-08-27T13:14:20.606740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'data/wiki_cn_sample/word/Pyramid/_file/token.txt',\n",
       " 'pos': 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.Channel_Hyper_Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:16:05.599625Z",
     "start_time": "2019-08-27T13:16:05.594457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sent2TokenMethod': 'pos',\n",
       " 'TOKENLevel': 'word',\n",
       " 'Channel_Hyper_Path': {'token': 'data/wiki_cn_sample/word/Pyramid/_file/token.txt',\n",
       "  'pos': 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt'},\n",
       " 'length': 8407}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:14:41.451415Z",
     "start_time": "2019-08-27T13:14:41.445707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text2SentMethod': 'whole',\n",
       " 'EndIDXTokens': array([  79,  184,  291,  425,  562,  626,  686,  720,  782,  869,  977,\n",
       "        1174, 1224, 1317, 1435, 1555, 1705, 1831, 1966, 2072, 2238, 2449,\n",
       "        2566, 2582, 2645, 2685, 2755, 2866, 2985, 3008, 3021, 3069, 3288,\n",
       "        3446, 3576, 3617, 3743, 3918, 4003, 4075, 4191, 4200, 4233, 4285,\n",
       "        4322, 4357, 4402, 4422, 4436, 4472, 4538, 4629, 4699, 4791, 4868,\n",
       "        5052, 5081, 5187, 5318, 5444, 5554, 5665, 5695, 5751, 5828, 5911,\n",
       "        5943, 6097, 6139, 6253, 6384, 6471, 6606, 6649, 6775, 6825, 6830,\n",
       "        6938, 6981, 7048, 7199, 7297, 7342, 7415, 7573, 7616, 7704, 7787,\n",
       "        7797, 7828, 7905, 7923, 7958, 8011, 8085, 8137, 8191, 8238, 8365,\n",
       "        8407]),\n",
       " 'data/wiki_cn_sample/word/Pyramid/_file/token.txt': array([  472,  1087,  1724,  2376,  3089,  3407,  3740,  3943,  4297,\n",
       "         4802,  5408,  6530,  6820,  7344,  8042,  8708,  9540, 10263,\n",
       "        10994, 11608, 12532, 13711, 14359, 14454, 14794, 15020, 15426,\n",
       "        16057, 16666, 16805, 16879, 17099, 18343, 19230, 19965, 20189,\n",
       "        20933, 21916, 22395, 22768, 23369, 23421, 23607, 23908, 24123,\n",
       "        24352, 24599, 24731, 24832, 25046, 25431, 25974, 26399, 26958,\n",
       "        27396, 28423, 28593, 29233, 29977, 30742, 31397, 32062, 32252,\n",
       "        32582, 33069, 33555, 33734, 34699, 34957, 35701, 36532, 37077,\n",
       "        37891, 38143, 38943, 39235, 39262, 39901, 40147, 40563, 41503,\n",
       "        42082, 42351, 42766, 43693, 43961, 44490, 44895, 44954, 45139,\n",
       "        45575, 45676, 45869, 46193, 46610, 46929, 47227, 47496, 48196,\n",
       "        48421]),\n",
       " 'data/wiki_cn_sample/word/Pyramid/_file/pos-bioes.txt': array([  279,   649,  1027,  1516,  2011,  2249,  2462,  2579,  2801,\n",
       "         3103,  3476,  4169,  4346,  4674,  5087,  5509,  6036,  6473,\n",
       "         6954,  7334,  7916,  8665,  9075,  9132,  9348,  9481,  9727,\n",
       "        10120, 10544, 10628, 10674, 10847, 11628, 12185, 12633, 12775,\n",
       "        13228, 13838, 14135, 14397, 14815, 14847, 14965, 15148, 15277,\n",
       "        15401, 15561, 15629, 15677, 15802, 16037, 16356, 16604, 16935,\n",
       "        17214, 17864, 17970, 18348, 18819, 19265, 19664, 20048, 20152,\n",
       "        20353, 20631, 20930, 21043, 21604, 21752, 22154, 22622, 22937,\n",
       "        23413, 23566, 24008, 24192, 24209, 24591, 24737, 24976, 25518,\n",
       "        25869, 26023, 26283, 26851, 27006, 27316, 27598, 27634, 27749,\n",
       "        28025, 28091, 28215, 28405, 28669, 28847, 29039, 29209, 29662,\n",
       "        29817]),\n",
       " 'length': 100}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject.SENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_en_sample/wiki_en_sample.txt\n",
      "Total Num of All    Tokens 47166\n",
      "Total Num of Unique Tokens 8620\n",
      "CORPUS\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 47166\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/wiki_en_sample/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_en_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 8620\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_en_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T18:02:40.940475Z",
     "start_time": "2019-08-29T13:04:00.771489Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/WikiChinese/zhwiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.637 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 -- 109 2019-08-29 21:11:20.086077 \t 14309025 \t 8647 8647 8647\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 229 2019-08-29 21:18:29.305268 \t 27054049 \t 10018 10018 10018\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 89 2019-08-29 21:28:27.699765 \t 39007078 \t 10947 10947 10947\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "400000 -- 51 2019-08-29 21:38:31.200964 \t 50755371 \t 11728 11728 11728\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "500000 -- 180 2019-08-29 21:48:06.176919 \t 62454145 \t 12306 12306 12306\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "600000 -- 23 2019-08-29 21:57:14.940143 \t 73773949 \t 12708 12708 12708\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "700000 -- 66 2019-08-29 22:06:42.479607 \t 84920503 \t 13172 13172 13172\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "800000 -- 283 2019-08-29 22:15:44.236144 \t 95600635 \t 13594 13594 13594\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "900000 -- 224 2019-08-29 22:24:56.213239 \t 105906774 \t 14001 14001 14001\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1000000 -- 148 2019-08-29 22:33:18.793415 \t 116116367 \t 14284 14284 14284\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1100000 -- 24 2019-08-29 22:42:33.524280 \t 126860053 \t 14533 14533 14533\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1200000 -- 56 2019-08-29 22:51:19.237393 \t 137105385 \t 14818 14818 14818\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1300000 -- 50 2019-08-29 23:00:00.890198 \t 147072557 \t 15083 15083 15083\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1400000 -- 15 2019-08-29 23:07:31.102643 \t 157407979 \t 15446 15446 15446\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1500000 -- 154 2019-08-29 23:13:45.685027 \t 167074979 \t 15634 15634 15634\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1600000 -- 42 2019-08-29 23:20:02.858721 \t 177009511 \t 16489 16489 16489\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1700000 -- 4 2019-08-29 23:26:09.308126 \t 186387768 \t 16664 16664 16664\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1800000 -- 83 2019-08-29 23:32:29.109867 \t 196234799 \t 16820 16820 16820\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1900000 -- 202 2019-08-29 23:38:44.561362 \t 206146558 \t 17022 17022 17022\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2000000 -- 53 2019-08-29 23:44:58.538516 \t 215888935 \t 17280 17280 17280\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2100000 -- 86 2019-08-29 23:51:39.597352 \t 225881745 \t 17436 17436 17436\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2200000 -- 82 2019-08-29 23:58:01.342594 \t 235659763 \t 17590 17590 17590\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2300000 -- 49 2019-08-30 00:04:05.978313 \t 245616705 \t 17703 17703 17703\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2400000 -- 7 2019-08-30 00:09:04.180148 \t 253814755 \t 17847 17847 17847\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2500000 -- 85 2019-08-30 00:12:01.666521 \t 258689969 \t 17924 17924 17924\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2600000 -- 86 2019-08-30 00:17:22.828123 \t 267578831 \t 18027 18027 18027\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2700000 -- 110 2019-08-30 00:22:27.074834 \t 275901904 \t 18106 18106 18106\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2800000 -- 85 2019-08-30 00:27:09.321282 \t 282415417 \t 18137 18137 18137\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2900000 -- 65 2019-08-30 00:31:35.004765 \t 289629858 \t 18213 18213 18213\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3000000 -- 66 2019-08-30 00:36:53.423466 \t 298947711 \t 18318 18318 18318\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3100000 -- 40 2019-08-30 00:42:48.065362 \t 308323976 \t 18430 18430 18430\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3200000 -- 151 2019-08-30 00:48:53.405724 \t 318422310 \t 18535 18535 18535\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3300000 -- 33 2019-08-30 00:55:01.026431 \t 328772138 \t 18638 18638 18638\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3400000 -- 217 2019-08-30 01:01:08.743846 \t 338846262 \t 18754 18754 18754\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3500000 -- 72 2019-08-30 01:07:15.333983 \t 348969630 \t 18867 18867 18867\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3600000 -- 219 2019-08-30 01:13:16.147061 \t 359379559 \t 18985 18985 18985\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3700000 -- 25 2019-08-30 01:19:23.672840 \t 369324191 \t 19106 19106 19106\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3800000 -- 256 2019-08-30 01:25:41.608379 \t 379153049 \t 19274 19274 19274\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3900000 -- 63 2019-08-30 01:32:05.426158 \t 389331809 \t 19372 19372 19372\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4000000 -- 21 2019-08-30 01:38:25.280387 \t 399482719 \t 19492 19492 19492\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4100000 -- 32 2019-08-30 01:44:08.309493 \t 408562199 \t 19670 19670 19670\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4200000 -- 88 2019-08-30 01:50:20.292630 \t 418511781 \t 19838 19838 19838\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4300000 -- 3 2019-08-30 01:56:35.004905 \t 428711196 \t 19971 19971 19971\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "4400000 -- 29 2019-08-30 02:01:37.770222 \t 437812808 \t 20085 20085 20085\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "Total Num of All    Tokens 439594499\n",
      "Total Num of Unique Tokens 20134\n",
      "CORPUS\tit is Dumped into file: data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tit is Dumped into file: data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tit is Dumped into file: data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594499\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/WikiChinese/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/WikiChinese/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 20134\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:01.467619Z",
     "start_time": "2019-09-04T10:02:01.103775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594499\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:02.100700Z",
     "start_time": "2019-09-04T10:02:02.086128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11146"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BasicObject.TokenVocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:20.748054Z",
     "start_time": "2019-09-04T10:02:19.087062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: pinyin\n",
      "Current Channel is        \t pinyin\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 6\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: stroke-n3t6-f236\n",
      "For channel: | stroke | build GrainUnique and LookUp\n",
      "\t\tFor Channel: stroke \t 0 2019-09-04 18:02:19.095709\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.voc\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.tsv\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.lkp\n",
      "\t\tWrite to: data/WikiChinese/char/Vocab/F10/stroke-n3t6-f236.freq\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'pinyin':  {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 10},\n",
    "    'subcomp': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 1152},\n",
    "    'stroke': { 'use': True, 'Min_Ngram': 3, 'Max_Ngram': 6, 'end_grain': False,  'min_grain_freq' : 236},\n",
    "    # 'pos':     {'use': True, 'tagScheme': 'BIOES'}\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:02:23.872444Z",
     "start_time": "2019-09-04T10:02:23.833558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinyin\n",
      "GrainNum: 5333\n",
      "count    11146.000000\n",
      "mean         3.095371\n",
      "std          2.109298\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          3.000000\n",
      "max         10.000000\n",
      "dtype: float64\n",
      "34501\n",
      "[64553112 45514885 33382420 ...       10       10       10]\n",
      "======================================\n",
      "\n",
      "\n",
      "subcomp\n",
      "GrainNum: 10001\n",
      "count    11146.000000\n",
      "mean         5.738651\n",
      "std          5.190094\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          5.000000\n",
      "75%          9.000000\n",
      "max         50.000000\n",
      "dtype: float64\n",
      "63963\n",
      "[53308092 44094327 42421777 ...     1155     1155     1152]\n",
      "1152\n",
      "======================================\n",
      "\n",
      "\n",
      "stroke\n",
      "GrainNum: 10005\n",
      "count    11146.000000\n",
      "mean        23.517046\n",
      "std         17.826554\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%         22.000000\n",
      "75%         37.000000\n",
      "max        106.000000\n",
      "dtype: float64\n",
      "262121\n",
      "[122034762  83201490  78056695 ...       236       236       236]\n",
      "236\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    try:\n",
    "        print(BasicObject.getFreq(fld, **CS[fld])[10000])\n",
    "    except:\n",
    "        pass\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T01:30:55.004508Z",
     "start_time": "2019-08-30T01:30:54.995241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'就 此 , 社 会 和 历 史 的 现 象 , 便 被 赋 予 一 种 在 哲 学 史 上 还 是 崭 新 的 显 赫 地 位 。 他 还 将 伦 理 学 划 归 到 这 个 领 域 , 从 而 在 伦 理 学 理 论 和 对 思 想 的 理 解 中 提 出 重 要 的 路 线 。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(100)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T13:05:15.303996Z",
     "start_time": "2019-08-26T13:05:12.115548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254754734\n",
      "**************************************** \n",
      "\n",
      "390106\n",
      "[16697636 10295030  9546380 ...       10       10       10]\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "##########################\n",
    "min_token_freq = 10\n",
    "##########################\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/word/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n",
    "\n",
    "print(len(BasicObject.TokenVocab[0]))\n",
    "print(BasicObject.idx2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T11:50:26.513936Z",
     "start_time": "2019-08-26T11:50:15.087545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char-n1t1-f15\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-08-26 19:50:15.093016\n",
      "\t\tFor Channel: char \t 100000 2019-08-26 19:50:16.215839\n",
      "\t\tFor Channel: char \t 200000 2019-08-26 19:50:17.275196\n",
      "\t\tFor Channel: char \t 300000 2019-08-26 19:50:18.376059\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.voc\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.tsv\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.lkp\n",
      "\t\tWrite to: data/WikiChinese/word/Vocab/F10/char-n1t1-f15.freq\n",
      "Deal with the Channel: pinyin\n",
      "Current Channel is        \t pinyin\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 4\n",
      "Deal with the Channel: stroke\n",
      "Current Channel is        \t stroke\n",
      "Current Channel Max_Ngram \t 6\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 1, 'end_grain': False,  'min_grain_freq' : 15},\n",
    "    'pinyin':  {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 15856},\n",
    "    'subcomp': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 4, 'end_grain': False,  'min_grain_freq' : 21759},\n",
    "    'stroke':  {'use': True, 'Min_Ngram': 3, 'Max_Ngram': 6, 'end_grain': False,  'min_grain_freq' : 19719},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T11:50:32.477202Z",
     "start_time": "2019-08-26T11:50:31.920671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char\n",
      "GrainNum: 10085\n",
      "count    390106.000000\n",
      "mean          2.960295\n",
      "std           1.583327\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max          43.000000\n",
      "dtype: float64\n",
      "1154829\n",
      "[16697636 10417042  9546380 ...       15       15       15]\n",
      "15\n",
      "======================================\n",
      "\n",
      "\n",
      "pinyin\n",
      "GrainNum: 10002\n",
      "count    390106.000000\n",
      "mean         12.527826\n",
      "std           6.721878\n",
      "min           0.000000\n",
      "25%           8.000000\n",
      "50%          11.000000\n",
      "75%          16.000000\n",
      "max         142.000000\n",
      "dtype: float64\n",
      "4887180\n",
      "[62864529 44459023 32781276 ...    15856    15856    15856]\n",
      "15856\n",
      "======================================\n",
      "\n",
      "\n",
      "subcomp\n",
      "GrainNum: 10002\n",
      "count    390106.000000\n",
      "mean         14.062181\n",
      "std           9.237083\n",
      "min           0.000000\n",
      "25%           7.000000\n",
      "50%          12.000000\n",
      "75%          18.000000\n",
      "max         149.000000\n",
      "dtype: float64\n",
      "5485741\n",
      "[52176468 43015399 41913615 ...    21760    21759    21759]\n",
      "21759\n",
      "======================================\n",
      "\n",
      "\n",
      "stroke\n",
      "GrainNum: 10001\n",
      "count    390106.000000\n",
      "mean         54.171041\n",
      "std          37.389900\n",
      "min           0.000000\n",
      "25%          30.000000\n",
      "50%          54.000000\n",
      "75%          77.000000\n",
      "max         493.000000\n",
      "dtype: float64\n",
      "21132448\n",
      "[120506684  85768534  80332806 ...     19736     19727     19719]\n",
      "19719\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    print(BasicObject.getFreq(fld, **CS[fld])[10000])\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init from origin corpus\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiEnglish/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 10\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:35:18.190191Z",
     "start_time": "2019-08-29T08:35:09.436457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiEnglish/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiEnglish/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiEnglish/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 37101011\n",
      "SENT\tread from pickle file : data/WikiEnglish/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 37101011\n",
      "TOKEN\tread from pickle file : data/WikiEnglish/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2429172233\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init from preprocessed data\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiEnglish/word/'\n",
    "min_token_freq = 30\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:35:18.195307Z",
     "start_time": "2019-08-29T08:35:18.191858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649068\n",
      "[131996922 123504303  98456073 ...        30        30        30]\n"
     ]
    }
   ],
   "source": [
    "print(len(BasicObject.TokenVocab[0]))\n",
    "print(BasicObject.idx2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:36:47.368714Z",
     "start_time": "2019-08-29T08:35:18.197598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: char-n1t3-f2514\n",
      "For channel: | char | build GrainUnique and LookUp\n",
      "\t\tFor Channel: char \t 0 2019-08-29 16:35:18.212042\n",
      "\t\tFor Channel: char \t 100000 2019-08-29 16:35:20.853349\n",
      "\t\tFor Channel: char \t 200000 2019-08-29 16:35:23.620032\n",
      "\t\tFor Channel: char \t 300000 2019-08-29 16:35:26.439164\n",
      "\t\tFor Channel: char \t 400000 2019-08-29 16:35:29.129255\n",
      "\t\tFor Channel: char \t 500000 2019-08-29 16:35:32.058197\n",
      "\t\tFor Channel: char \t 600000 2019-08-29 16:35:34.759977\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.voc\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.tsv\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.lkp\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/char-n1t3-f2514.freq\n",
      "Deal with the Channel: phoneme\n",
      "Current Channel is        \t phoneme\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: phoneme-n1t3-f1979\n",
      "For channel: | phoneme | build GrainUnique and LookUp\n",
      "\t\tFor Channel: phoneme \t 0 2019-08-29 16:36:28.778297\n",
      "\t\tFor Channel: phoneme \t 100000 2019-08-29 16:36:30.528603\n",
      "\t\tFor Channel: phoneme \t 200000 2019-08-29 16:36:31.695531\n",
      "\t\tFor Channel: phoneme \t 300000 2019-08-29 16:36:32.661209\n",
      "\t\tFor Channel: phoneme \t 400000 2019-08-29 16:36:33.540937\n",
      "\t\tFor Channel: phoneme \t 500000 2019-08-29 16:36:34.383411\n",
      "\t\tFor Channel: phoneme \t 600000 2019-08-29 16:36:37.300633\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.voc\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.tsv\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.lkp\n",
      "\t\tWrite to: data/WikiEnglish/word/Vocab/F30/phoneme-n1t3-f1979.freq\n"
     ]
    }
   ],
   "source": [
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 3, 'end_grain': False,  'min_grain_freq' : 2514},\n",
    "    'phoneme': {'use': True, 'Min_Ngram': 1, 'Max_Ngram': 3, 'end_grain': False,  'min_grain_freq' : 1979},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE_FOR_WIKICN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:37:02.502701Z",
     "start_time": "2019-08-29T08:37:02.259120Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char\n",
      "GrainNum: 20003\n",
      "count    649068.000000\n",
      "mean         19.552623\n",
      "std           9.138646\n",
      "min           0.000000\n",
      "25%          12.000000\n",
      "50%          18.000000\n",
      "75%          24.000000\n",
      "max         243.000000\n",
      "dtype: float64\n",
      "12690982\n",
      "[1163857687  829148338  801646984 ...       2514       2514       2514]\n",
      "2514\n",
      "======================================\n",
      "\n",
      "\n",
      "phoneme\n",
      "GrainNum: 20001\n",
      "count    649068.00000\n",
      "mean          4.10151\n",
      "std           6.71414\n",
      "min           1.00000\n",
      "25%           1.00000\n",
      "50%           1.00000\n",
      "75%           1.00000\n",
      "max          54.00000\n",
      "dtype: float64\n",
      "2662159\n",
      "[773536812 609749367 509849261 ...      1980      1979      1979]\n",
      "1979\n",
      "======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "CS = BasicObject.CHANNEL_SETTINGS\n",
    "for fld in CS: \n",
    "    if fld == 'token': continue\n",
    "    # fld = 'stroke'\n",
    "    idx2grain = BasicObject.getGrainVocab(fld, **CS[fld])[0]\n",
    "\n",
    "    LKP = BasicObject.getLookUp(fld, **CS[fld])[0]\n",
    "    # LKP\n",
    "    leng_of_char = np.array([len(i) for i in LKP])\n",
    "\n",
    "    leng = pd.Series(leng_of_char)\n",
    "    print(fld)\n",
    "    print('GrainNum:', len(idx2grain))\n",
    "    print(leng.describe())\n",
    "    print(leng.sum())\n",
    "    print(BasicObject.getFreq(fld, **CS[fld]))\n",
    "    print(BasicObject.getFreq(fld, **CS[fld])[20000])\n",
    "    print('======================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T08:32:26.869876Z",
     "start_time": "2019-08-29T08:32:26.857005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lincoln removed McClellan in March 1862 , after McClellan offered unsolicited political advice . In July Lincoln elevated Henry Halleck . Lincoln appointed John Pope as head of the new Army of Virginia . Pope complied with Lincoln 's desire to advance on Richmond from the north , thus protecting Washington from counterattack .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(488)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu EMR Data (Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n",
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_OperName.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/DailyRecDatas_Text.p\n",
      "100000 -- 24 2019-07-30 16:18:20.403440 \t 3154859 \t 2760 2760 2760\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 68 2019-07-30 16:20:49.838954 \t 7155597 \t 3052 3052 3052\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 7 2019-07-30 16:23:14.390641 \t 11112892 \t 3159 3159 3159\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "400000 -- 39 2019-07-30 16:25:33.842378 \t 14845292 \t 3253 3253 3253\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "500000 -- 43 2019-07-30 16:27:49.810102 \t 18795500 \t 3320 3320 3320\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "600000 -- 21 2019-07-30 16:29:45.730380 \t 21826015 \t 3424 3424 3424\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "700000 -- 8 2019-07-30 16:31:38.027917 \t 25189347 \t 3443 3443 3443\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "800000 -- 72 2019-07-30 16:33:48.805814 \t 28586018 \t 3468 3468 3468\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "900000 -- 23 2019-07-30 16:36:08.149465 \t 32826030 \t 3507 3507 3507\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_Text.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "1000000 -- 43 2019-07-30 16:37:52.993866 \t 36296262 \t 3537 3537 3537\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "Total Num of All    Tokens 36455911\n",
      "Total Num of Unique Tokens 3539\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 44\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 213908\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1005357\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36455911\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuCorpus/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3539\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### LuohuCorpus ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "\n",
    "Corpus2GroupMethod = '.p'\n",
    "\n",
    "Group2TextMethod   = 'element'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fudan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/fudan/C39-Sports\n",
      "corpus/fudan/C31-Enviornment\n",
      "corpus/fudan/C38-Politics\n",
      "corpus/fudan/C34-Economy\n",
      "corpus/fudan/C32-Agriculture\n",
      "Total Num of All    Tokens 17252831\n",
      "Total Num of Unique Tokens 401556\n",
      "CORPUS\tit is Dumped into file: data/fudan/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/fudan/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/fudan/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 5885\n",
      "SENT\tit is Dumped into file: data/fudan/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 5885\n",
      "TOKEN\tit is Dumped into file: data/fudan/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 17252831\n",
      "**************************************** \n",
      "\n",
      "token   \tis Dumped into file: data/fudan/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 401556\n",
      "\t\tWrite to: data/fudan/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = []\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sentidx = 4000\n",
    "sent = Sentence(sentidx)\n",
    "sent.IdxGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.522745Z",
     "start_time": "2019-09-04T06:22:26.837828Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/fudan/C39-Sports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.660 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/fudan/C31-Enviornment\n",
      "corpus/fudan/C38-Politics\n",
      "corpus/fudan/C34-Economy\n",
      "corpus/fudan/C32-Agriculture\n",
      "Total Num of All    Tokens 29798646\n",
      "Total Num of Unique Tokens 5828\n",
      "CORPUS\tit is Dumped into file: data/fudan/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/fudan/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/fudan/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 5885\n",
      "SENT\tit is Dumped into file: data/fudan/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 5885\n",
      "TOKEN\tit is Dumped into file: data/fudan/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 29798646\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/fudan/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/fudan/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/fudan/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 5828\n",
      "\t\tWrite to: data/fudan/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/fudan/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:18:45.492616Z",
     "start_time": "2019-09-04T06:18:45.481986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/fudan/C39-Sports\n",
      "1209\n",
      "corpus/fudan/C31-Enviornment\n",
      "1068\n",
      "corpus/fudan/C38-Politics\n",
      "1017\n",
      "corpus/fudan/C34-Economy\n",
      "1596\n",
      "corpus/fudan/C32-Agriculture\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "from nlptext.folder import Folder\n",
    "for i in range(BasicObject.GROUP['length']):\n",
    "    f = Folder(i)\n",
    "    print(f.name)\n",
    "    s, e = f.IdxSentStartEnd\n",
    "    print(e - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.536292Z",
     "start_time": "2019-09-04T06:32:06.524055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'group_idx' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-39c361d20571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                  \u001b[0mSent2TokenMethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENLevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_token_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_token_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                  \u001b[0muse_hyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_hyper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                  anno = False, anno_keywords = anno_keywords)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/nlptext/nlptext/base.py\u001b[0m in \u001b[0;36mINIT\u001b[0;34m(cls, CORPUSPath, Corpus2GroupMethod, Group2TextMethod, Text2SentMethod, Sent2TokenMethod, TOKENLevel, min_token_freq, use_hyper, Channel_Dep_Methods, Channel_Dep_TagSets, anno, anno_keywords)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# if there is an error, folderIdx will be referenced before assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mlenCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mCORPUS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EndIDXGroups'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlenCorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'group_idx' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/newsgroup/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir'\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:32:06.536918Z",
     "start_time": "2019-09-04T06:22:57.048Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlptext.folder import Folder\n",
    "for i in range(BasicObject.GROUP['length']):\n",
    "    f = Folder(i)\n",
    "    print(f.name)\n",
    "    s, e = f.IdxSentStartEnd\n",
    "    print(e - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
