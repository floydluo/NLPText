{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese Sample \n",
    "\n",
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 13878\n",
      "Total Num of Unique Tokens 1087\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 13878\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1087\n",
      "\t\tWrite to: data/wiki_cn_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_cn_sample/sample_wiki_smp.txt\n",
      "Total Num of All    Tokens 8407\n",
      "Total Num of Unique Tokens 2000\n",
      "CORPUS\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tit is Dumped into file: data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/wiki_cn_sample/word/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_cn_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2000\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_cn_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki_en_sample/wiki_en_sample.txt\n",
      "Total Num of All    Tokens 47166\n",
      "Total Num of Unique Tokens 8620\n",
      "CORPUS\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 500\n",
      "TOKEN\tit is Dumped into file: data/wiki_en_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 47166\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/wiki_en_sample/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/pos_en-bioes.tsv\n",
      "token   \tis Dumped into file: data/wiki_en_sample/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 8620\n",
      "\t\tWrite to: data/wiki_en_sample/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/wiki_en_sample/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/WikiChinese/zhwiki_smp.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4313337\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4313337\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439223084\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiChinese/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254754734\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "Data_Dir = 'data/WikiChinese/word/'\n",
    "min_token_freq = 10\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 19\n",
    "obj = Sentence(locidx)\n",
    "obj.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lincoln removed McClellan in March 1862 , after McClellan offered unsolicited political advice . In July Lincoln elevated Henry Halleck . Lincoln appointed John Pope as head of the new Army of Virginia . Pope complied with Lincoln 's desire to advance on Richmond from the north , thus protecting Washington from counterattack .\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(488)\n",
    "\n",
    "sent.get_grain_str('token')\n",
    "sent.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init from origin corpus\n",
    "\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/WikiEnglish/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'pos_en'\n",
    "TOKENLevel = 'word'\n",
    "min_token_freq = 10\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiEnglish/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiEnglish/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiEnglish/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 37101011\n",
      "SENT\tread from pickle file : data/WikiEnglish/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 37101011\n",
      "TOKEN\tread from pickle file : data/WikiEnglish/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2429172233\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init from preprocessed data\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "Data_Dir = 'data/WikiEnglish/word/'\n",
    "min_token_freq = 10\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu EMR Data (Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n",
      "100000 -- 90 2019-07-30 14:46:39.860947 \t 2990782 \t 1997 1997 1997\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "200000 -- 57 2019-07-30 14:49:02.120300 \t 6711659 \t 2403 2403 2403\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 41 2019-07-30 14:50:51.324409 \t 10237291 \t 2803 2803 2803\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "400000 -- 13 2019-07-30 14:52:43.698536 \t 12978616 \t 3027 3027 3027\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "500000 -- 93 2019-07-30 14:54:33.806349 \t 15868164 \t 3061 3061 3061\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "600000 -- 24 2019-07-30 14:56:34.996580 \t 18774290 \t 3061 3061 3061\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "700000 -- 101 2019-07-30 14:58:51.354449 \t 22732371 \t 3140 3140 3140\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "800000 -- 9 2019-07-30 15:01:04.774708 \t 26760476 \t 3142 3142 3142\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "900000 -- 12 2019-07-30 15:02:25.367223 \t 29502098 \t 3142 3142 3142\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "1000000 -- 16 2019-07-30 15:04:13.084691 \t 32725922 \t 3247 3247 3247\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1100000 -- 29 2019-07-30 15:05:50.328103 \t 35793253 \t 3247 3247 3247\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "1200000 -- 28 2019-07-30 15:08:04.012949 \t 39216386 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "1300000 -- 11 2019-07-30 15:09:26.641335 \t 41907991 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1400000 -- 10 2019-07-30 15:11:50.377120 \t 45575133 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "1500000 -- 33 2019-07-30 15:13:02.501302 \t 48492338 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "1600000 -- 26 2019-07-30 15:15:20.912692 \t 52033088 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "1700000 -- 63 2019-07-30 15:16:53.608976 \t 55156126 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "1800000 -- 20 2019-07-30 15:18:52.738263 \t 58502008 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "1900000 -- 42 2019-07-30 15:20:45.185016 \t 62089410 \t 3250 3250 3250\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2000000 -- 71 2019-07-30 15:22:52.183227 \t 65749482 \t 3279 3279 3279\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DailyRecdatas.p\n",
      "2100000 -- 14 2019-07-30 15:24:57.572770 \t 69574300 \t 3382 3382 3382\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2200000 -- 55 2019-07-30 15:27:19.740367 \t 73682582 \t 3455 3455 3455\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2300000 -- 12 2019-07-30 15:29:39.241329 \t 77735179 \t 3494 3494 3494\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "2400000 -- 47 2019-07-30 15:31:22.412025 \t 81135968 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "2500000 -- 44 2019-07-30 15:33:29.128791 \t 84474565 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "2600000 -- 14 2019-07-30 15:35:42.301699 \t 88200424 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "2700000 -- 13 2019-07-30 15:37:28.498959 \t 91372183 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "2800000 -- 23 2019-07-30 15:39:03.336006 \t 93829415 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "2900000 -- 21 2019-07-30 15:40:39.183978 \t 97329444 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "3000000 -- 22 2019-07-30 15:42:46.799734 \t 101214829 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "3100000 -- 17 2019-07-30 15:44:49.303253 \t 104827488 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "3200000 -- 31 2019-07-30 15:47:02.049223 \t 108846609 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "3300000 -- 8 2019-07-30 15:48:48.104346 \t 112186267 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "3400000 -- 72 2019-07-30 15:50:34.253659 \t 115611534 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "3500000 -- 36 2019-07-30 15:52:23.201304 \t 119143040 \t 3523 3523 3523\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OperRecdatas.p\n",
      "Total Num of All    Tokens 121235880\n",
      "Total Num of Unique Tokens 3538\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 43\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 865017\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 3556824\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 121235880\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuCorpus/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3538\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "\n",
    "Corpus2GroupMethod = '.p'\n",
    "\n",
    "Group2TextMethod   = 'element'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/LuohuCorpus/RANinfos_PresentIllness.p\n",
      "corpus/LuohuCorpus/hourRecs_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_OperName.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/hourRecs_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/DailyRecDatas_Text.p\n",
      "100000 -- 24 2019-07-30 16:18:20.403440 \t 3154859 \t 2760 2760 2760\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "200000 -- 68 2019-07-30 16:20:49.838954 \t 7155597 \t 3052 3052 3052\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "300000 -- 7 2019-07-30 16:23:14.390641 \t 11112892 \t 3159 3159 3159\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Outdiag.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Indiag.p\n",
      "400000 -- 39 2019-07-30 16:25:33.842378 \t 14845292 \t 3253 3253 3253\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistoryFlag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Characteristics.p\n",
      "500000 -- 43 2019-07-30 16:27:49.810102 \t 18795500 \t 3320 3320 3320\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Ininfo.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Tentativediag.p\n",
      "corpus/LuohuCorpus/RANinfos_Familyhistory.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_AllergicHistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Indiag.p\n",
      "corpus/LuohuCorpus/RANinfos_Specialityexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Deathdiag.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Basicinfo.p\n",
      "600000 -- 21 2019-07-30 16:29:45.730380 \t 21826015 \t 3424 3424 3424\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/RANinfos_Pasthistory.p\n",
      "corpus/LuohuCorpus/RANinfos_ChiefComplaint.p\n",
      "corpus/LuohuCorpus/RANinfos_Reviseddiagnosis.p\n",
      "corpus/LuohuCorpus/RANinfos_PhysicalExamination.p\n",
      "700000 -- 8 2019-07-30 16:31:38.027917 \t 25189347 \t 3443 3443 3443\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/RANinfos_Menstrualhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Indiag.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Treatplan.p\n",
      "corpus/LuohuCorpus/RANinfos_Tentativediagnosis.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outinfo.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_Accessoryexamination.p\n",
      "corpus/LuohuCorpus/OpEMRDatas_TreatMent.p\n",
      "800000 -- 72 2019-07-30 16:33:48.805814 \t 28586018 \t 3468 3468 3468\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/FirstCorinfos_Basicinfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Obstericalhistory.p\n",
      "corpus/LuohuCorpus/DeathRecordinfos_Reasonofdeath.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagdiscern.p\n",
      "corpus/LuohuCorpus/FirstCorinfos_Diagacord.p\n",
      "900000 -- 23 2019-07-30 16:36:08.149465 \t 32826030 \t 3507 3507 3507\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "corpus/LuohuCorpus/hourRecs_Ininfo.p\n",
      "corpus/LuohuCorpus/RANinfos_Personalhistory.p\n",
      "corpus/LuohuCorpus/hourRecs_Outoeord.p\n",
      "corpus/LuohuCorpus/OperRecdatas_Text.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Treatment.p\n",
      "corpus/LuohuCorpus/GetDisRecordinfos_Outdiag.p\n",
      "1000000 -- 43 2019-07-30 16:37:52.993866 \t 36296262 \t 3537 3537 3537\n",
      "\t 5000000 token\n",
      "\t 5000000 pos\n",
      "Total Num of All    Tokens 36455911\n",
      "Total Num of Unique Tokens 3539\n",
      "CORPUS\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 44\n",
      "TEXT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 213908\n",
      "SENT\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1005357\n",
      "TOKEN\tit is Dumped into file: data/LuohuCorpus/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 36455911\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuCorpus/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuCorpus/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3539\n",
      "\t\tWrite to: data/LuohuCorpus/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "\n",
    "Corpus2GroupMethod = '.p'\n",
    "\n",
    "Group2TextMethod   = 'element'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
