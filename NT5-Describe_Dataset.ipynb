{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Luohu Corpus\n",
    "\n",
    "## 1.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from nlptext.folder import Folder\n",
    "\n",
    "def folder_statistics(BasicObject):\n",
    "\n",
    "    folder_num = len(BasicObject.FOLDER['EndIDXTexts'])\n",
    "    \n",
    "    for i in range(folder_num):\n",
    "        f = Folder(i)\n",
    "    \n",
    "    All = []\n",
    "    for f in corpus.Folders:\n",
    "        d = {}\n",
    "        name = f.name.split('/')[-1].split('.')[0]\n",
    "        t_s, t_e = f.IdxTextStartEnd\n",
    "        s_s, s_e = f.IdxSentStartEnd\n",
    "        tk_s, tk_e = f.IdxTokenStartEnd\n",
    "        d['name'] = name \n",
    "        d['TextNum'] = t_e - t_s\n",
    "        d['SentNum'] = s_e - s_s\n",
    "        d['TokenNum']= tk_e - tk_s\n",
    "        All.append(d)\n",
    "    cols = ['name', 'TextNum', 'SentNum', 'TokenNum']\n",
    "    \n",
    "    DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "\n",
    "    d = DF.sum().to_dict()\n",
    "    d['name'] = '0_Total'\n",
    "    All.append(d)\n",
    "    DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuCorpus/'\n",
    "corpusFileIden = '.p'\n",
    "textType   = 'element'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = False # TODO\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "\n",
    "# BOB = 'channel/LuohuCorpus/char/Token3619/BOBMeta.p'\n",
    "# LGU = 'channel/LuohuCorpus/char/Token3619/LGUDict.p'\n",
    "# BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "CORPUSPath = 'corpus/medicalSent/'\n",
    "corpusFileIden = '.p'\n",
    "textType   = 'element'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = False # TODO\n",
    "annoKW = {}\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Folder Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "\n",
    "All = []\n",
    "for f in corpus.Folders:\n",
    "    d = {}\n",
    "    name = f.name.split('/')[-1].split('.')[0]\n",
    "    t_s, t_e = f.IdxTextStartEnd\n",
    "    s_s, s_e = f.IdxSentStartEnd\n",
    "    tk_s, tk_e = f.IdxTokenStartEnd\n",
    "    d['name'] = name \n",
    "    d['TextNum'] = t_e - t_s\n",
    "    d['SentNum'] = s_e - s_s\n",
    "    d['TokenNum']= tk_e - tk_s\n",
    "    All.append(d)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "cols = ['name', 'TextNum', 'SentNum', 'TokenNum']\n",
    "\n",
    "\n",
    "DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "\n",
    "d = DF.sum().to_dict()\n",
    "d['name'] = '0_Total'\n",
    "\n",
    "All.append(d)\n",
    "DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "DF.to_csv('LuohuCorpus_Data_Description.csv')\n",
    "DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF.to_csv('luohuCorpus_summary.csv', sep = ',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randidx = np.random.randint(0, high=100, size = 5)\n",
    "randidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus.Folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.folder import Folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "st_idx = 0\n",
    "\n",
    "output_dir = 'output/LuohuCorpus'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "        \n",
    "        \n",
    "for fd in corpus.Folders:\n",
    "    \n",
    "    Texts = fd.Texts\n",
    "    randidx = np.random.randint(0, high=len(Texts), size = 5)\n",
    "    # print(randidx)\n",
    "    fd_name = fd.name.split('/')[-1].split('.')[0]\n",
    "    print(fd_name)\n",
    "    for idx in randidx:\n",
    "        txt = Texts[idx]\n",
    "        file_name = fd_name + '--' + str(idx)+'.txt'\n",
    "\n",
    "        \n",
    "        with open(output_dir + '/' + file_name, 'w', encoding='utf-8') as f:\n",
    "            for st in txt.Sentences:\n",
    "                print('-->',st.sentence, '//')\n",
    "                f.write(st.sentence + ' //\\n\\n')\n",
    "                st_idx = st_idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Text Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BasicObject.TEXT['NUMSents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "\n",
    "for textIdx in range(len(BasicObject.TEXT['NUMSents'])):\n",
    "    d = {}\n",
    "    txt = Text(textIdx)\n",
    "    s, e = txt.IdxSentStartEnd\n",
    "    d['sentNum']  = e - s\n",
    "    s, e = txt.IdxTokenStartEnd\n",
    "    d['tokenNum'] = e - s \n",
    "    L.append(d)\n",
    "    if textIdx % 10000 == 0:\n",
    "        print(textIdx)\n",
    "\n",
    "TextInfo = pd.DataFrame(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "TextSents = TextInfo['sentNum']\n",
    "SentNumInText = TextSents.value_counts().sort_index()\n",
    "SentNumInText.plot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentNumInText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "TextTokens = TextInfo['tokenNum']\n",
    "TokenNumInText = TextTokens.value_counts().sort_index()\n",
    "TokenNumInText.plot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenNumInText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenNumInSent = pd.Series(BasicObject.SENT['NUMTokens']).value_counts().sort_index()\n",
    "%matplotlib inline\n",
    "TokenNumInText.plot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TokenNumInSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BasicObject.TOKEN['ORIGTokenIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenList = list(range(27656)) # 4G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.token import Token\n",
    "\n",
    "tk = Token(2)\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Luohu NER Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in *_LabeledEntity.txt; do mv -- \"$f\" \"${f//_LabeledEntity.txt/.Entity}\"; done\n",
    "# for f in *_LabeledAnatomy.txt; do mv -- \"$f\" \"${f//_LabeledAnatomy.txt/.Antonomy}\"; done\n",
    "# for f in *.Antonomy; do mv -- \"$f\" \"${f//.Antonomy/.Anatomy}\"; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "# from nlptext.utils import reCutText2Sent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Luohu NER ###########\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuNER/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.NER'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TEXT['ANNOFileName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TOKEN['ORIGTokenIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "\n",
    "st_id = 10\n",
    "\n",
    "st = Sentence(st_id)\n",
    "st.IdxTokenStartEnd\n",
    "\n",
    "\n",
    "# st = Sentence(sentence='消化内科')\n",
    "\n",
    "fd = st.Folder\n",
    "\n",
    "fd.Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.getChannelGrain('basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.TokenNum_Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = st.Tokens[8]\n",
    "tk.getChannelGrain('stroke', Max_Ngram = 3, end_grain = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.getTensor('stroke',Max_Ngram = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in BasicObject.TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_statistics(BasicObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "st_id = 100\n",
    "st = Sentence(st_id)\n",
    "\n",
    "print(st.sentence, '\\n')\n",
    "\n",
    "pprint(list(zip(st.sentence, st.getChannelGrain('annoE'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CCKS NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "\n",
    "########### CCKS NER ###########\n",
    "\n",
    "CORPUSPath = 'corpus/ccks/'\n",
    "corpusFileIden = None\n",
    "textType   = 'file'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = '.NER'\n",
    "annoKW = {\n",
    "    'sep': '\\t',\n",
    "    'notZeroIndex': 0,\n",
    "    'notRightOpen': 1,\n",
    "}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nlptext.folder import Folder\n",
    "\n",
    "def folder_statistics(BasicObject):\n",
    "\n",
    "    folder_num = len(BasicObject.FOLDER['EndIDXTexts'])\n",
    "    \n",
    "    for i in range(folder_num):\n",
    "        f = Folder(i)\n",
    "    \n",
    "    All = []\n",
    "    for f in corpus.Folders:\n",
    "        d = {}\n",
    "        name = f.name.split('/')[-1].split('.')[0]\n",
    "        t_s, t_e = f.IdxTextStartEnd\n",
    "        s_s, s_e = f.IdxSentStartEnd\n",
    "        tk_s, tk_e = f.IdxTokenStartEnd\n",
    "        d['name'] = name \n",
    "        d['TextNum'] = t_e - t_s\n",
    "        d['SentNum'] = s_e - s_s\n",
    "        d['TokenNum']= tk_e - tk_s\n",
    "        All.append(d)\n",
    "    cols = ['name', 'TextNum', 'SentNum', 'TokenNum']\n",
    "    \n",
    "    DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "\n",
    "    d = DF.sum().to_dict()\n",
    "    d['name'] = '0_Total'\n",
    "    All.append(d)\n",
    "    DF = pd.DataFrame(All)[cols].sort_values('name').reset_index(drop = True)\n",
    "    \n",
    "    return DF\n",
    "\n",
    "\n",
    "folder_statistics(BasicObject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from nlptext.utils import reCutText2Sent\n",
    "\n",
    "\n",
    "\n",
    "########### Wiki ###########\n",
    "CORPUSPath = 'dataset/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "\n",
    "textType   = 'line'\n",
    "\n",
    "Text2SentMethod  = reCutText2Sent\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "anno = False\n",
    "annoKW = {}\n",
    "\n",
    "\n",
    "MaxTextIdx = False\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx)\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "# corpus.IdxFolderStartEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicObject.CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
